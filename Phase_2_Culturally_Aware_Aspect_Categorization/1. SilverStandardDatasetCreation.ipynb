{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f350ab8",
   "metadata": {},
   "source": [
    "#########################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3e5bc",
   "metadata": {},
   "source": [
    "# Silver Standard Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422ff85",
   "metadata": {},
   "source": [
    "#########################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe0ccc",
   "metadata": {},
   "source": [
    "# 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8c7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4eb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews =pd.read_pickle(r'C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\clean_data2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ce00f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['place_id', 'name', 'address', 'latitude', 'longitude', 'types',\n",
       "       'googleMapsUri', 'priceRange', 'place_overall_rating',\n",
       "       'userRatingCount', 'extract_date', 'author_name', 'user_review_rating',\n",
       "       'text', 'author_id', 'uniqueID', 'timestamp', 'main_category',\n",
       "       'sub_category', 'state', 'operating_hours_cleaned', 'Monday_hours',\n",
       "       'Tuesday_hours', 'Wednesday_hours', 'Thursday_hours', 'Friday_hours',\n",
       "       'Saturday_hours', 'Sunday_hours', 'clean_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5cd81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA OVERVIEW:\n",
      "Total restaurant reviews: 19,330\n",
      "Unique restaurants: 3,866\n",
      "Unique reviewers: 14,590\n",
      "Date range: 2016-03-16 09:49:09 to 2025-10-26 03:19:38\n",
      "GEOGRAPHIC DISTRIBUTION:\n",
      "state\n",
      "Kuala Lumpur       2625\n",
      "Selangor           2300\n",
      "Perak              1905\n",
      "Johor              1705\n",
      "Penang             1475\n",
      "Pahang             1440\n",
      "Sabah              1310\n",
      "Kedah              1265\n",
      "Negeri Sembilan    1220\n",
      "Melaka             1120\n",
      "Sarawak            1005\n",
      "Putrajaya           625\n",
      "Terengganu          490\n",
      "Kelantan            440\n",
      "Perlis              290\n",
      "Labuan              115\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter to restaurant category\n",
    "df_restaurant = df_reviews[df_reviews['main_category'] == 'restaurant'].copy()\n",
    "\n",
    "print(f\"DATA OVERVIEW:\")\n",
    "print(f\"Total restaurant reviews: {len(df_restaurant):,}\")\n",
    "print(f\"Unique restaurants: {df_restaurant['place_id'].nunique():,}\")\n",
    "print(f\"Unique reviewers: {df_restaurant['author_id'].nunique():,}\")\n",
    "print(f\"Date range: {df_restaurant['timestamp'].min()} to {df_restaurant['timestamp'].max()}\")\n",
    "\n",
    "# Check state distribution\n",
    "print(f\"GEOGRAPHIC DISTRIBUTION:\")\n",
    "print(df_restaurant['state'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad2a05",
   "metadata": {},
   "source": [
    "# 1. Weak Supervision using Star Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fcf923",
   "metadata": {},
   "source": [
    "**Strategies**:\n",
    "\n",
    "1-2 stars â†’ Negative\n",
    "\n",
    "4-5 stars â†’ Positive  \n",
    "\n",
    "3 stars â†’ Neutral (EXCLUDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4a2518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RATING DISTRIBUTION:\n",
      "user_review_rating\n",
      "1     1198\n",
      "2      709\n",
      "3     1996\n",
      "4     4886\n",
      "5    10541\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check rating distribution\n",
    "print(f\"RATING DISTRIBUTION:\")\n",
    "print(df_restaurant['user_review_rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f21d4c",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "Class imbalance, more 4 and 5 stars reviews than negative reviews (1 and 2 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333e5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ASSIGN WEAK LABEL\n",
    "# ============================================================\n",
    "def assign_weak_label(rating):\n",
    "    \"\"\"\n",
    "    Assign weak sentiment labels based on star ratings\n",
    "    Following Zhao et al. (2018) \n",
    "    \n",
    "    1-2 stars â†’ Negative\n",
    "    4-5 stars â†’ Positive  \n",
    "    3 stars â†’ Neutral (EXCLUDED)\n",
    "    \"\"\"\n",
    "    if rating <= 2:\n",
    "        return 'negative'\n",
    "    elif rating >= 4:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b46b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WEAK LABEL DISTRIBUTION (Before Filtering)\n",
      "======================================================================\n",
      "weak_label\n",
      "positive    15427\n",
      "neutral      1996\n",
      "negative     1907\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply to restaurant data\n",
    "df_restaurant['weak_label'] = df_restaurant['user_review_rating'].apply(assign_weak_label)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WEAK LABEL DISTRIBUTION (Before Filtering)\")\n",
    "print(\"=\"*70)\n",
    "print(df_restaurant['weak_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8eecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FILTERING OUT 3-STAR (NEUTRAL) REVIEWS\n",
      "======================================================================\n",
      "\n",
      "BEFORE FILTERING:\n",
      "Total reviews: 19,330\n",
      "  - 5 stars: 10,541\n",
      "  - 4 stars: 4,886\n",
      "  - 3 stars: 1,996 â† REMOVED\n",
      "  - 2 stars: 709\n",
      "  - 1 stars: 1,198\n",
      "\n",
      "AFTER FILTERING:\n",
      "Total reviews: 17,334\n",
      "  - Positive (4-5 stars): 15,427\n",
      "  - Negative (1-2 stars): 1,907\n",
      "\n",
      "CLASS BALANCE:\n",
      "Positive: 89.0%\n",
      "Negative: 11.0%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FILTER OUT 3-STAR REVIEWS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILTERING OUT 3-STAR (NEUTRAL) REVIEWS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Keep only positive (4-5 stars) and negative (1-2 stars)\n",
    "df_filtered = df_restaurant[df_restaurant['weak_label'] != 'neutral'].copy()\n",
    "\n",
    "print(f\"\\nBEFORE FILTERING:\")\n",
    "print(f\"Total reviews: {len(df_restaurant):,}\")\n",
    "print(f\"  - 5 stars: {(df_restaurant['user_review_rating']==5).sum():,}\")\n",
    "print(f\"  - 4 stars: {(df_restaurant['user_review_rating']==4).sum():,}\")\n",
    "print(f\"  - 3 stars: {(df_restaurant['user_review_rating']==3).sum():,} â† REMOVED\")\n",
    "print(f\"  - 2 stars: {(df_restaurant['user_review_rating']==2).sum():,}\")\n",
    "print(f\"  - 1 stars: {(df_restaurant['user_review_rating']==1).sum():,}\")\n",
    "\n",
    "print(f\"\\nAFTER FILTERING:\")\n",
    "print(f\"Total reviews: {len(df_filtered):,}\")\n",
    "print(f\"  - Positive (4-5 stars): {(df_filtered['weak_label']=='positive').sum():,}\")\n",
    "print(f\"  - Negative (1-2 stars): {(df_filtered['weak_label']=='negative').sum():,}\")\n",
    "\n",
    "# Calculate class balance\n",
    "positive_pct = (df_filtered['weak_label']=='positive').sum() / len(df_filtered) * 100\n",
    "negative_pct = (df_filtered['weak_label']=='negative').sum() / len(df_filtered) * 100\n",
    "\n",
    "print(f\"\\nCLASS BALANCE:\")\n",
    "print(f\"Positive: {positive_pct:.1f}%\")\n",
    "print(f\"Negative: {negative_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db135ed0",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- Severe class imbalance!!! (89.0% positive) \n",
    "\n",
    "Decision:\n",
    "- Consider class weights in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097102e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOGRAPHIC DISTRIBUTION (After Filtering):\n",
      "state\n",
      "Kuala Lumpur       2405\n",
      "Selangor           2066\n",
      "Perak              1665\n",
      "Johor              1542\n",
      "Penang             1347\n",
      "Pahang             1263\n",
      "Sabah              1170\n",
      "Kedah              1153\n",
      "Negeri Sembilan    1071\n",
      "Melaka             1015\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check state distribution after filtering\n",
    "print(f\"GEOGRAPHIC DISTRIBUTION (After Filtering):\")\n",
    "print(df_filtered['state'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449dc84",
   "metadata": {},
   "source": [
    "# 2. Assign Class Weight in Handling Imbalance Data\n",
    "\n",
    "Calculate the Weights using formula below: \n",
    "\n",
    "Weight = Total_Samples / (Number_of_Classes * Class_Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f8a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Weight (Class 0): 4.5448\n",
      "Positive Weight (Class 1): 0.5618\n"
     ]
    }
   ],
   "source": [
    "# 1. Manually input your counts \n",
    "# We know: 17,334 Total, 1,907 Negative (Label 0), 15,427 Positive (Label 1)\n",
    "y_train = [0] * 1907 + [1] * 15427 \n",
    "\n",
    "# 2. Compute Weights\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "\n",
    "# 3. Print Results\n",
    "print(f\"Negative Weight (Class 0): {weights[0]:.4f}\")\n",
    "print(f\"Positive Weight (Class 1): {weights[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b0253",
   "metadata": {},
   "source": [
    "# 3. Noise Reduction: Sentiment Consistency Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5edc0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating VADER scores...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONSISTENCY FILTERING (Reduce Label Noise)\n",
    "# ============================================================\n",
    "\n",
    "# 1. Initialize VADER\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"Calculating VADER scores...\")\n",
    "\n",
    "# 2. Create the 'vader_score' column FIRST\n",
    "# We handle NaN/Empty values by assigning them a score of 0.0\n",
    "df_filtered['vader_score'] = df_filtered['text'].apply(\n",
    "    lambda x: vader.polarity_scores(str(x))['compound'] if pd.notna(x) and str(x).strip() != \"\" else 0.0\n",
    ")\n",
    "\n",
    "# 3. Define the Consistency Logic (Using the 0.5 Threshold)\n",
    "def check_consistency_row(row):\n",
    "    \"\"\"\n",
    "    Check if text sentiment matches weak label from rating\n",
    "    Returns True if consistent, False if inconsistent\n",
    "    \n",
    "    Following Chen et al., (2025) and Sharifpour & Lahmiri, (2023)\n",
    "    \"\"\"\n",
    "    score = row['vader_score']\n",
    "    label = row['weak_label']\n",
    "    \n",
    "    if label == 'positive':\n",
    "        # Keep unless STRONGLY negative (worse than -0.5)\n",
    "        return score > -0.5\n",
    "    elif label == 'negative':\n",
    "        # Keep unless STRONGLY positive (better than 0.5)\n",
    "        return score < 0.5\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 4. Apply the Logic\n",
    "df_filtered['is_consistent'] = df_filtered.apply(check_consistency_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa0802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews before filtering: 17,334\n",
      "Consistent reviews: 16,626\n",
      "Inconsistent reviews: 708\n",
      "Inconsistency rate: 4.1%\n",
      "\n",
      "Comparison with literature:\n",
      "Zhao et al. (2018): 13.4% label noise\n",
      "This data: 4.1% label noise\n",
      "\n",
      "SILVER STANDARD DATASET:\n",
      "Total reviews: 16,626\n",
      "Positive: 15,338\n",
      "Negative: 1,288\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reviews before filtering: {len(df_filtered):,}\")\n",
    "print(f\"Consistent reviews: {df_filtered['is_consistent'].sum():,}\")\n",
    "print(f\"Inconsistent reviews: {(~df_filtered['is_consistent']).sum():,}\")\n",
    "print(f\"Inconsistency rate: {(~df_filtered['is_consistent']).sum()/len(df_filtered)*100:.1f}%\")\n",
    "\n",
    "# Compare with Zhao et al. (2018): 13.4% noise\n",
    "print(f\"\\nComparison with literature:\")\n",
    "print(f\"Zhao et al. (2018): 13.4% label noise\")\n",
    "print(f\"This data: {(~df_filtered['is_consistent']).sum()/len(df_filtered)*100:.1f}% label noise\")\n",
    "\n",
    "# Keep only consistent reviews\n",
    "df_silver_standard = df_filtered[df_filtered['is_consistent']].copy()\n",
    "\n",
    "print(f\"\\nSILVER STANDARD DATASET:\")\n",
    "print(f\"Total reviews: {len(df_silver_standard):,}\")\n",
    "print(f\"Positive: {(df_silver_standard['weak_label']=='positive').sum():,}\")\n",
    "print(f\"Negative: {(df_silver_standard['weak_label']=='negative').sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592b463",
   "metadata": {},
   "source": [
    "# 4. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90de9326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEXT CHARACTERISTICS ANALYSIS\n",
      "============================================================\n",
      "\n",
      "TEXT LENGTH STATISTICS:\n",
      "count    16626.000000\n",
      "mean       391.442079\n",
      "std        307.357374\n",
      "min          6.000000\n",
      "25%        197.000000\n",
      "50%        308.000000\n",
      "75%        488.000000\n",
      "max       4093.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "WORD COUNT STATISTICS:\n",
      "count    16626.000000\n",
      "mean        68.800974\n",
      "std         53.626057\n",
      "min          1.000000\n",
      "25%         35.000000\n",
      "50%         54.000000\n",
      "75%         86.000000\n",
      "max        684.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate text lengths\n",
    "df_silver_standard['text_length'] = df_silver_standard['text'].str.len()\n",
    "df_silver_standard['word_count'] = df_silver_standard['text'].str.split().str.len()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT CHARACTERISTICS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTEXT LENGTH STATISTICS:\")\n",
    "print(df_silver_standard['text_length'].describe())\n",
    "\n",
    "print(\"\\nWORD COUNT STATISTICS:\")\n",
    "print(df_silver_standard['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e8f1d",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "- mean word count 68.8 -> It means the average Malaysian review in the dataset is ~69 words long.\n",
    "- max Length -> The longest review is 684 words. That is a huge essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faf37c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA QUALITY CHECKS:\n",
      "Reviews < 5 words: 16\n",
      "Reviews > 200 words: 495\n",
      "Empty reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for very short/long reviews\n",
    "print(f\"\\nDATA QUALITY CHECKS:\")\n",
    "print(f\"Reviews < 5 words: {(df_silver_standard['word_count'] < 5).sum():,}\")\n",
    "print(f\"Reviews > 200 words: {(df_silver_standard['word_count'] > 200).sum():,}\")\n",
    "print(f\"Empty reviews: {df_silver_standard['text'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f54a5cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE POSITIVE REVIEWS:\n",
      "['Aaliya Famansara serves some of the best crab curry and prawn curry aroundâ€¦ rich, aromatic, and full of authentic spice flavours. The seafood is fresh and cooked perfectly. Donâ€™t miss the sweet appam with brown sugar and coconut cream - the perfect finish to the meal. A must-visit for true Malaysian comfort food!', \"The food, service and atmosphere is top class and worthy of a fine dining establishment. Everything I had was good quality and tasted fine, although some dishes worked better than others.\\n\\nThe meal started on a high note with some complimentary Rasam served in small cups. I'm not sure what type of rasam it was but it may possibly be the tastiest rasam I've ever had. Compliments to the Chef!\\n\\nThe Fish Cutlets were tasty but had a strong fishy aroma, perhaps from the type of fish used? The Lamb Cutlets were better in terms of taste and aroma and I preferred it over the fish.\\n\\nThe stars of the meal for me were the Sambols or salads. The Gottu Kola, Bitter Gourd Salad and Amma's Eggplant Salad were all unique and tasty. All the salads were seasoned with grated coconut and sliced onions. These are ubiquitous of Sri Lankan cuisine and are Highly Recommended. Stir Fried Four Angle Beans were on the Daily Specials and this was very good too.\\n\\nI found the Fish Sothy to be a little too thick with coconut cream for my liking. It was also lacking some spiciness from green chilies which are usually added. The Drumstick Curry was similarly a little too thick with coconut cream but had a nice sourness. Both would pair nicely with string hoppers.\\n\\nThe Signature Mutton Paal Poriyal was a dry preparation, but I felt it was somewhat mild compared to the spicy and fiery versions in Sri Lanka.\\n\\nBoth the Vegetarian and Chicken Kothu were decent with the choice of String Hoppers. Personally I prefer the Kothu with Roti instead.\\n\\nKulaiyal are packets of rice with choice of Vegetables, Chicken, Lamb or Seafood. The Vegetable and Seafood versions we had were decent but I felt the rice ended up rather mushy from the steaming. I'd prefer just plain rice with the dishes, like in a Lamprais packet.\\n\\nMasala Tea was rather disappointing as it came with frothed milk. Not something I'd expect in a good Sri Lankan tea, and it felt out of place.\", \"I recently had the pleasure of dining at Aliyaa and it was nothing short of extraordinary. From the moment I stepped in, the warm hospitality and aromatic spices transported me straight to the heart of Sri Lanka.\\n\\nThe menu is a vibrant showcase of authentic Sri Lankan cuisine, and each dish bursts with bold, complex flavors. The lampraisâ€”a fragrant banana leaf-wrapped parcel of rice, meat, and sambolsâ€”was a standout, packed with flavor and cooked to perfection.\\n\\nOne dish that absolutely stole the show was the seafood kolaiyal. Perfectly cooked prawns and squid were dry-fried with roasted spices, curry leaves, and just the right amount of heat, creating a dish that was intensely flavorful, smoky, and addictive. The sothiâ€”a light turmeric and coconut milk curryâ€”offered a soothing contrast, while the beetroot curry added a sweet, earthy touch that brought harmony to the entire spread.\\n\\nThe appam were a true highlightâ€”crispy, lace-edged bowls with soft, fluffy centers that paired beautifully with creamy coconut milk and brown sugar.\\n\\nWhat sets this place apart is the attention to detail and authenticity. The chefs clearly know and love the food they create, and it shines through in every dish. This is not just a mealâ€”itâ€™s an experience.\\n\\nIf you're looking to explore the true essence of Sri Lankan cuisine, Aliyaa is a must-visit. Extraordinary food, heartfelt service, and a cultural journey in every bite.\"]\n",
      "\n",
      "SAMPLE NEGATIVE REVIEWS:\n",
      "['Overpriced for what they offer. Food portion was ridiculously small, and definitely not worth the money. Please do spend some money come to bars in kl and learn!', 'My order was order 177 we took Minutes to Walk around and we came back and our food Was still Not ready when we finally get our food, this is what we get two raw pieces of chicken. Pathetic I got poisoning, And I have bookings for a theme park tomorrow', 'Hello McD Genting â€¦ I think this is unacceptable lahâ€¦ first round of crinkle fries was over fried, we requested change and server said all are like tat, insisting thatâ€™s the colour of the fries. We showed the app photo and she agreed to change it. The second fries is what u see in the photo. Itâ€™s darker than the first box ðŸ˜µ this is burnt fries laâ€¦ how can u serve something like this, Sumore claiming thatâ€™s how itâ€™s supposed to be??? Pls explain']\n"
     ]
    }
   ],
   "source": [
    "# Sample reviews\n",
    "print(f\"\\nSAMPLE POSITIVE REVIEWS:\")\n",
    "print(df_silver_standard[df_silver_standard['weak_label']=='positive']['text'].head(3).tolist())\n",
    "\n",
    "print(f\"\\nSAMPLE NEGATIVE REVIEWS:\")\n",
    "print(df_silver_standard[df_silver_standard['weak_label']=='negative']['text'].head(3).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc18b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‡²ðŸ‡¾ MANGLISH PREVALENCE:\n",
      "Reviews with Manglish terms: 4,765 (28.7%)\n"
     ]
    }
   ],
   "source": [
    "# Check for Manglish patterns\n",
    "manglish_patterns = ['sedap', 'lambat', 'giler', 'laju', 'sangat', 'sikit', \n",
    "                     'banyak', 'panas', 'sejuk', 'makan', 'tapau', 'bungkus', \n",
    "                     'mamak', 'kureng', 'masin', 'wok-hei', 'manis', 'pedas', \n",
    "                     'kick', 'shiok', 'sambal', 'goreng', 'kantoi', 'boss',\n",
    "                     'amoi', 'anneh',  'abang', 'kakak', 'auntie', 'uncle', \n",
    "                     'mahal', 'murah', 'berbaloi', 'kecil', 'besar', 'jauh', \n",
    "                     'dekat', 'senyap', 'sesak', 'solat', 'bising', 'senyap',\n",
    "                     'nasi', 'roti',  'kuah', 'nasi lemak', 'char kuey teow', \n",
    "                     'roti canai', 'laksa', 'satay', 'rendang', 'dim sum', \n",
    "                     'wan tan mee', 'hokkien mee', 'chicken rice', 'nasi kandar', \n",
    "                     'banana leaf', 'tomyam', 'cendol', 'teh tarik', 'kopi', \n",
    "                     'kek', 'kerabu', 'bak kut teh', 'yong tau foo', 'naan', \n",
    "                     'otak-otak', 'kuih', 'apam', 'chee cheong fun', 'biryani',\n",
    "                     'ikan', 'ayam']\n",
    "df_silver_standard['has_manglish'] = df_silver_standard['text'].str.lower().str.contains('|'.join(manglish_patterns), na=False)\n",
    "\n",
    "print(f\"\\nðŸ‡²ðŸ‡¾ MANGLISH PREVALENCE:\")\n",
    "print(f\"Reviews with Manglish terms: {df_silver_standard['has_manglish'].sum():,} ({df_silver_standard['has_manglish'].sum()/len(df_silver_standard)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93efa22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver_standard.to_csv(r'C:\\Users\\Ong Hui Ling\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\silver_std.csv')\n",
    "df_silver_standard.to_pickle(r'C:\\Users\\Ong Hui Ling\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\silver_std.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
