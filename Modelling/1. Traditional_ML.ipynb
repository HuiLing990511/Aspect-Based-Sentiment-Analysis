{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38297865",
      "metadata": {
        "id": "38297865"
      },
      "source": [
        "# Traditional ML Baseline for ABSA Sentiment Classification\n",
        "\n",
        "**Objective:** Train Random Forest classifier with TF-IDF features as baseline to compare with XLM-RoBERTa\n",
        "\n",
        "**Academic Justification:**\n",
        "- Establishes baseline performance using classical ML (Scikit-learn Random Forest)\n",
        "- TF-IDF captures term importance without contextual embeddings\n",
        "- Comparison validates whether transformer pre-training provides value for Manglish code-switching\n",
        "- Following best practices: same train/test split, same evaluation protocol, same class imbalance handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e89709",
      "metadata": {
        "id": "b0e89709"
      },
      "source": [
        "# Stage 0: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e31676a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e31676a4",
        "outputId": "9ad2ab6d-8b14-48c7-80ff-dadc7c1ce62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to google drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive (To save the model checkpoints)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e9e635b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9e635b0",
        "outputId": "68c22510-e97d-4eab-8f0e-00eb38d59693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì All libraries loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP Processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# ML & Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"‚úì All libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "caaa0290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caaa0290",
        "outputId": "d0e28573-c17e-47de-d8be-186fd7c8fae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì NLTK resources downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"‚úì NLTK resources downloaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b61515",
      "metadata": {
        "id": "84b61515"
      },
      "source": [
        "# Stage 1: Configuration & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8600f635",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8600f635",
        "outputId": "50f23731-e25e-47ee-a486-f27a805d0b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Configuration loaded\n",
            "  Data path: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/aspect_categorization_after_filtering.pkl\n",
            "  Gold path: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/Final_Gold_Standard.csv\n",
            "  Random seed: 42\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATA_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/aspect_categorization_after_filtering.pkl'\n",
        "GOLD_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/Final_Gold_Standard.csv'\n",
        "#DATA_PATH = r'C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\aspect_categorization_after_filtering.pkl'\n",
        "#GOLD_PATH = r'C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\Final_Gold_Standard.csv'\n",
        "\n",
        "# Output path\n",
        "OUTPUT_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.15\n",
        "VAL_SIZE = 0.10\n",
        "\n",
        "# Label encoding\n",
        "LABEL2ID = {\"negative\": 0, \"positive\": 1}\n",
        "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "print(f\"‚úì Configuration loaded\")\n",
        "print(f\"  Data path: {DATA_PATH}\")\n",
        "print(f\"  Gold path: {GOLD_PATH}\")\n",
        "print(f\"  Random seed: {RANDOM_SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DoWhnIjudadg",
      "metadata": {
        "id": "DoWhnIjudadg"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "77a2e808",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77a2e808",
        "outputId": "4a76db0a-b07c-41eb-e39e-f088a05cbb62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "LOADING & PREPARING DATA\n",
            "======================================================================\n",
            "  Raw segments loaded: 128,778\n",
            "\n",
            "  ‚ö†Ô∏è  DATA LEAKAGE PREVENTION:\n",
            "  Loading gold standard to identify held-out review IDs...\n",
            "  ‚ö†Ô∏è  Warning: Could not find review ID column in gold dataset\n",
            "  ‚úì Gold dataset loaded: 645 annotations\n",
            "  ‚úì Unique review IDs in gold: 0\n",
            "  ‚úì Filtered out 0 segments from gold reviews (0.0%)\n",
            "  ‚úì Training segments remaining: 128,778\n",
            "\n",
            "  FILTERING STRATEGY:\n",
            "    Single-aspect segments:   98,693 ( 76.6%) ‚Üí KEPT\n",
            "    Multi-aspect segments:    30,085 ( 23.4%) ‚Üí DROPPED\n",
            "\n",
            "  Label distribution:\n",
            "    negative  :   6,783 (  6.9%)\n",
            "    positive  :  91,910 ( 93.1%)\n",
            "\n",
            "‚úì Data preparation complete: 98,693 segments ready for training\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING & PREPARING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load training data\n",
        "df = pd.read_pickle(DATA_PATH)\n",
        "print(f\"  Raw segments loaded: {len(df):,}\")\n",
        "\n",
        "# --- PREVENT DATA LEAKAGE: Exclude gold standard review IDs ----------\n",
        "print(f\"\\n  ‚ö†Ô∏è  DATA LEAKAGE PREVENTION:\")\n",
        "print(f\"  Loading gold standard to identify held-out review IDs...\")\n",
        "\n",
        "try:\n",
        "    gold_df = pd.read_csv(GOLD_PATH)\n",
        "\n",
        "    # Extract unique Original_Review_IDs from gold dataset\n",
        "    if 'Original_Review_ID' in gold_df.columns:\n",
        "        gold_review_ids = set(gold_df['Original_Review_ID'].unique())\n",
        "    elif 'Review_ID' in gold_df.columns:\n",
        "        gold_review_ids = set(gold_df['Review_ID'].unique())\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  Warning: Could not find review ID column in gold dataset\")\n",
        "        gold_review_ids = set()\n",
        "\n",
        "    print(f\"  ‚úì Gold dataset loaded: {len(gold_df):,} annotations\")\n",
        "    print(f\"  ‚úì Unique review IDs in gold: {len(gold_review_ids):,}\")\n",
        "\n",
        "    # Filter out segments from gold review IDs\n",
        "    n_before = len(df)\n",
        "    df = df[~df['Original_Review_ID'].isin(gold_review_ids)].copy()\n",
        "    n_after = len(df)\n",
        "    n_removed = n_before - n_after\n",
        "\n",
        "    print(f\"  ‚úì Filtered out {n_removed:,} segments from gold reviews ({n_removed/n_before*100:.1f}%)\")\n",
        "    print(f\"  ‚úì Training segments remaining: {n_after:,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ‚úó Error loading gold dataset: {e}\")\n",
        "\n",
        "# Filter to single-aspect segments\n",
        "df[\"num_aspects\"] = df[\"Aspect_Labels\"].apply(len)\n",
        "df_single = df[df[\"num_aspects\"] == 1].copy()\n",
        "df_single[\"aspect\"] = df_single[\"Aspect_Labels\"].apply(lambda x: x[0])\n",
        "\n",
        "n_multi = len(df) - len(df_single)\n",
        "pct_retained = (len(df_single) / len(df)) * 100\n",
        "\n",
        "print(f\"\\n  FILTERING STRATEGY:\")\n",
        "print(f\"    Single-aspect segments:  {len(df_single):>7,} ({pct_retained:>5.1f}%) ‚Üí KEPT\")\n",
        "print(f\"    Multi-aspect segments:   {n_multi:>7,} ({100-pct_retained:>5.1f}%) ‚Üí DROPPED\")\n",
        "\n",
        "# Encode labels\n",
        "df_single[\"label\"] = df_single[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "print(f\"\\n  Label distribution:\")\n",
        "for label_name, label_id in LABEL2ID.items():\n",
        "    count = (df_single[\"label\"] == label_id).sum()\n",
        "    pct = count / len(df_single) * 100\n",
        "    print(f\"    {label_name:<10}: {count:>7,} ({pct:>5.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚úì Data preparation complete: {len(df_single):,} segments ready for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "873d5195",
      "metadata": {
        "id": "873d5195"
      },
      "source": [
        "# Stage 2: Text Preprocessing (Conventional NLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "884e0529",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "884e0529",
        "outputId": "c11c1d8b-7419-42c6-e047-f4b64c2a6e85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  The nasi lemak was incredibly sedap but the service was lambat!\n",
            "Processed: nasi lemak incredibly sedap service lambat\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Apply conventional NLP preprocessing pipeline.\n",
        "\n",
        "    Steps:\n",
        "    1. Lowercase conversion\n",
        "    2. Remove special characters (keep letters, numbers, spaces)\n",
        "    3. Tokenization\n",
        "    4. Remove English stopwords\n",
        "    5. Lemmatization\n",
        "\n",
        "    Why:\n",
        "        Traditional ML models (Random Forest, SVM) lack contextual understanding.\n",
        "        Preprocessing reduces noise and dimensionality for TF-IDF vectorization.\n",
        "\n",
        "    Note:\n",
        "        We do NOT remove Manglish terms (sedap, mamak) as they carry sentiment.\n",
        "        Stopwords removal is conservative to preserve sentiment-bearing phrases.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters but keep spaces\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join back\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Test preprocessing\n",
        "sample_text = \"The nasi lemak was incredibly sedap but the service was lambat!\"\n",
        "print(f\"Original:  {sample_text}\")\n",
        "print(f\"Processed: {preprocess_text(sample_text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e8d45f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70e8d45f",
        "outputId": "808d7671-926f-4920-c0aa-71ceb52a86a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PREPROCESSING TEXT DATA\n",
            "======================================================================\n",
            "Processing 98,693 segments...\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PREPROCESSING TEXT DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Apply preprocessing to all segments\n",
        "print(f\"Processing {len(df_single):,} segments...\")\n",
        "df_single['processed_text'] = df_single['Segment'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b2b5e51c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "b2b5e51c",
        "outputId": "339deabc-eed9-4444-8f60-615f7b866624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EMPTY PROCESSED TEXTS ORIGNIAL SEGMENTS\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Segment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>again</th>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>once again</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>over again</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>again)</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>again not Ë±¨Ê≤πÊ∏£</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>again ‚ù§Ô∏èüòä</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>here again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>was again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>because again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>will be here again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i was here again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>so so√¢‚Ç¨¬¶</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "Segment\n",
              "again                 46\n",
              "once again             4\n",
              "over again             2\n",
              "again)                 1\n",
              "again not Ë±¨Ê≤πÊ∏£          1\n",
              "again ‚ù§Ô∏èüòä              1\n",
              "here again             1\n",
              "was again              1\n",
              "because again          1\n",
              "will be here again     1\n",
              "i was here again       1\n",
              "so so√¢‚Ç¨¬¶               1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for empty processed texts\n",
        "print(\"=\"*70)\n",
        "print(\"EMPTY PROCESSED TEXTS ORIGNIAL SEGMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_single[df_single['processed_text'].str.len() == 0]['Segment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "93c97e67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c97e67",
        "outputId": "6ce2ee88-958f-4c96-d7c6-fe87176bb60a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚ö†Ô∏è  Warning: 61 segments became empty after preprocessing\n",
            "  ‚úì Removed empty segments. Remaining: 98,632\n",
            "\n",
            "‚úì Text preprocessing complete\n",
            "\n",
            "Sample processed segments:\n",
            "\n",
            "  Original:  coconut cream - the perfect finish to the meal...\n",
            "  Processed: coconut cream perfect finish meal...\n",
            "\n",
            "  Original:  cooked perfectly...\n",
            "  Processed: cooked perfectly...\n",
            "\n",
            "  Original:  don‚Äôt miss the sweet appam with brown sugar...\n",
            "  Processed: dont miss sweet appam brown sugar...\n"
          ]
        }
      ],
      "source": [
        "# Check for empty processed texts\n",
        "n_empty = (df_single['processed_text'].str.len() == 0).sum()\n",
        "\n",
        "if n_empty > 0:\n",
        "    print(f\"  ‚ö†Ô∏è  Warning: {n_empty} segments became empty after preprocessing\")\n",
        "    df_single = df_single[df_single['processed_text'].str.len() > 0].copy()\n",
        "    print(f\"  ‚úì Removed empty segments. Remaining: {len(df_single):,}\")\n",
        "\n",
        "print(f\"\\n‚úì Text preprocessing complete\")\n",
        "print(f\"\\nSample processed segments:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\n  Original:  {df_single.iloc[i]['Segment'][:80]}...\")\n",
        "    print(f\"  Processed: {df_single.iloc[i]['processed_text'][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3846e566",
      "metadata": {
        "id": "3846e566"
      },
      "source": [
        "# Stage 3: Train/Val/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3e32ef73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e32ef73",
        "outputId": "6bb755e3-3c24-4120-d68f-954a27e9d251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAIN/VAL/TEST SPLIT\n",
            "======================================================================\n",
            "\n",
            "Split sizes:\n",
            "  Train :  73,973 rows | pos: 68,891 ( 93.1%) | neg:  5,082 (  6.9%)\n",
            "  Val   :   9,864 rows | pos:  9,186 ( 93.1%) | neg:    678 (  6.9%)\n",
            "  Test  :  14,795 rows | pos: 13,779 ( 93.1%) | neg:  1,016 (  6.9%)\n",
            "\n",
            "‚úì Data split complete\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TRAIN/VAL/TEST SPLIT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Stage 1: Separate test set (stratified)\n",
        "df_trainval, df_test = train_test_split(\n",
        "    df_single,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=df_single[\"label\"],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Stage 2: Split remainder into train + val\n",
        "adjusted_val_size = VAL_SIZE / (1.0 - TEST_SIZE)\n",
        "df_train, df_val = train_test_split(\n",
        "    df_trainval,\n",
        "    test_size=adjusted_val_size,\n",
        "    stratify=df_trainval[\"label\"],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit sizes:\")\n",
        "for name, split_df in [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]:\n",
        "    pos = (split_df[\"label\"] == 1).sum()\n",
        "    neg = (split_df[\"label\"] == 0).sum()\n",
        "    print(f\"  {name:<6}: {len(split_df):>7,} rows | \"\n",
        "          f\"pos: {pos:>6,} ({pos/len(split_df)*100:>5.1f}%) | \"\n",
        "          f\"neg: {neg:>6,} ({neg/len(split_df)*100:>5.1f}%)\")\n",
        "\n",
        "# Extract X (processed text) and y (labels)\n",
        "X_train = df_train['processed_text'].values\n",
        "X_val = df_val['processed_text'].values\n",
        "X_test = df_test['processed_text'].values\n",
        "\n",
        "y_train = df_train['label'].values\n",
        "y_val = df_val['label'].values\n",
        "y_test = df_test['label'].values\n",
        "\n",
        "print(f\"\\n‚úì Data split complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96efa955",
      "metadata": {
        "id": "96efa955"
      },
      "source": [
        "# Stage 4: TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e488170a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e488170a",
        "outputId": "6731c73b-02f7-425f-c829-0afebe319f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TF-IDF VECTORIZATION\n",
            "======================================================================\n",
            "TF-IDF Configuration:\n",
            "  Max features:  5,000\n",
            "  N-gram range:  (1, 2)\n",
            "  Min doc freq:  2\n",
            "  Max doc freq:  0.8\n",
            "\n",
            "Fitting TF-IDF on 73,973 training samples...\n",
            "\n",
            "‚úì TF-IDF vectorization complete\n",
            "  Train shape: (73973, 5000)\n",
            "  Val shape:   (9864, 5000)\n",
            "  Test shape:  (14795, 5000)\n",
            "  Vocabulary size: 5,000 terms\n",
            "\n",
            "Top 20 features by TF-IDF score:\n",
            "  food                 2054.48\n",
            "  service              1326.58\n",
            "  good                 1203.42\n",
            "  delicious            1177.20\n",
            "  friendly             1018.66\n",
            "  staff                918.34\n",
            "  chicken              827.56\n",
            "  taste                801.65\n",
            "  price                790.02\n",
            "  nice                 779.33\n",
            "  fresh                730.00\n",
            "  place                644.65\n",
            "  great                643.54\n",
            "  attentive            638.20\n",
            "  tasty                637.55\n",
            "  dish                 613.12\n",
            "  restaurant           545.40\n",
            "  come                 531.83\n",
            "  rice                 512.88\n",
            "  also                 509.62\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TF-IDF VECTORIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,     # Limit to top 5000 features (prevent overfitting)\n",
        "    ngram_range=(1, 2),    # Unigrams + bigrams (capture phrases like \"nasi lemak\")\n",
        "    min_df=2,              # Ignore terms appearing in < 2 documents\n",
        "    max_df=0.8,            # Ignore terms appearing in > 80% of documents\n",
        "    sublinear_tf=True      # Apply sublinear tf scaling (1 + log(tf))\n",
        ")\n",
        "\n",
        "print(f\"TF-IDF Configuration:\")\n",
        "print(f\"  Max features:  {vectorizer.max_features:,}\")\n",
        "print(f\"  N-gram range:  {vectorizer.ngram_range}\")\n",
        "print(f\"  Min doc freq:  {vectorizer.min_df}\")\n",
        "print(f\"  Max doc freq:  {vectorizer.max_df}\")\n",
        "\n",
        "# Fit on training data and transform all splits\n",
        "print(f\"\\nFitting TF-IDF on {len(X_train):,} training samples...\")\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"\\n‚úì TF-IDF vectorization complete\")\n",
        "print(f\"  Train shape: {X_train_tfidf.shape}\")\n",
        "print(f\"  Val shape:   {X_val_tfidf.shape}\")\n",
        "print(f\"  Test shape:  {X_test_tfidf.shape}\")\n",
        "print(f\"  Vocabulary size: {len(vectorizer.vocabulary_):,} terms\")\n",
        "\n",
        "# Show top features by TF-IDF score\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = X_train_tfidf.sum(axis=0).A1\n",
        "top_indices = tfidf_scores.argsort()[-20:][::-1]\n",
        "\n",
        "print(f\"\\nTop 20 features by TF-IDF score:\")\n",
        "for idx in top_indices:\n",
        "    print(f\"  {feature_names[idx]:<20} {tfidf_scores[idx]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0905141c",
      "metadata": {
        "id": "0905141c"
      },
      "source": [
        "# Stage 5: Multi-Model Training\n",
        "\n",
        "**Academic Justification:**\n",
        "- Compare 5 traditional ML classifiers: Logistic Regression, SVM, Naive Bayes, XGBoost, Random Forest\n",
        "- Evaluate on validation set to identify best model(s) for hyperparameter tuning\n",
        "- Different models capture different patterns: linear (LR, SVM) vs non-linear (RF, XGB) vs probabilistic (NB)\n",
        "- Best practice: broad comparison before expensive hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "537af86a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "537af86a",
        "outputId": "48c3c79f-503d-45c2-e923-a1027767af8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAINING MULTIPLE ML MODELS\n",
            "======================================================================\n",
            "Class imbalance handling:\n",
            "  Negative weight: 7.2779\n",
            "  Positive weight: 0.5369\n",
            "  XGBoost scale_pos_weight: 13.5559\n",
            "\n",
            "======================================================================\n",
            "TRAINING 5 MODELS\n",
            "======================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "  ‚úì Complete in 1.60 seconds\n",
            "\n",
            "Training SVM (Linear)...\n",
            "  ‚úì Complete in 225.01 seconds\n",
            "\n",
            "Training Naive Bayes...\n",
            "  ‚úì Complete in 0.01 seconds\n",
            "\n",
            "Training XGBoost...\n",
            "  ‚úì Complete in 5.01 seconds\n",
            "\n",
            "Training Random Forest...\n",
            "  ‚úì Complete in 1.07 seconds\n",
            "\n",
            "‚úì All models trained successfully\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING MULTIPLE ML MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compute class weights (for models that support it)\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = {i: class_weights_array[i] for i in range(len(class_weights_array))}\n",
        "\n",
        "# Calculate scale_pos_weight for XGBoost (ratio of negative to positive)\n",
        "scale_pos_weight = class_weights_array[0] / class_weights_array[1]\n",
        "\n",
        "print(f\"Class imbalance handling:\")\n",
        "print(f\"  Negative weight: {class_weights_dict[0]:.4f}\")\n",
        "print(f\"  Positive weight: {class_weights_dict[1]:.4f}\")\n",
        "print(f\"  XGBoost scale_pos_weight: {scale_pos_weight:.4f}\")\n",
        "\n",
        "# Define 5 models with reasonable default parameters\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    \"SVM (Linear)\": SVC(\n",
        "        kernel='linear',\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=RANDOM_SEED\n",
        "    ),\n",
        "    \"Naive Bayes\": MultinomialNB(\n",
        "        alpha=1.0\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='logloss'\n",
        "    ),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"TRAINING {len(models)} MODELS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Train all models and store results\n",
        "trained_models = {}\n",
        "training_times = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    trained_models[model_name] = model\n",
        "    training_times[model_name] = training_time\n",
        "\n",
        "    print(f\"  ‚úì Complete in {training_time:.2f} seconds\\n\")\n",
        "\n",
        "print(f\"‚úì All models trained successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9acccc",
      "metadata": {
        "id": "8d9acccc"
      },
      "source": [
        "# Stage 6: Model Comparison on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f8383a56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8383a56",
        "outputId": "8a3c52c7-9bad-4469-87e8-ffb102dabfa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COMPARING MODELS ON VALIDATION SET\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "VALIDATION SET RESULTS (Ranked by Macro-F1)\n",
            "======================================================================\n",
            "\n",
            "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
            "Logistic Regression    0.8161    0.6254       0.3581       0.8927           1.599633\n",
            "       SVM (Linear)    0.7981    0.6102       0.3395       0.8808         225.013811\n",
            "        Naive Bayes    0.9379    0.6002       0.2328       0.9676           0.009229\n",
            "            XGBoost    0.9342    0.5240       0.0820       0.9659           5.010101\n",
            "      Random Forest    0.6187    0.4926       0.2397       0.7456           1.072564\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPARING MODELS ON VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate all models on validation set\n",
        "results = []\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute same metrics as BERT for fair comparison.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    per_class_f1 = f1_score(y_true, y_pred, average=None, labels=[0, 1])\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": round(acc, 4),\n",
        "        \"macro_f1\": round(macro_f1, 4),\n",
        "        \"neg_f1\": round(per_class_f1[0], 4),\n",
        "        \"pos_f1\": round(per_class_f1[1], 4),\n",
        "    }\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    # Predict on validation set\n",
        "    y_pred_val = model.predict(X_val_tfidf)\n",
        "\n",
        "    # Compute metrics\n",
        "    val_metrics = compute_metrics(y_val, y_pred_val)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': val_metrics['accuracy'],\n",
        "        'Macro-F1': val_metrics['macro_f1'],\n",
        "        'Negative F1': val_metrics['neg_f1'],\n",
        "        'Positive F1': val_metrics['pos_f1'],\n",
        "        'Training Time (s)': training_times[model_name]\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"VALIDATION SET RESULTS (Ranked by Macro-F1)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ffcb075",
      "metadata": {
        "id": "3ffcb075"
      },
      "source": [
        "# Stage 7: Model Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "341e09de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "341e09de",
        "outputId": "b1257b6b-ac94-4939-bc89-01eeb3b028b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COMPARING MODELS ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "TEST SET RESULTS (Ranked by Macro-F1)\n",
            "======================================================================\n",
            "\n",
            "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
            "Logistic Regression    0.8158    0.6253       0.3581       0.8925           1.599633\n",
            "       SVM (Linear)    0.7982    0.6103       0.3397       0.8809         225.013811\n",
            "        Naive Bayes    0.9398    0.6096       0.2506       0.9686           0.009229\n",
            "            XGBoost    0.9342    0.5253       0.0847       0.9659           5.010101\n",
            "      Random Forest    0.6212    0.4936       0.2394       0.7478           1.072564\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPARING MODELS ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate all models on test set\n",
        "results = []\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    # Predict on test set\n",
        "    y_pred_test = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_metrics = compute_metrics(y_test, y_pred_test)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': test_metrics['accuracy'],\n",
        "        'Macro-F1': test_metrics['macro_f1'],\n",
        "        'Negative F1': test_metrics['neg_f1'],\n",
        "        'Positive F1': test_metrics['pos_f1'],\n",
        "        'Training Time (s)': training_times[model_name]\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"TEST SET RESULTS (Ranked by Macro-F1)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7fa5ef",
      "metadata": {
        "id": "be7fa5ef"
      },
      "source": [
        "# Stage 8: Gold Standard Evaluation (Before Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "51f17999",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51f17999",
        "outputId": "d1be23fe-dad9-4136-dc8d-d4875b7707c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EVALUATING ON GOLD STANDARD (GROUND TRUTH)\n",
            "======================================================================\n",
            "  Gold dataset loaded: 645 rows\n",
            "  After exploding: 721 aspect-segment pairs\n",
            "\n",
            "Preprocessing gold segments...\n",
            "Vectorizing gold segments...\n",
            "Running inference on gold set using ...\n",
            "\n",
            "======================================================================\n",
            "GOLD SET RESULTS (Ranked by Macro-F1)\n",
            "======================================================================\n",
            "\n",
            "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
            "       SVM (Linear)    0.8363    0.8251       0.7807       0.8695         225.013811\n",
            "Logistic Regression    0.8322    0.8179       0.7669       0.8689           1.599633\n",
            "      Random Forest    0.7767    0.7706       0.7330       0.8081           1.072564\n",
            "        Naive Bayes    0.6852    0.5472       0.2972       0.7971           0.009229\n",
            "            XGBoost    0.6477    0.4601       0.1419       0.7784           5.010101\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"EVALUATING ON GOLD STANDARD (GROUND TRUTH)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load gold standard\n",
        "gold_df = pd.read_csv(GOLD_PATH)\n",
        "print(f\"  Gold dataset loaded: {len(gold_df):,} rows\")\n",
        "\n",
        "# Prepare gold data (same as BERT)\n",
        "gold_df_prep = gold_df.copy()\n",
        "gold_df_prep.rename(columns={\n",
        "    \"Manual_Aspect\": \"aspect\",\n",
        "    \"Manual_Sentiment\": \"Sentiment_Label\",\n",
        "}, inplace=True)\n",
        "\n",
        "# Normalize sentiment labels to lowercase\n",
        "gold_df_prep[\"Sentiment_Label\"] = gold_df_prep[\"Sentiment_Label\"].str.lower()\n",
        "\n",
        "# Handle multi-aspect segments: explode into separate rows\n",
        "import ast\n",
        "def parse_aspect(val):\n",
        "    if isinstance(val, str):\n",
        "        try:\n",
        "            parsed = ast.literal_eval(val)\n",
        "            if isinstance(parsed, list):\n",
        "                return parsed\n",
        "            else:\n",
        "                return [parsed]\n",
        "        except (ValueError, SyntaxError):\n",
        "            return [val]\n",
        "    elif isinstance(val, list):\n",
        "        return val\n",
        "    else:\n",
        "        return [str(val)]\n",
        "\n",
        "gold_df_prep[\"aspect\"] = gold_df_prep[\"aspect\"].apply(parse_aspect)\n",
        "gold_df_exploded = gold_df_prep.explode(\"aspect\").reset_index(drop=True)\n",
        "\n",
        "print(f\"  After exploding: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "\n",
        "# Encode labels\n",
        "gold_df_exploded[\"label\"] = gold_df_exploded[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "# Preprocess gold text\n",
        "print(f\"\\nPreprocessing gold segments...\")\n",
        "gold_df_exploded['processed_text'] = gold_df_exploded['Segment'].apply(preprocess_text)\n",
        "\n",
        "# Vectorize gold text\n",
        "print(f\"Vectorizing gold segments...\")\n",
        "X_gold = gold_df_exploded['processed_text'].values\n",
        "X_gold_tfidf = vectorizer.transform(X_gold)\n",
        "y_gold = gold_df_exploded['label'].values\n",
        "\n",
        "# Predict on gold using best model\n",
        "print(f\"Running inference on gold set using ...\")\n",
        "\n",
        "# Evaluate all models on gold data set\n",
        "results = []\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    # Predict on test set\n",
        "    y_pred_gold = model.predict(X_gold_tfidf)\n",
        "\n",
        "    # Compute metrics\n",
        "    gold_metrics = compute_metrics(y_gold, y_pred_gold)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': gold_metrics['accuracy'],\n",
        "        'Macro-F1': gold_metrics['macro_f1'],\n",
        "        'Negative F1': gold_metrics['neg_f1'],\n",
        "        'Positive F1': gold_metrics['pos_f1'],\n",
        "        'Training Time (s)': training_times[model_name]\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"GOLD SET RESULTS (Ranked by Macro-F1)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J17r0Hh-bvVg",
      "metadata": {
        "id": "J17r0Hh-bvVg"
      },
      "source": [
        "**Findings:**\n",
        "1. Best model: SVM (Linear)\n",
        "- Pro: Highest accuracy and Macro-F1 for Gold Standard Dataset\n",
        "- Cons: Longest training time compared to others\n",
        "\n",
        "2. Best model balance between training time and performance: Logistic Regression\n",
        "- Pro: Highest accuracy and Macro-F1 for Gold Standard Dataset, Low Training Time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196035b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "196035b6",
        "outputId": "18829432-f277-44c3-91bb-e36edbe1dc38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üèÜ BEST MODEL: SVM (Linear)\n",
            "‚òÖ GOLD TEST SET RESULTS - SVM (Linear) (Before Tuning)\n",
            "‚òÖ Total samples: 721 aspect-segment pairs\n",
            "======================================================================\n",
            "\n",
            "OVERALL PERFORMANCE:\n",
            "  Accuracy:  0.8363\n",
            "  Macro-F1:  0.8251\n",
            "    Negative F1: 0.7807\n",
            "    Positive F1: 0.8695\n",
            "\n",
            "‚ö†Ô∏è  Note: This is baseline performance before hyperparameter tuning\n"
          ]
        }
      ],
      "source": [
        "# Identify best model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_macro_f1 = results_df.iloc[0]['Macro-F1']\n",
        "\n",
        "# Store best model for hyperparameter tuning\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
        "\n",
        "# Best model prediction results\n",
        "y_pred_gold = best_model.predict(X_gold_tfidf)\n",
        "\n",
        "# Compute metrics\n",
        "gold_metrics = compute_metrics(y_gold, y_pred_gold)\n",
        "\n",
        "print(f\"‚òÖ GOLD TEST SET RESULTS - {best_model_name}\")\n",
        "print(f\"‚òÖ Total samples: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "print(f\"  Accuracy:  {gold_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:  {gold_metrics['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {gold_metrics['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {gold_metrics['pos_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Note: This is baseline performance before hyperparameter tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ca082d8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca082d8a",
        "outputId": "7ec37771-a05e-4afb-e366-60fc7a789356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PER-ASPECT BREAKDOWN (for comparison with BERT):\n",
            "Aspect                Samples   Accuracy   Macro-F1\n",
            "-------------------- -------- ---------- ----------\n",
            "AMBIENCE                   62     0.8226     0.7244\n",
            "AUTHENTICITY & LOCAL VIBE       16     0.8125     0.4483\n",
            "FOOD                      303     0.8383     0.8176\n",
            "HALAL COMPLIANCE            2     1.0000     1.0000\n",
            "LOCATION                   21     0.7619     0.7529\n",
            "LOYALTY (RETURN INTENT)       91     0.8022     0.7861\n",
            "NON-HALAL ELEMENTS         10     0.8000     0.7917\n",
            "SERVICE                   128     0.9062     0.9062\n",
            "VALUE                      88     0.7955     0.7950\n"
          ]
        }
      ],
      "source": [
        "# Per-aspect breakdown\n",
        "print(f\"\\nPER-ASPECT BREAKDOWN (for comparison with BERT):\")\n",
        "print(f\"{'Aspect':<20} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
        "print(f\"{'-'*20} {'-'*8} {'-'*10} {'-'*10}\")\n",
        "\n",
        "aspects_unique = sorted(gold_df_exploded[\"aspect\"].unique())\n",
        "for aspect in aspects_unique:\n",
        "    mask = gold_df_exploded[\"aspect\"] == aspect\n",
        "    y_aspect = y_gold[mask]\n",
        "    y_pred_aspect = y_pred_gold[mask]\n",
        "\n",
        "    try:\n",
        "        metrics = compute_metrics(y_aspect, y_pred_aspect)\n",
        "        n_samples = mask.sum()\n",
        "        print(f\"{aspect:<20} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
        "    except:\n",
        "        print(f\"{aspect:<20} {mask.sum():>8} {'N/A':>10} {'N/A':>10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "928fd992",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "928fd992",
        "outputId": "c064841a-560d-4785-f5a5-a5a85a8e481f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FULL CLASSIFICATION REPORT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.7985    0.7636    0.7807       275\n",
            "    Positive     0.8581    0.8812    0.8695       446\n",
            "\n",
            "    accuracy                         0.8363       721\n",
            "   macro avg     0.8283    0.8224    0.8251       721\n",
            "weighted avg     0.8353    0.8363    0.8356       721\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nFULL CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_gold, y_pred_gold, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "dgvHMbh0cd_z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgvHMbh0cd_z",
        "outputId": "c8b320d9-3ed9-4f89-b67e-3161ba996d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Saved TF-IDF Vectorizer to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/tfidf_vectorizer.pkl\n",
            "‚úÖ Saved model 'Logistic Regression' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/Logistic_Regression.pkl\n",
            "‚úÖ Saved model 'SVM (Linear)' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/SVM_(Linear).pkl\n",
            "‚úÖ Saved model 'Naive Bayes' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/Naive_Bayes.pkl\n",
            "‚úÖ Saved model 'XGBoost' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/XGBoost.pkl\n",
            "‚úÖ Saved model 'Random Forest' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/Random_Forest.pkl\n",
            "\n",
            "All saving operations completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "\n",
        "# Save the Vectorizer\n",
        "vectorizer_path = os.path.join(OUTPUT_PATH, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(vectorizer, vectorizer_path)\n",
        "print(f\"‚úÖ Saved TF-IDF Vectorizer to: {vectorizer_path}\")\n",
        "\n",
        "# Save each model from the 'trained_models' dictionary\n",
        "for model_name, model in trained_models.items():\n",
        "    # create a safe filename (replace spaces with underscores)\n",
        "    safe_name = model_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
        "    file_path = os.path.join(OUTPUT_PATH, f\"{safe_name}.pkl\")\n",
        "\n",
        "    # Save the model object\n",
        "    joblib.dump(model, file_path)\n",
        "    print(f\"‚úÖ Saved model '{model_name}' to: {file_path}\")\n",
        "\n",
        "print(\"\\nAll saving operations completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NQojnFHQaD34",
      "metadata": {
        "id": "NQojnFHQaD34"
      },
      "source": [
        "# Stage 9: VADER Baseline Comparison\n",
        "\n",
        "**Academic Justification:**\n",
        "- VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool\n",
        "- Establishes rule-based baseline for 3-way architecture comparison: Rule-Based (VADER) vs Traditional ML vs Deep Learning (BERT)\n",
        "- VADER requires no training - applies directly to raw text\n",
        "- Evaluates on **same data splits** (validation, test, gold) for fair comparison\n",
        "- Expected to underperform on Manglish code-switching (lacks cultural context and mixed-language support)\n",
        "- Demonstrates value of ML/DL approaches for low-resource languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "68MJfD4iaEZS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68MJfD4iaEZS",
        "outputId": "3f069353-a885-451f-d95c-b1b1ea248ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing VADER...\n",
            "‚úì VADER installed successfully\n",
            "\n",
            "======================================================================\n",
            "VADER SENTIMENT ANALYSIS - RULE-BASED BASELINE\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  VADER Characteristics:\n",
            "  ‚Ä¢ Rule-based lexicon approach (no training required)\n",
            "  ‚Ä¢ Designed for English social media text\n",
            "  ‚Ä¢ Returns compound score: [-1.0, 1.0]\n",
            "  ‚Ä¢ Decision rule: compound >= 0.05 ‚Üí positive, < 0.05 ‚Üí negative\n",
            "  ‚Ä¢ Expected challenge: Manglish code-switching (e.g., 'sedap', 'mamak')\n"
          ]
        }
      ],
      "source": [
        "# Install and import VADER\n",
        "try:\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    print(\"‚úì VADER already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing VADER...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'vaderSentiment'])\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    print(\"‚úì VADER installed successfully\")\n",
        "\n",
        "# Initialize VADER analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VADER SENTIMENT ANALYSIS - RULE-BASED BASELINE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  VADER Characteristics:\")\n",
        "print(\"  ‚Ä¢ Rule-based lexicon approach (no training required)\")\n",
        "print(\"  ‚Ä¢ Designed for English social media text\")\n",
        "print(\"  ‚Ä¢ Returns compound score: [-1.0, 1.0]\")\n",
        "print(\"  ‚Ä¢ Decision rule: compound >= 0.05 ‚Üí positive, < 0.05 ‚Üí negative\")\n",
        "print(\"  ‚Ä¢ Expected challenge: Manglish code-switching (e.g., 'sedap', 'mamak')\")\n",
        "\n",
        "def vader_predict(text):\n",
        "    \"\"\"\n",
        "    Apply VADER sentiment analysis and convert to binary classification.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text segment\n",
        "\n",
        "    Returns:\n",
        "        int: 0 for negative, 1 for positive\n",
        "\n",
        "    Why compound score?\n",
        "        VADER's compound score normalizes sentiment across text length.\n",
        "        Standard threshold: >= 0.05 is positive, < 0.05 is negative.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return 1  # Default to positive if empty\n",
        "\n",
        "    scores = vader_analyzer.polarity_scores(text)\n",
        "    compound = scores['compound']\n",
        "\n",
        "    # Binary classification using standard threshold\n",
        "    return 1 if compound >= 0.05 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "BV--7N2maLHF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV--7N2maLHF",
        "outputId": "0ff1455a-6657-4b2b-cb69-0144f47b554f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "VADER EVALUATION ON VALIDATION SET\n",
            "======================================================================\n",
            "\n",
            "Applying VADER to 9,864 validation segments...\n",
            "\n",
            "‚òÖ VADER VALIDATION SET RESULTS:\n",
            "  Accuracy:    0.5314\n",
            "  Macro-F1:    0.4420\n",
            "    Negative F1: 0.2187\n",
            "    Positive F1: 0.6654\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.1235    0.9543    0.2187       678\n",
            "    Positive     0.9933    0.5002    0.6654      9186\n",
            "\n",
            "    accuracy                         0.5314      9864\n",
            "   macro avg     0.5584    0.7272    0.4420      9864\n",
            "weighted avg     0.9335    0.5314    0.6347      9864\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"VADER EVALUATION ON VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use ORIGINAL text (not preprocessed) for VADER\n",
        "X_val_original = df_val['Segment'].values\n",
        "\n",
        "print(f\"\\nApplying VADER to {len(X_val_original):,} validation segments...\")\n",
        "y_pred_val_vader = np.array([vader_predict(text) for text in X_val_original])\n",
        "\n",
        "# Compute metrics\n",
        "val_metrics_vader = compute_metrics(y_val, y_pred_val_vader)\n",
        "\n",
        "print(f\"\\n‚òÖ VADER VALIDATION SET RESULTS:\")\n",
        "print(f\"  Accuracy:    {val_metrics_vader['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:    {val_metrics_vader['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {val_metrics_vader['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {val_metrics_vader['pos_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred_val_vader, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "Kiz-ZklMaNIW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiz-ZklMaNIW",
        "outputId": "3f55dcbb-f54c-44fd-8577-b7bb63a4abdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "VADER EVALUATION ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "Applying VADER to 14,795 test segments...\n",
            "\n",
            "‚òÖ VADER TEST SET RESULTS:\n",
            "  Accuracy:    0.5350\n",
            "  Macro-F1:    0.4448\n",
            "    Negative F1: 0.2210\n",
            "    Positive F1: 0.6686\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.1249    0.9606    0.2210      1016\n",
            "    Positive     0.9943    0.5037    0.6686     13779\n",
            "\n",
            "    accuracy                         0.5350     14795\n",
            "   macro avg     0.5596    0.7321    0.4448     14795\n",
            "weighted avg     0.9346    0.5350    0.6379     14795\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"VADER EVALUATION ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use ORIGINAL text (not preprocessed) for VADER\n",
        "X_test_original = df_test['Segment'].values\n",
        "\n",
        "print(f\"\\nApplying VADER to {len(X_test_original):,} test segments...\")\n",
        "y_pred_test_vader = np.array([vader_predict(text) for text in X_test_original])\n",
        "\n",
        "# Compute metrics\n",
        "test_metrics_vader = compute_metrics(y_test, y_pred_test_vader)\n",
        "\n",
        "print(f\"\\n‚òÖ VADER TEST SET RESULTS:\")\n",
        "print(f\"  Accuracy:    {test_metrics_vader['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:    {test_metrics_vader['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {test_metrics_vader['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {test_metrics_vader['pos_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_test_vader, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5Aj3OXGXbWew",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aj3OXGXbWew",
        "outputId": "3442099d-52ce-4597-a06d-af78a86ea6a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "VADER EVALUATION ON GOLD STANDARD\n",
            "======================================================================\n",
            "\n",
            "Applying VADER to 721 gold standard segments...\n",
            "\n",
            "‚òÖ VADER GOLD STANDARD RESULTS:\n",
            "‚òÖ Total samples: 721 aspect-segment pairs\n",
            "\n",
            "OVERALL PERFORMANCE:\n",
            "  Accuracy:  0.7351\n",
            "  Macro-F1:  0.7336\n",
            "    Negative F1: 0.7136\n",
            "    Positive F1: 0.7535\n",
            "\n",
            "PER-ASPECT BREAKDOWN:\n",
            "Aspect                Samples   Accuracy   Macro-F1\n",
            "-------------------- -------- ---------- ----------\n",
            "AMBIENCE                   62     0.7903     0.7569\n",
            "AUTHENTICITY & LOCAL VIBE       16     0.6250     0.6000\n",
            "FOOD                      303     0.7393     0.7342\n",
            "HALAL COMPLIANCE            2     0.5000     0.3333\n",
            "LOCATION                   21     0.7143     0.7083\n",
            "LOYALTY (RETURN INTENT)       91     0.6923     0.6859\n",
            "NON-HALAL ELEMENTS         10     0.7000     0.6970\n",
            "SERVICE                   128     0.7109     0.7069\n",
            "VALUE                      88     0.7955     0.7928\n",
            "\n",
            "FULL CLASSIFICATION REPORT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.6071    0.8655    0.7136       275\n",
            "    Positive     0.8875    0.6547    0.7535       446\n",
            "\n",
            "    accuracy                         0.7351       721\n",
            "   macro avg     0.7473    0.7601    0.7336       721\n",
            "weighted avg     0.7806    0.7351    0.7383       721\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"VADER EVALUATION ON GOLD STANDARD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use ORIGINAL gold segments\n",
        "X_gold_original = gold_df_exploded['Segment'].values\n",
        "y_gold = gold_df_exploded['label'].values\n",
        "\n",
        "print(f\"\\nApplying VADER to {len(X_gold_original):,} gold standard segments...\")\n",
        "y_pred_gold_vader = np.array([vader_predict(text) for text in X_gold_original])\n",
        "\n",
        "# Compute metrics\n",
        "gold_metrics_vader = compute_metrics(y_gold, y_pred_gold_vader)\n",
        "\n",
        "print(f\"\\n‚òÖ VADER GOLD STANDARD RESULTS:\")\n",
        "print(f\"‚òÖ Total samples: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "print(f\"  Accuracy:  {gold_metrics_vader['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:  {gold_metrics_vader['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {gold_metrics_vader['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {gold_metrics_vader['pos_f1']:.4f}\")\n",
        "\n",
        "# Per-aspect breakdown\n",
        "print(f\"\\nPER-ASPECT BREAKDOWN:\")\n",
        "print(f\"{'Aspect':<20} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
        "print(f\"{'-'*20} {'-'*8} {'-'*10} {'-'*10}\")\n",
        "\n",
        "aspects_unique = sorted(gold_df_exploded[\"aspect\"].unique())\n",
        "for aspect in aspects_unique:\n",
        "    mask = gold_df_exploded[\"aspect\"] == aspect\n",
        "    y_aspect = y_gold[mask]\n",
        "    y_pred_aspect = y_pred_gold_vader[mask]\n",
        "\n",
        "    try:\n",
        "        metrics = compute_metrics(y_aspect, y_pred_aspect)\n",
        "        n_samples = mask.sum()\n",
        "        print(f\"{aspect:<20} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
        "    except:\n",
        "        print(f\"{aspect:<20} {mask.sum():>8} {'N/A':>10} {'N/A':>10}\")\n",
        "\n",
        "print(f\"\\nFULL CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_gold, y_pred_gold_vader, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fpGgIu4Hbig8",
      "metadata": {
        "id": "fpGgIu4Hbig8"
      },
      "source": [
        "# Stage 12: Three-Way Architecture Comparison\n",
        "\n",
        "**Comparison Framework:**\n",
        "- **VADER (Rule-Based)**: Lexicon + rules, no training, English-centric\n",
        "- **Traditional ML (Tuned)**: TF-IDF + SVM/Logistic Regression, supervised learning\n",
        "- **XLM-RoBERTa (BERT)**: Transformer with cross-lingual pre-training, contextual embeddings\n",
        "\n",
        "**Evaluation Protocol:**\n",
        "- Same data splits (validation, test, gold standard)\n",
        "- Same metrics (accuracy, macro-F1, negative F1, positive F1)\n",
        "- Gold standard as final ground truth for thesis comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BJVyfj-MbjCQ",
      "metadata": {
        "id": "BJVyfj-MbjCQ"
      },
      "outputs": [],
      "source": [
        "# print(\"=\"*70)\n",
        "# print(\"THREE-WAY MODEL COMPARISON\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Create comprehensive comparison table\n",
        "# comparison_data = []\n",
        "\n",
        "# # VADER (Rule-Based)\n",
        "# comparison_data.append({\n",
        "#     'Model': 'VADER (Rule-Based)',\n",
        "#     'Architecture': 'Lexicon + Rules',\n",
        "#     'Training': 'None',\n",
        "#     'Val Accuracy': val_metrics_vader['accuracy'],\n",
        "#     'Val Macro-F1': val_metrics_vader['macro_f1'],\n",
        "#     'Test Accuracy': test_metrics_vader['accuracy'],\n",
        "#     'Test Macro-F1': test_metrics_vader['macro_f1'],\n",
        "#     'Gold Accuracy': gold_metrics_vader['accuracy'],\n",
        "#     'Gold Macro-F1': gold_metrics_vader['macro_f1'],\n",
        "#     'Gold Neg F1': gold_metrics_vader['neg_f1'],\n",
        "#     'Gold Pos F1': gold_metrics_vader['pos_f1']\n",
        "# })\n",
        "\n",
        "# # Traditional ML (Tuned)\n",
        "# comparison_data.append({\n",
        "#     'Model': f'Traditional ML ({best_model_name})',\n",
        "#     'Architecture': 'TF-IDF + ML',\n",
        "#     'Training': f'{len(X_train):,} samples',\n",
        "#     'Val Accuracy': val_metrics['accuracy'],  # From Stage 6\n",
        "#     'Val Macro-F1': val_metrics['macro_f1'],\n",
        "#     'Test Accuracy': test_metrics_tuned['accuracy'],\n",
        "#     'Test Macro-F1': test_metrics_tuned['macro_f1'],\n",
        "#     'Gold Accuracy': gold_metrics_tuned['accuracy'],\n",
        "#     'Gold Macro-F1': gold_metrics_tuned['macro_f1'],\n",
        "#     'Gold Neg F1': gold_metrics_tuned['neg_f1'],\n",
        "#     'Gold Pos F1': gold_metrics_tuned['pos_f1']\n",
        "# })\n",
        "\n",
        "# # XLM-RoBERTa (from your completed BERT training)\n",
        "# # Note: Update these values with your actual BERT results\n",
        "# comparison_data.append({\n",
        "#     'Model': 'XLM-RoBERTa (BERT)',\n",
        "#     'Architecture': 'Transformer',\n",
        "#     'Training': f'{len(X_train):,} samples',\n",
        "#     'Val Accuracy': 0.0000,  # ‚Üê UPDATE with your BERT val accuracy\n",
        "#     'Val Macro-F1': 0.0000,  # ‚Üê UPDATE with your BERT val macro-F1\n",
        "#     'Test Accuracy': 0.0000,  # ‚Üê UPDATE with your BERT test accuracy\n",
        "#     'Test Macro-F1': 0.0000,  # ‚Üê UPDATE with your BERT test macro-F1\n",
        "#     'Gold Accuracy': 0.9240,  # Your reported BERT gold accuracy\n",
        "#     'Gold Macro-F1': 0.9170,  # Your reported BERT gold macro-F1\n",
        "#     'Gold Neg F1': 0.8940,    # Your reported BERT negative F1\n",
        "#     'Gold Pos F1': 0.9340     # Your reported BERT positive F1\n",
        "# })\n",
        "\n",
        "# comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(f\"COMPARISON TABLE: GOLD STANDARD (GROUND TRUTH)\")\n",
        "# print(f\"{'='*70}\\n\")\n",
        "\n",
        "# # Display gold standard comparison (most important)\n",
        "# gold_comparison = comparison_df[['Model', 'Architecture', 'Gold Accuracy', 'Gold Macro-F1', 'Gold Neg F1', 'Gold Pos F1']]\n",
        "# print(gold_comparison.to_string(index=False))\n",
        "\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(f\"FULL COMPARISON TABLE: ALL EVALUATION SETS\")\n",
        "# print(f\"{'='*70}\\n\")\n",
        "# print(comparison_df.to_string(index=False))\n",
        "\n",
        "# # Summary insights\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(f\"KEY FINDINGS\")\n",
        "# print(f\"{'='*70}\")\n",
        "\n",
        "# vader_gold_f1 = gold_metrics_vader['macro_f1']\n",
        "# ml_gold_f1 = gold_metrics_tuned['macro_f1']\n",
        "# bert_gold_f1 = 0.9170  # Update with your actual BERT value\n",
        "\n",
        "# print(f\"\\nüìä Gold Standard Macro-F1 Performance:\")\n",
        "# print(f\"  1. XLM-RoBERTa (BERT):     {bert_gold_f1:.4f} ü•á\")\n",
        "# print(f\"  2. Traditional ML:         {ml_gold_f1:.4f} ü•à\")\n",
        "# print(f\"  3. VADER (Rule-Based):     {vader_gold_f1:.4f} ü•â\")\n",
        "\n",
        "# improvement_ml_vs_vader = ((ml_gold_f1 - vader_gold_f1) / vader_gold_f1) * 100\n",
        "# improvement_bert_vs_ml = ((bert_gold_f1 - ml_gold_f1) / ml_gold_f1) * 100\n",
        "\n",
        "# print(f\"\\nüìà Relative Improvements:\")\n",
        "# print(f\"  Traditional ML vs VADER:   +{improvement_ml_vs_vader:.1f}%\")\n",
        "# print(f\"  BERT vs Traditional ML:    +{improvement_bert_vs_ml:.1f}%\")\n",
        "\n",
        "# print(f\"\\nüí° Implications:\")\n",
        "# print(f\"  ‚úì Rule-based (VADER) struggles with Manglish code-switching\")\n",
        "# print(f\"  ‚úì Traditional ML benefits from supervised learning on domain data\")\n",
        "# print(f\"  ‚úì BERT's cross-lingual pre-training provides significant advantage\")\n",
        "# print(f\"  ‚úì Transformer architecture captures contextual nuances in code-switched text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D8-6f5JZbme-",
      "metadata": {
        "id": "D8-6f5JZbme-"
      },
      "outputs": [],
      "source": [
        "# # Visualization: Gold Standard Comparison\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# # Plot 1: Macro-F1 Comparison\n",
        "# models = ['VADER\\n(Rule-Based)', f'Traditional ML\\n({best_model_name})', 'XLM-RoBERTa\\n(BERT)']\n",
        "# gold_f1_scores = [\n",
        "#     gold_metrics_vader['macro_f1'],\n",
        "#     gold_metrics_tuned['macro_f1'],\n",
        "#     0.9170  # Update with your actual BERT value\n",
        "# ]\n",
        "\n",
        "# colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
        "# bars = axes[0].bar(models, gold_f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "# axes[0].set_ylabel('Macro-F1 Score', fontsize=12, fontweight='bold')\n",
        "# axes[0].set_title('Gold Standard: Macro-F1 Comparison', fontsize=14, fontweight='bold')\n",
        "# axes[0].set_ylim([0, 1.0])\n",
        "# axes[0].axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, label='Strong Performance (0.8)')\n",
        "# axes[0].grid(axis='y', alpha=0.3)\n",
        "# axes[0].legend()\n",
        "\n",
        "# # Add value labels on bars\n",
        "# for bar in bars:\n",
        "#     height = bar.get_height()\n",
        "#     axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "#                 f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# # Plot 2: Per-Class F1 Comparison\n",
        "# class_labels = ['Negative F1', 'Positive F1']\n",
        "# x = np.arange(len(class_labels))\n",
        "# width = 0.25\n",
        "\n",
        "# vader_scores = [gold_metrics_vader['neg_f1'], gold_metrics_vader['pos_f1']]\n",
        "# ml_scores = [gold_metrics_tuned['neg_f1'], gold_metrics_tuned['pos_f1']]\n",
        "# bert_scores = [0.8940, 0.9340]  # Update with your actual BERT values\n",
        "\n",
        "# axes[1].bar(x - width, vader_scores, width, label='VADER', color=colors[0], alpha=0.8, edgecolor='black')\n",
        "# axes[1].bar(x, ml_scores, width, label=f'Traditional ML', color=colors[1], alpha=0.8, edgecolor='black')\n",
        "# axes[1].bar(x + width, bert_scores, width, label='BERT', color=colors[2], alpha=0.8, edgecolor='black')\n",
        "\n",
        "# axes[1].set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
        "# axes[1].set_title('Gold Standard: Per-Class F1 Comparison', fontsize=14, fontweight='bold')\n",
        "# axes[1].set_xticks(x)\n",
        "# axes[1].set_xticklabels(class_labels, fontsize=11)\n",
        "# axes[1].set_ylim([0, 1.0])\n",
        "# axes[1].legend(fontsize=10)\n",
        "# axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# print(f\"\\n‚úì Visualization complete: Gold Standard performance comparison\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
