{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eCJk5vDPVr_5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCJk5vDPVr_5",
        "outputId": "bebd6ca4-7644-44c3-d194-28d6d97a78ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis\n",
            "ðŸ”„ Updating your repo...\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Aspect-Based-Sentiment-Analysis\n",
            "Already up to date.\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Aspect-Based-Sentiment-Analysis\n",
            "\n",
            "READY! We are now running on Google's Computer.\n"
          ]
        }
      ],
      "source": [
        "# cell 1\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive (To save the model checkpoints)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Clone/Pull the Git Repo\n",
        "REPO_URL = \"https://github.com/HuiLing990511/Aspect-Based-Sentiment-Analysis\"\n",
        "REPO_NAME = \"Aspect-Based-Sentiment-Analysis\"\n",
        "PROJECT_PATH = f\"/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis\"\n",
        "\n",
        "if not os.path.exists(PROJECT_PATH):\n",
        "    os.makedirs(PROJECT_PATH)\n",
        "\n",
        "%cd {PROJECT_PATH}\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(\"ðŸš€ Cloning your repo...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(\"ðŸ”„ Updating your repo...\")\n",
        "    %cd {REPO_NAME}\n",
        "    !git pull\n",
        "    %cd ..\n",
        "\n",
        "# 3. Enter the repo folder\n",
        "%cd {REPO_NAME}\n",
        "\n",
        "# 4. Install Libraries \n",
        "!pip install transformers accelerate tokenizers -q\n",
        "\n",
        "print(\"\\nREADY! We are now running on Google's Computer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ca1b1d4",
      "metadata": {
        "id": "1ca1b1d4",
        "outputId": "ccb15944-19bf-4f25-cfc5-554e6c1a4406"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' \\n================================================================================\\nXLM-RoBERTa Aspect-Based Sentiment Classification\\n================================================================================\\nPROJECT : NLP-Driven ABSA for Gastronomy Tourism Insights in Malaysia\\nPIPELINE: Pipelined-ABSA (Decoupled) â€” Step 3: Sentiment Classification\\nINPUT   : Dataset/aspect_categorization.pkl   (output of Notebook 5)\\nOUTPUT  : models/xlm_roberta_absa_best.pt     (best checkpoint)\\n          results/training_metrics.json        (loss/acc curves)\\n\\nACADEMIC JUSTIFICATION\\n----------------------\\n- XLM-RoBERTa (Conneau et al., 2020): Pre-trained on 100 languages including\\n  Malay and Chinese. Superior zero/few-shot cross-lingual transfer vs.\\n  monolingual BERT, critical for Manglish code-switching.\\n- Aspect-Conditioned Input (Sun et al., 2019): We prepend the aspect category\\n  to the segment text as \"[aspect] [SEP] [segment]\". This forces the model to\\n  learn aspect-specific sentiment representations rather than general polarity.\\n- Class-Weighted Loss (Japkowicz & Stephen, 2002): Our dataset is severely\\n  imbalanced (89% positive). We use the inverse-frequency weights computed in\\n  Notebook 4 to prevent the model from trivially predicting \"positive\".\\n- Weak Supervision (Ratner et al., 2016): Star ratings are noisy proxies for\\n  sentiment. The consistency filtering in Notebook 4 already removed the worst\\n  offenders (4.1% noise). Residual noise is tolerable for fine-tuning.\\n================================================================================\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "================================================================================\n",
        "XLM-RoBERTa Aspect-Based Sentiment Classification\n",
        "================================================================================\n",
        "PROJECT : NLP-Driven ABSA for Gastronomy Tourism Insights in Malaysia\n",
        "PIPELINE: Pipelined-ABSA (Decoupled) â€” Step 3: Sentiment Classification\n",
        "INPUT   : Dataset/aspect_categorization.pkl   (output of Notebook 5)\n",
        "OUTPUT  : models/xlm_roberta_absa_best.pt     (best checkpoint)\n",
        "          results/training_metrics.json        (loss/acc curves)\n",
        "\n",
        "ACADEMIC JUSTIFICATION\n",
        "----------------------\n",
        "- XLM-RoBERTa (Conneau et al., 2020): Pre-trained on 100 languages including\n",
        "  Malay and Chinese. Superior zero/few-shot cross-lingual transfer vs.\n",
        "  monolingual BERT, critical for Manglish code-switching.\n",
        "- Aspect-Conditioned Input (Sun et al., 2019): We prepend the aspect category\n",
        "  to the segment text as \"[aspect] [SEP] [segment]\". This forces the model to\n",
        "  learn aspect-specific sentiment representations rather than general polarity.\n",
        "- Class-Weighted Loss (Japkowicz & Stephen, 2002): Our dataset is severely\n",
        "  imbalanced (89% positive). We use the inverse-frequency weights computed in\n",
        "  Notebook 4 to prevent the model from trivially predicting \"positive\".\n",
        "- Weak Supervision (Ratner et al., 2016): Star ratings are noisy proxies for\n",
        "  sentiment. The consistency filtering in Notebook 4 already removed the worst\n",
        "  offenders (4.1% noise). Residual noise is tolerable for fine-tuning.\n",
        "================================================================================\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18ea3c5",
      "metadata": {
        "id": "b18ea3c5"
      },
      "source": [
        "# STAGE 0: Environment & Dependency Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f2fb06",
      "metadata": {
        "id": "c4f2fb06"
      },
      "outputs": [],
      "source": [
        "# pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4fec26",
      "metadata": {
        "id": "7a4fec26"
      },
      "outputs": [],
      "source": [
        "# Fix PyTorch DLL loading issue on Windows\n",
        "import os\n",
        "import platform\n",
        "if platform.system() == \"Windows\":\n",
        "    import ctypes\n",
        "    from importlib.util import find_spec\n",
        "    try:\n",
        "        if (spec := find_spec(\"torch\")) and spec.origin and os.path.exists(\n",
        "            dll_path := os.path.join(os.path.dirname(spec.origin), \"lib\", \"c10.dll\")\n",
        "        ):\n",
        "            ctypes.CDLL(os.path.normpath(dll_path))\n",
        "    except Exception:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a870b8d3",
      "metadata": {
        "id": "a870b8d3",
        "outputId": "67149857-b061-4410-f37a-0a776bae560f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ENVIRONMENT CHECK\n",
            "======================================================================\n",
            "  âœ“  PyTorch                        v2.8.0+cpu\n",
            "  âœ“  HuggingFace Transformers       v4.41.2\n",
            "  âœ“  Pandas                         v2.2.3\n",
            "  âœ“  NumPy                          v2.2.0\n",
            "  âœ“  Scikit-Learn                   v1.5.1\n",
            "\n",
            "  GPU Available: False  â†’  CPU only\n",
            "  Python:        3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import importlib\n",
        "\n",
        "REQUIRED = {\n",
        "    \"torch\": \"PyTorch\",\n",
        "    \"transformers\": \"HuggingFace Transformers\",\n",
        "    \"pandas\": \"Pandas\",\n",
        "    \"numpy\": \"NumPy\",\n",
        "    \"sklearn\": \"Scikit-Learn\",\n",
        "}\n",
        "\n",
        "\n",
        "def check_environment():\n",
        "    \"\"\"Verify all required packages are installed and print versions.\n",
        "\n",
        "    Why:\n",
        "        Explicit environment checks prevent cryptic import errors mid-training,\n",
        "        which is especially costly when running on GPU with long epoch times.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ENVIRONMENT CHECK\")\n",
        "    print(\"=\" * 70)\n",
        "    all_ok = True\n",
        "    for module_name, display_name in REQUIRED.items():\n",
        "        try:\n",
        "            mod = importlib.import_module(module_name)\n",
        "            version = getattr(mod, \"__version__\", \"unknown\")\n",
        "            print(f\"  âœ“  {display_name:<30} v{version}\")\n",
        "        except ImportError:\n",
        "            print(f\"  âœ—  {display_name:<30} NOT INSTALLED\")\n",
        "            all_ok = False\n",
        "\n",
        "    # Special check: torch CUDA availability\n",
        "    import torch\n",
        "\n",
        "    cuda_avail = torch.cuda.is_available()\n",
        "    device_name = torch.cuda.get_device_name(0) if cuda_avail else \"CPU only\"\n",
        "    print(f\"\\n  GPU Available: {cuda_avail}  â†’  {device_name}\")\n",
        "    print(f\"  Python:        {sys.version}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if not all_ok:\n",
        "        raise RuntimeError(\n",
        "            \"Some packages are missing. Install them before continuing.\"\n",
        "        )\n",
        "    return torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
        "\n",
        "\n",
        "DEVICE = check_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d863039",
      "metadata": {
        "id": "5d863039"
      },
      "source": [
        "# STAGE 1: Hyperparameter Configuration (Single Source of Truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5f8a24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5f8a24",
        "outputId": "2d32863d-bf51-4507-bfa9-fca6f96e80d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Config loaded. Model: xlm-roberta-base | Epochs: 5 | Batch: 32 | LR: 2e-05\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Central configuration object for the entire training run.\n",
        "\n",
        "    Why a dataclass:\n",
        "        Keeps hyperparameters serializable (can be logged to JSON for\n",
        "        reproducibility) and gives IDE auto-completion â€” important when\n",
        "        iterating quickly on a GPU budget.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Model -----------------------------------------------------------\n",
        "    model_name: str = \"xlm-roberta-base\"\n",
        "    # \"xlm-roberta-base\" (278M params) is the default.\n",
        "    # Switch to \"xlm-roberta-large\" (550M) if GPU memory allows (â‰¥16 GB).\n",
        "\n",
        "    num_labels: int = 2  # 0 = negative, 1 = positive\n",
        "\n",
        "    # --- Data ------------------------------------------------------------\n",
        "    data_path: str = r\"Dataset/aspect_categorization.pkl\"\n",
        "    max_seq_length: int = 128\n",
        "    # 128 covers ~95th percentile of your segment lengths (median ~54 words).\n",
        "    # Increase to 256 only if you see significant truncation in logs.\n",
        "\n",
        "    test_size: float = 0.15  # 15% held out for evaluation\n",
        "    val_size: float = 0.10   # 10% for early-stopping validation\n",
        "    random_seed: int = 42\n",
        "\n",
        "    # --- Training --------------------------------------------------------\n",
        "    batch_size: int = 32       # TO-DO: Reduce to 16 if OOM on your GPU\n",
        "    learning_rate: float = 2e-5  # Classic BERT fine-tuning sweet spot\n",
        "    num_epochs: int = 5\n",
        "    warmup_ratio: float = 0.1  # 10% of total steps used for LR warm-up\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # --- Class Weights (computed dynamically from training data) ---------\n",
        "    # Will be computed via sklearn.utils.class_weight.compute_class_weight\n",
        "    # Formula: n_samples / (n_classes * np.bincount(y))\n",
        "    # This ensures minority class gets higher weight to balance gradients\n",
        "    class_weights: Optional[List[float]] = None\n",
        "\n",
        "    # --- Output ----------------------------------------------------------\n",
        "    output_dir: str = \"models\"\n",
        "    best_model_path: str = \"models/xlm_roberta_absa_best.pt\"\n",
        "    metrics_path: str = \"results/training_metrics.json\"\n",
        "\n",
        "    # --- Gold Standard Dataset (for final evaluation) --------------------\n",
        "    # Manually-annotated ground truth for thesis results\n",
        "    gold_data_path: str = r\"Dataset/Final_Gold_Standard.csv\"\n",
        "    gold_results_path: str = \"results/gold_evaluation.json\"\n",
        "\n",
        "\n",
        "CFG = TrainingConfig()\n",
        "print(f\"\\nâœ“ Config loaded. Model: {CFG.model_name} | Epochs: {CFG.num_epochs} | \"\n",
        "      f\"Batch: {CFG.batch_size} | LR: {CFG.learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be7a186",
      "metadata": {
        "id": "0be7a186"
      },
      "source": [
        "# STAGE 2: Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9546dc82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9546dc82",
        "outputId": "6f2f70c7-fba3-4cdf-b851-f74efd63dd1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING & SPLITTING DATA\n",
            "======================================================================\n",
            "  Raw segments loaded: 132,637\n",
            "  Single-aspect segments (for training): 100,557\n",
            "  Multi-aspect segments (dropped for training): 32,080\n",
            "\n",
            "  Label distribution:\n",
            "label\n",
            "  0 (negative)     7673\n",
            "  1 (positive)    92884\n",
            "\n",
            "  Split sizes:\n",
            "    train :  75,417 rows | pos: 69,662 (92.4%) | neg: 5,755 (7.6%)\n",
            "    val   :  10,056 rows | pos: 9,289 (92.4%) | neg: 767 (7.6%)\n",
            "    test  :  15,084 rows | pos: 13,933 (92.4%) | neg: 1,151 (7.6%)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Transforms aspect_categorization.pkl â†’ train/val/test splits\n",
        "# ready for the PyTorch DataLoader.\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Label encoding map (explicit is better than implicit)\n",
        "LABEL2ID = {\"negative\": 0, \"positive\": 1}\n",
        "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "\n",
        "def load_and_prepare_data(cfg: TrainingConfig) -> dict:\n",
        "    \"\"\"Load aspect segments and split into train / val / test.\n",
        "\n",
        "    Why stratified split:\n",
        "        With 89/11 class imbalance, a random split can accidentally create\n",
        "        a val/test set with zero or near-zero negative samples. Stratification\n",
        "        guarantees each split mirrors the overall class ratio.\n",
        "\n",
        "    Why we filter out multi-aspect segments for training:\n",
        "        When a segment maps to multiple aspects (e.g., [FOOD, AMBIENCE]),\n",
        "        the weak label (derived from the whole review) is even noisier for\n",
        "        that segment. We keep only single-aspect segments for cleaner\n",
        "        supervision signal. Multi-aspect segments can still be predicted\n",
        "        at inference time.\n",
        "\n",
        "    Args:\n",
        "        cfg: TrainingConfig instance.\n",
        "\n",
        "    Returns:\n",
        "        Dict with keys: 'train', 'val', 'test' â€” each a DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LOADING & SPLITTING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df = pd.read_pickle(cfg.data_path)\n",
        "    print(f\"  Raw segments loaded: {len(df):,}\")\n",
        "\n",
        "    # --- Filter to single-aspect segments for cleaner weak supervision ----\n",
        "    df[\"num_aspects\"] = df[\"Aspect_Labels\"].apply(len)\n",
        "    df_single = df[df[\"num_aspects\"] == 1].copy()\n",
        "    df_single[\"aspect\"] = df_single[\"Aspect_Labels\"].apply(lambda x: x[0])\n",
        "    \n",
        "    n_multi = len(df) - len(df_single)\n",
        "    pct_retained = (len(df_single) / len(df)) * 100\n",
        "    \n",
        "    print(f\"\\n  âš ï¸  FILTERING STRATEGY (for training only):\")\n",
        "    print(f\"    Single-aspect segments:  {len(df_single):>7,} ({pct_retained:>5.1f}%) â†’ KEPT for training\")\n",
        "    print(f\"    Multi-aspect segments:   {n_multi:>7,} ({100-pct_retained:>5.1f}%) â†’ DROPPED from training\")\n",
        "    print(f\"    Segments remaining:      {len(df_single):>7,}\")\n",
        "    print(f\"\\n  ðŸ“Š For inference/visualization: Use the FULL dataset (all segments)\")\n",
        "    print(f\"     including multi-aspect ones to get complete review coverage.\")\n",
        "\n",
        "    # --- Encode labels ---------------------------------------------------\n",
        "    df_single[\"label\"] = df_single[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "    # Sanity check: no NaN labels\n",
        "    assert df_single[\"label\"].isna().sum() == 0, (\n",
        "        \"Found NaN labels! Check Sentiment_Label column values.\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  Label distribution:\")\n",
        "    print(df_single[\"label\"].value_counts().sort_index().to_string(\n",
        "        index=True).replace(\"0\", \"  0 (negative)\").replace(\"1\", \"  1 (positive)\")\n",
        "    )\n",
        "\n",
        "    # --- Train / Val / Test split (two-stage stratified) -----------------\n",
        "    # Stage 1: Separate test set\n",
        "    df_trainval, df_test = train_test_split(\n",
        "        df_single,\n",
        "        test_size=cfg.test_size,\n",
        "        stratify=df_single[\"label\"],\n",
        "        random_state=cfg.random_seed,\n",
        "    )\n",
        "    # Stage 2: Split remainder into train + val\n",
        "    # Adjust val_size relative to the remaining data\n",
        "    adjusted_val_size = cfg.val_size / (1.0 - cfg.test_size)\n",
        "    df_train, df_val = train_test_split(\n",
        "        df_trainval,\n",
        "        test_size=adjusted_val_size,\n",
        "        stratify=df_trainval[\"label\"],\n",
        "        random_state=cfg.random_seed,\n",
        "    )\n",
        "\n",
        "    splits = {\"train\": df_train, \"val\": df_val, \"test\": df_test}\n",
        "\n",
        "    print(f\"\\n  Split sizes:\")\n",
        "    for name, split_df in splits.items():\n",
        "        pos = (split_df[\"label\"] == 1).sum()\n",
        "        neg = (split_df[\"label\"] == 0).sum()\n",
        "        print(f\"    {name:<6}: {len(split_df):>7,} rows \"\n",
        "              f\"| pos: {pos:,} ({pos/len(split_df)*100:.1f}%) \"\n",
        "              f\"| neg: {neg:,} ({neg/len(split_df)*100:.1f}%)\")\n",
        "\n",
        "    # --- Compute Class Weights Dynamically -------------------------------\n",
        "    # Using inverse frequency weighting: n_samples / (n_classes * np.bincount(y))\n",
        "    # This is the standard approach from sklearn and research literature\n",
        "    print(f\"\\n  Computing class weights from training data...\")\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(df_train[\"label\"]),\n",
        "        y=df_train[\"label\"]\n",
        "    )\n",
        "    cfg.class_weights = class_weights.tolist()\n",
        "    \n",
        "    print(f\"  Class weights (balanced):\")\n",
        "    for class_id, weight in enumerate(cfg.class_weights):\n",
        "        print(f\"    {ID2LABEL[class_id]:<10} (class {class_id}): {weight:.4f}\")\n",
        "    \n",
        "    print(f\"\\n  Interpretation:\")\n",
        "    print(f\"    Higher weight = minority class â†’ model penalized more for errors\")\n",
        "    print(f\"    Formula: n_samples / (n_classes Ã— count_per_class)\")\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "DATA = load_and_prepare_data(CFG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f802384a",
      "metadata": {
        "id": "f802384a"
      },
      "source": [
        "# STAGE 3: PyTorch Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68796217",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68796217",
        "outputId": "7525bcfe-b7d3-43cf-f0ea-41e30743a0d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING DATALOADERS\n",
            "======================================================================\n",
            "  âœ“ Tokenizer loaded: xlm-roberta-base\n",
            "    Vocab size: 250,002\n",
            "\n",
            "  Dataset sizes & batches:\n",
            "    train :  75,417 samples â†’ 2,357 batches (batch_size=32)\n",
            "    val   :  10,056 samples â†’  315 batches (batch_size=32)\n",
            "    test  :  15,084 samples â†’  472 batches (batch_size=32)\n",
            "\n",
            "  Sample input (decoded):\n",
            "    \"<s> FOOD</s></s> cooked to perfection</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\"\n",
            "    Label: positive\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Wraps a DataFrame split into a tokenized, aspect-conditioned dataset.\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    \"\"\"Aspect-conditioned sentiment dataset for XLM-RoBERTa.\n",
        "\n",
        "    Input Format (Sun et al., 2019 â€” Aspect-Based Sentiment):\n",
        "        Input:  \"[aspect] </s></s> [segment text]\"\n",
        "        Label:  0 (negative) or 1 (positive)\n",
        "\n",
        "    Why \"</s></s>\" (double SEP):\n",
        "        XLM-RoBERTa uses </s> as its separator token (unlike BERT's [SEP]).\n",
        "        The double </s></s> pattern is the standard way RoBERTa-family models\n",
        "        denote a sentence boundary â€” this is baked into its pre-training.\n",
        "\n",
        "    Example:\n",
        "        Input text:  \"FOOD </s></s> the nasi lemak was incredibly sedap\"\n",
        "        Tokenized:   <s> FOOD </s> </s> the nasi lemak was incredibly sedap </s>\n",
        "        Label:       1 (positive)\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['aspect', 'Segment', 'label'].\n",
        "        tokenizer: HuggingFace tokenizer for xlm-roberta.\n",
        "        max_length: Maximum token length (default 128).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        max_length: int = 128,\n",
        "    ):\n",
        "        self.texts = df[\"Segment\"].tolist()\n",
        "        self.aspects = df[\"aspect\"].tolist()\n",
        "        self.labels = df[\"label\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        \"\"\"Tokenize a single (aspect, segment) pair on-the-fly.\n",
        "\n",
        "        Why on-the-fly tokenization (not pre-tokenized):\n",
        "            For datasets of this size (~100K), pre-tokenizing and caching\n",
        "            in memory is faster but uses ~2-3 GB RAM. On-the-fly keeps\n",
        "            memory footprint low and simplifies the code. If training\n",
        "            speed becomes the bottleneck, switch to a pre-tokenized cache.\n",
        "\n",
        "        Returns:\n",
        "            Dict with 'input_ids', 'attention_mask', 'labels' tensors.\n",
        "        \"\"\"\n",
        "        aspect = self.aspects[idx]\n",
        "        segment = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # --- Construct aspect-conditioned input --------------------------\n",
        "        # Format: \"ASPECT_LABEL </s></s> segment_text\"\n",
        "        # The aspect is uppercased to visually distinguish it as a\n",
        "        # \"prompt token\" â€” the model learns to treat it as a conditioning\n",
        "        # signal rather than natural language.\n",
        "        conditioned_text = f\"{aspect.upper()} </s></s> {segment}\"\n",
        "\n",
        "        # --- Tokenize ----------------------------------------------------\n",
        "        encoding = self.tokenizer(\n",
        "            conditioned_text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,           \n",
        "            padding=False,\n",
        "            return_tensors=None,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"],\n",
        "            \"attention_mask\": encoding[\"attention_mask\"],\n",
        "            \"labels\": label,\n",
        "        }\n",
        "\n",
        "\n",
        "def build_dataloaders(data: dict, cfg: TrainingConfig) -> dict:\n",
        "    \"\"\"Instantiate tokenizer, datasets, and DataLoaders.\n",
        "\n",
        "    Args:\n",
        "        data: Dict with 'train', 'val', 'test' DataFrames.\n",
        "        cfg: TrainingConfig.\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'train', 'val', 'test' DataLoaders and 'tokenizer'.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BUILDING DATALOADERS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Load tokenizer (downloads ~1 MB vocab file on first run)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "    print(f\"  âœ“ Tokenizer loaded: {cfg.model_name}\")\n",
        "    print(f\"    Vocab size: {tokenizer.vocab_size:,}\")\n",
        "\n",
        "    # OPTIMIZATION: Smart Collator handles dynamic padding\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    loaders = {}\n",
        "    datasets_info = {}\n",
        "\n",
        "    for split_name, df in data.items():\n",
        "        dataset = ABSADataset(df, tokenizer, cfg.max_seq_length)\n",
        "\n",
        "        # Training set uses shuffle; val/test do not\n",
        "        is_train = split_name == \"train\"\n",
        "\n",
        "        # OPTIMIZATION: Increase num_workers on Linux/Colab\n",
        "        # If on Windows, keep at 0. If on Colab, use 2.\n",
        "        import os\n",
        "        workers = 2 if os.name == 'posix' else 0\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=cfg.batch_size,\n",
        "            shuffle=is_train,\n",
        "            collate_fn=data_collator,\n",
        "            num_workers=workers,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        loaders[split_name] = loader\n",
        "        datasets_info[split_name] = len(dataset)\n",
        "\n",
        "    loaders[\"tokenizer\"] = tokenizer\n",
        "\n",
        "    print(f\"\\n  Dataset sizes & batches:\")\n",
        "    for name, size in datasets_info.items():\n",
        "        n_batches = size // cfg.batch_size + (1 if size % cfg.batch_size else 0)\n",
        "        print(f\"    {name:<6}: {size:>7,} samples â†’ {n_batches:>4,} batches \"\n",
        "              f\"(batch_size={cfg.batch_size})\")\n",
        "\n",
        "    # --- Quick sanity check: decode one sample ---------------------------\n",
        "    sample_batch = next(iter(loaders[\"train\"]))\n",
        "    sample_text = tokenizer.decode(\n",
        "        sample_batch[\"input_ids\"][0], skip_special_tokens=False\n",
        "    )\n",
        "    print(f\"\\n  Sample input (decoded):\")\n",
        "    print(f\"    \\\"{sample_text}\\\"\")\n",
        "    print(f\"    Label: {ID2LABEL[sample_batch['labels'][0].item()]}\")\n",
        "\n",
        "    return loaders\n",
        "\n",
        "\n",
        "LOADERS = build_dataloaders(DATA, CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a770bc9e",
      "metadata": {
        "id": "a770bc9e"
      },
      "source": [
        "# STAGE 4: Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "SW_kc5mhaeyU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_kc5mhaeyU",
        "outputId": "2ee5f00a-74d7-4844-d3d3-6c005c0a5ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Using GPU:  Tesla T4\n",
            "\n",
            "======================================================================\n",
            "BUILDING MODEL\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ“ Model loaded on cuda\n",
            "    Total params:      278,045,186\n",
            "    Trainable params:  278,045,186\n",
            "    Class weights:    [4.5448, 0.5618]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Define the device (GPU or CPU)\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"âœ… Using GPU: \", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"âš ï¸ Using CPU (Warning: Slow!)\")\n",
        "\n",
        "# 2. Now run your build model command\n",
        "MODEL = build_model(CFG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "971bcec3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "971bcec3",
        "outputId": "609136f0-fa21-471c-de3f-d4cb11fa90ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING MODEL\n",
            "======================================================================\n",
            "  âœ“ Model loaded on cuda\n",
            "    Total params:      278,045,186\n",
            "    Trainable params:  278,045,186\n",
            "    Class weights:    [4.5448, 0.5618]\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# XLM-RoBERTa + a 2-class classification head + class-weighted loss.\n",
        "# ==============================================================================\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ABSASentimentClassifier(nn.Module):\n",
        "    \"\"\"XLM-RoBERTa with a classification head for binary sentiment.\n",
        "\n",
        "    Why AutoModelForSequenceClassification instead of raw AutoModel:\n",
        "        The \"ForSequenceClassification\" variant already includes:\n",
        "          - The [CLS] token pooling (first token representation)\n",
        "          - A dropout layer\n",
        "          - A linear projection to num_labels\n",
        "        Building these manually adds no value and risks subtle bugs\n",
        "        (e.g., forgetting dropout â†’ overfitting).\n",
        "\n",
        "    Why we store class_weights on the model:\n",
        "        This ensures the weights move to the correct device (CPU/GPU)\n",
        "        alongside the model when .to(device) is called. Forgetting this\n",
        "        is one of the most common PyTorch bugs.\n",
        "\n",
        "    Args:\n",
        "        cfg: TrainingConfig.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: TrainingConfig):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModelForSequenceClassification.from_pretrained(\n",
        "            cfg.model_name,\n",
        "            num_labels=cfg.num_labels,\n",
        "        )\n",
        "        # Register class weights as a buffer (not a parameter â€”\n",
        "        # it won't be updated by the optimizer, but WILL move with .to())\n",
        "        self.register_buffer(\n",
        "            \"class_weights\",\n",
        "            torch.tensor(cfg.class_weights, dtype=torch.float),\n",
        "        )\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        labels: torch.Tensor = None,\n",
        "    ) -> dict:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs (batch_size, seq_len).\n",
        "            attention_mask: 1 for real tokens, 0 for padding (batch_size, seq_len).\n",
        "            labels: Ground truth labels (batch_size,). Optional â€” if None,\n",
        "                    only logits are returned (useful for inference).\n",
        "\n",
        "        Returns:\n",
        "            Dict with 'loss' (if labels provided) and 'logits'.\n",
        "        \"\"\"\n",
        "        outputs = self.backbone(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        logits = outputs.logits  # (batch_size, num_labels)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "\n",
        "def build_model(cfg: TrainingConfig) -> ABSASentimentClassifier:\n",
        "    \"\"\"Instantiate model and move to device.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BUILDING MODEL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model = ABSASentimentClassifier(cfg)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  âœ“ Model loaded on {DEVICE}\")\n",
        "    print(f\"    Total params:     {total_params:>12,}\")\n",
        "    print(f\"    Trainable params: {trainable_params:>12,}\")\n",
        "    print(f\"    Class weights:    {cfg.class_weights}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "MODEL = build_model(CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d3468a",
      "metadata": {
        "id": "75d3468a"
      },
      "source": [
        "# STAGE 5: Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60febb3f",
      "metadata": {
        "id": "60febb3f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.auto import tqdm  \n",
        "\n",
        "def compute_metrics(predictions: list, labels: list) -> dict:\n",
        "    \"\"\"Compute accuracy, macro-F1, and per-class F1.\n",
        "\n",
        "    Why macro-F1 over accuracy:\n",
        "        With 89/11 imbalance, a model predicting \"positive\" always gets\n",
        "        89% accuracy. Macro-F1 weights both classes equally, so it\n",
        "        actually measures whether the model learned the minority class.\n",
        "\n",
        "    Args:\n",
        "        predictions: List of predicted class IDs.\n",
        "        labels: List of ground-truth class IDs.\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'accuracy', 'macro_f1', 'neg_f1', 'pos_f1'.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
        "    per_class_f1 = f1_score(labels, predictions, average=None)  # [neg_f1, pos_f1]\n",
        "    return {\n",
        "        \"accuracy\": round(acc, 4),\n",
        "        \"macro_f1\": round(macro_f1, 4),\n",
        "        \"neg_f1\": round(per_class_f1[0], 4),\n",
        "        \"pos_f1\": round(per_class_f1[1], 4),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device) -> dict:\n",
        "    \"\"\"Run one full training epoch.\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'loss' (average over all batches).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # WRAP DATALOADER WITH TQDM FOR PROGRESS BAR\n",
        "    # This creates the visual bar: [=====>      ] 45%\n",
        "    progress_bar = tqdm(dataloader, desc=\"  Training\", leave=False)\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        loss = outputs[\"loss\"]\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Gradient clipping: prevents exploding gradients in transformers\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # UPDATE PROGRESS BAR\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return {\"loss\": round(total_loss / num_batches, 6)}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device) -> dict:\n",
        "    \"\"\"Run evaluation on val or test set (no gradient computation).\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'loss', 'accuracy', 'macro_f1', 'neg_f1', 'pos_f1'.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        total_loss += outputs[\"loss\"].item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Argmax â†’ predicted class\n",
        "        preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "        all_preds.extend(preds.cpu().numpy().tolist())\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    metrics = compute_metrics(all_preds, all_labels)\n",
        "    metrics[\"loss\"] = round(total_loss / num_batches, 6)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train(model, loaders, cfg, device) -> dict:\n",
        "    \"\"\"Full training loop with early stopping on val macro-F1.\n",
        "\n",
        "    Why early stopping on macro-F1 (not loss):\n",
        "        Validation loss can continue decreasing even as the model starts\n",
        "        overfitting to the majority class. Macro-F1 directly measures\n",
        "        what we care about: balanced performance on both classes.\n",
        "\n",
        "    Args:\n",
        "        model: ABSASentimentClassifier (already on device).\n",
        "        loaders: Dict with 'train', 'val', 'test' DataLoaders.\n",
        "        cfg: TrainingConfig.\n",
        "        device: torch.device.\n",
        "\n",
        "    Returns:\n",
        "        Dict with full training history (for plotting / logging).\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # --- Optimizer: AdamW (standard for transformer fine-tuning) --------\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=cfg.learning_rate,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "    )\n",
        "\n",
        "    # OPTIMIZATION: Initialize Scaler for FP16\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # --- Learning rate scheduler: linear warm-up then linear decay -------\n",
        "    # Why: Transformers are sensitive to LR. A warm-up phase prevents\n",
        "    # catastrophic early updates to pre-trained weights.\n",
        "    total_steps = len(loaders[\"train\"]) * cfg.num_epochs\n",
        "    warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "    print(f\"  Total training steps:  {total_steps:,}\")\n",
        "    print(f\"  Warmup steps:          {warmup_steps:,}\")\n",
        "\n",
        "    # --- Output directory -------------------------------------------------\n",
        "    os.makedirs(os.path.dirname(cfg.best_model_path) or \".\", exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(cfg.metrics_path) or \".\", exist_ok=True)\n",
        "\n",
        "    # --- Training history -------------------------------------------------\n",
        "    history = {\"train\": [], \"val\": [], \"test\": None}\n",
        "    best_val_f1 = -1.0\n",
        "    patience = 2   # Stop if val macro-F1 doesn't improve for 2 epochs\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, cfg.num_epochs + 1):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # --- Train ---\n",
        "        train_metrics = train_epoch(model, loaders[\"train\"], optimizer,\n",
        "                                    scheduler, device)\n",
        "        history[\"train\"].append(train_metrics)\n",
        "\n",
        "        # --- Validate ---\n",
        "        val_metrics = evaluate(model, loaders[\"val\"], device)\n",
        "        history[\"val\"].append(val_metrics)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        # --- Log -------------------------------------------------------------\n",
        "        print(f\"\\n  Epoch {epoch}/{cfg.num_epochs}  ({epoch_time:.1f}s)\")\n",
        "        print(f\"    Train Loss:      {train_metrics['loss']:.6f}\")\n",
        "        print(f\"    Val  Loss:       {val_metrics['loss']:.6f}\")\n",
        "        print(f\"    Val  Accuracy:   {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"    Val  Macro-F1:   {val_metrics['macro_f1']:.4f}  \"\n",
        "              f\"(neg: {val_metrics['neg_f1']:.4f} | \"\n",
        "              f\"pos: {val_metrics['pos_f1']:.4f})\")\n",
        "\n",
        "        # --- Early stopping & best-model checkpoint -------------------------\n",
        "        if val_metrics[\"macro_f1\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"macro_f1\"]\n",
        "            patience_counter = 0\n",
        "            # Save only the model state_dict (not the whole object)\n",
        "            torch.save(model.state_dict(), cfg.best_model_path)\n",
        "            print(f\"    â˜… New best model saved  (macro-F1: {best_val_f1:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"    âœ— No improvement. Patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\n  âš¡ Early stopping at epoch {epoch}.\")\n",
        "                break\n",
        "\n",
        "    # --- Final evaluation on TEST set (only once, after training) -----------\n",
        "    # Load the best checkpoint before evaluating\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"  LOADING BEST MODEL FOR FINAL TEST EVALUATION\")\n",
        "    print(\"-\" * 70)\n",
        "    model.load_state_dict(torch.load(cfg.best_model_path, map_location=device))\n",
        "\n",
        "    test_metrics = evaluate(model, loaders[\"test\"], device)\n",
        "    history[\"test\"] = test_metrics\n",
        "\n",
        "    print(f\"\\n  â˜… FINAL TEST RESULTS (best checkpoint):\")\n",
        "    print(f\"    Test Loss:       {test_metrics['loss']:.6f}\")\n",
        "    print(f\"    Test Accuracy:   {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"    Test Macro-F1:   {test_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"      Negative F1:   {test_metrics['neg_f1']:.4f}\")\n",
        "    print(f\"      Positive  F1:  {test_metrics['pos_f1']:.4f}\")\n",
        "\n",
        "    # --- Full classification report (for the report / thesis) --------------\n",
        "    # Re-run test set to collect all predictions for the report\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loaders[\"test\"]:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            # OPTIMIZATION: Mixed Precision Context\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(): # <--- This magic line enables FP16\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels,\n",
        "                )\n",
        "                loss = outputs[\"loss\"]\n",
        "\n",
        "            # OPTIMIZATION: Scale loss and step\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(batch[\"labels\"].numpy().tolist())\n",
        "\n",
        "    print(f\"\\n  Classification Report:\")\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=[\"Negative\", \"Positive\"],\n",
        "    ))\n",
        "\n",
        "    # --- Save training history as JSON ------------------------------------\n",
        "    with open(cfg.metrics_path, \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "    print(f\"  âœ“ Training metrics saved to: {cfg.metrics_path}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "835fdf30",
      "metadata": {
        "id": "835fdf30"
      },
      "source": [
        "# STAGE 6: Execute Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff6ca80",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_on_gold(model, cfg: TrainingConfig, tokenizer, device) -> dict:\n",
        "    \"\"\"Evaluate trained model on the Gold Standard (manually-annotated) dataset.\n",
        "\n",
        "    Why separate gold evaluation:\n",
        "        - Training uses WEAK labels (star ratings) with inherent noise\n",
        "        - Gold standard has HUMAN-ANNOTATED labels (ground truth)\n",
        "        - We evaluate on gold separately to measure TRUE model performance\n",
        "        - This is what goes into your thesis results section (Table 5)\n",
        "\n",
        "    Academic Justification:\n",
        "        Following the evaluation protocol of Sun et al. (2019) and Pontiki et al.\n",
        "        (2016), we report performance on a gold standard test set annotated by\n",
        "        domain experts. This accounts for label noise in the weak supervision\n",
        "        training set and provides trustworthy F1 scores for the final thesis.\n",
        "\n",
        "    Args:\n",
        "        model: Trained ABSASentimentClassifier (already on best checkpoint)\n",
        "        cfg: TrainingConfig with gold_data_path\n",
        "        tokenizer: XLM-RoBERTa tokenizer\n",
        "        device: torch.device (GPU or CPU)\n",
        "\n",
        "    Returns:\n",
        "        Dict with overall metrics + per-aspect breakdown for thesis reporting\n",
        "    \"\"\"\n",
        "    print(f\"\\n  Loading gold standard from: {cfg.gold_data_path}\")\n",
        "\n",
        "    # --- Load and preprocess gold data ---\n",
        "    try:\n",
        "        gold_df = pd.read_csv(cfg.gold_data_path)\n",
        "        print(f\"  âœ“ Gold dataset loaded: {len(gold_df):,} rows (before exploding multi-aspect)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error loading gold data: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    # Rename columns to match training format\n",
        "    # Input columns: Segment, Manual_Aspect, Manual_Sentiment\n",
        "    gold_df_prep = gold_df.copy()\n",
        "    gold_df_prep.rename(columns={\n",
        "        \"Manual_Aspect\": \"aspect\",\n",
        "        \"Manual_Sentiment\": \"Sentiment_Label\",\n",
        "    }, inplace=True)\n",
        "\n",
        "    # --- Handle multi-aspect segments: explode into separate rows -------\n",
        "    # If aspect is stored as string representation of list (e.g., \"['FOOD', 'VALUE']\"),\n",
        "    # convert to actual list\n",
        "    import ast\n",
        "    def parse_aspect(val):\n",
        "        \"\"\"Parse aspect column - handle both strings and lists.\"\"\"\n",
        "        if isinstance(val, str):\n",
        "            try:\n",
        "                # Try to parse as Python literal (handles \"['FOOD', 'VALUE']\")\n",
        "                parsed = ast.literal_eval(val)\n",
        "                if isinstance(parsed, list):\n",
        "                    return parsed\n",
        "                else:\n",
        "                    return [parsed]  # Single aspect as string\n",
        "            except (ValueError, SyntaxError):\n",
        "                # Already a plain string like \"FOOD\"\n",
        "                return [val]\n",
        "        elif isinstance(val, list):\n",
        "            return val\n",
        "        else:\n",
        "            return [str(val)]\n",
        "\n",
        "    gold_df_prep[\"aspect\"] = gold_df_prep[\"aspect\"].apply(parse_aspect)\n",
        "    \n",
        "    # Count single vs multi-aspect rows BEFORE exploding\n",
        "    n_single_aspect = sum(len(aspects) == 1 for aspects in gold_df_prep[\"aspect\"])\n",
        "    n_multi_aspect = len(gold_df_prep) - n_single_aspect\n",
        "    \n",
        "    # Explode: one row per aspect (same segment can appear multiple times)\n",
        "    gold_df_exploded = gold_df_prep.explode(\"aspect\").reset_index(drop=True)\n",
        "    \n",
        "    print(f\"    Original rows:        {len(gold_df):,}\")\n",
        "    print(f\"      Single-aspect:      {n_single_aspect:,}\")\n",
        "    print(f\"      Multi-aspect:       {n_multi_aspect:,}\")\n",
        "    print(f\"    After exploding:      {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "\n",
        "    # Encode sentiment labels to numeric format\n",
        "    gold_df_exploded[\"label\"] = gold_df_exploded[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "    # Sanity check: warn if any labels couldn't be mapped\n",
        "    n_unmapped = gold_df_exploded[\"label\"].isna().sum()\n",
        "    if n_unmapped > 0:\n",
        "        print(f\"  âš ï¸  Warning: {n_unmapped} rows with unmapped sentiment labels\")\n",
        "        print(f\"     Available values: {gold_df_exploded['Sentiment_Label'].unique()}\")\n",
        "        print(f\"     Expected: {list(LABEL2ID.keys())}\")\n",
        "        # Drop unmapped rows\n",
        "        gold_df_exploded = gold_df_exploded.dropna(subset=[\"label\"])\n",
        "        print(f\"     Proceeding with {len(gold_df_exploded):,} valid samples\")\n",
        "\n",
        "    print(f\"\\n  Gold dataset label distribution:\")\n",
        "    for label_id in sorted(gold_df_exploded[\"label\"].unique()):\n",
        "        count = (gold_df_exploded[\"label\"] == label_id).sum()\n",
        "        pct = count / len(gold_df_exploded) * 100\n",
        "        print(f\"    {ID2LABEL[label_id]:<10}: {count:>5,} samples ({pct:>5.1f}%)\")\n",
        "    \n",
        "    # Use the exploded dataframe for evaluation\n",
        "    gold_df_prep = gold_df_exploded\n",
        "\n",
        "    # --- Create dataset and dataloader ---\n",
        "    gold_dataset = ABSADataset(gold_df_prep, tokenizer, cfg.max_seq_length)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    gold_loader = torch.utils.data.DataLoader(\n",
        "        gold_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "        num_workers=0,  # Keep at 0 for compatibility (Windows/Colab)\n",
        "    )\n",
        "\n",
        "    # --- Inference on gold set (no gradients) ---\n",
        "    print(f\"\\n  Running inference on gold set...\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_aspects = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(gold_loader):\n",
        "            if (batch_idx + 1) % max(1, len(gold_loader) // 5) == 0:\n",
        "                print(f\"    Progress: {batch_idx + 1}/{len(gold_loader)} batches\")\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            # Get predicted classes (argmax over logits)\n",
        "            preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    # Get corresponding aspects for per-aspect breakdown\n",
        "    all_aspects = gold_df_prep[\"aspect\"].tolist()\n",
        "\n",
        "    # --- Compute overall metrics ---\n",
        "    overall_metrics = compute_metrics(all_preds, all_labels)\n",
        "\n",
        "    # --- Per-aspect breakdown ---\n",
        "    aspects_unique = sorted(set(all_aspects))\n",
        "    per_aspect_metrics = {}\n",
        "\n",
        "    for aspect in aspects_unique:\n",
        "        # Filter to samples of this aspect\n",
        "        mask = [i for i, a in enumerate(all_aspects) if a == aspect]\n",
        "        if not mask:\n",
        "            continue\n",
        "\n",
        "        aspect_preds = [all_preds[i] for i in mask]\n",
        "        aspect_labels = [all_labels[i] for i in mask]\n",
        "        per_aspect_metrics[aspect] = compute_metrics(aspect_preds, aspect_labels)\n",
        "\n",
        "    # --- Format results for saving ---\n",
        "    gold_results = {\n",
        "        \"overall\": overall_metrics,\n",
        "        \"per_aspect\": per_aspect_metrics,\n",
        "        \"n_samples\": len(gold_df_prep),\n",
        "        \"aspects\": aspects_unique,\n",
        "    }\n",
        "\n",
        "    # --- Print results for immediate feedback ---\n",
        "    print(f\"\\n  {'='*70}\")\n",
        "    print(f\"  â˜… GOLD STANDARD EVALUATION RESULTS ({len(gold_df_prep):,} samples)\")\n",
        "    print(f\"  {'='*70}\")\n",
        "    print(f\"\\n  OVERALL PERFORMANCE:\")\n",
        "    print(f\"    Accuracy:  {overall_metrics['accuracy']:.4f}\")\n",
        "    print(f\"    Macro-F1:  {overall_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"      Negative F1 (Recall on negative class):  {overall_metrics['neg_f1']:.4f}\")\n",
        "    print(f\"      Positive F1 (Recall on positive class):  {overall_metrics['pos_f1']:.4f}\")\n",
        "\n",
        "    print(f\"\\n  PER-ASPECT BREAKDOWN (for Kano Model analysis):\")\n",
        "    print(f\"  {'Aspect':<16} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
        "    print(f\"  {'-'*16} {'-'*8} {'-'*10} {'-'*10}\")\n",
        "    for aspect in aspects_unique:\n",
        "        metrics = per_aspect_metrics[aspect]\n",
        "        n_samples = sum(1 for a in all_aspects if a == aspect)\n",
        "        print(f\"  {aspect:<16} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
        "\n",
        "    print(f\"\\n  FULL CLASSIFICATION REPORT (for thesis):\")\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=[\"Negative\", \"Positive\"],\n",
        "        digits=4,\n",
        "    ))\n",
        "\n",
        "    return gold_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b2a9cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "90b2a9cf",
        "outputId": "a6684b10-c75f-422b-9a70-56aeedc8d455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING\n",
            "======================================================================\n",
            "  Total training steps:  11,785\n",
            "  Warmup steps:          1,178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3591665734.py:140: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Epoch 1/5  (632.4s)\n",
            "    Train Loss:      0.511156\n",
            "    Val  Loss:       0.392413\n",
            "    Val  Accuracy:   0.9003\n",
            "    Val  Macro-F1:   0.7346  (neg: 0.5249 | pos: 0.9443)\n",
            "    â˜… New best model saved  (macro-F1: 0.7346)\n",
            "\n",
            "  Epoch 2/5  (636.2s)\n",
            "    Train Loss:      0.384388\n",
            "    Val  Loss:       0.435052\n",
            "    Val  Accuracy:   0.9271\n",
            "    Val  Macro-F1:   0.7664  (neg: 0.5726 | pos: 0.9602)\n",
            "    â˜… New best model saved  (macro-F1: 0.7664)\n",
            "\n",
            "  Epoch 3/5  (635.5s)\n",
            "    Train Loss:      0.332344\n",
            "    Val  Loss:       0.490137\n",
            "    Val  Accuracy:   0.9308\n",
            "    Val  Macro-F1:   0.7762  (neg: 0.5901 | pos: 0.9622)\n",
            "    â˜… New best model saved  (macro-F1: 0.7762)\n",
            "\n",
            "  Epoch 4/5  (636.9s)\n",
            "    Train Loss:      0.296671\n",
            "    Val  Loss:       0.523235\n",
            "    Val  Accuracy:   0.9333\n",
            "    Val  Macro-F1:   0.7731  (neg: 0.5825 | pos: 0.9637)\n",
            "    âœ— No improvement. Patience: 1/2\n",
            "\n",
            "  Epoch 5/5  (636.0s)\n",
            "    Train Loss:      0.271154\n",
            "    Val  Loss:       0.668442\n",
            "    Val  Accuracy:   0.9379\n",
            "    Val  Macro-F1:   0.7788  (neg: 0.5911 | pos: 0.9664)\n",
            "    â˜… New best model saved  (macro-F1: 0.7788)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "  LOADING BEST MODEL FOR FINAL TEST EVALUATION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "  â˜… FINAL TEST RESULTS (best checkpoint):\n",
            "    Test Loss:       0.642298\n",
            "    Test Accuracy:   0.9415\n",
            "    Test Macro-F1:   0.7874\n",
            "      Negative F1:   0.6063\n",
            "      Positive  F1:  0.9684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3591665734.py:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(): # <--- This magic line enables FP16\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-585777798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHISTORY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOADERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAINING COMPLETE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3591665734.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loaders, cfg, device)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;31m# OPTIMIZATION: Scale loss and step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "HISTORY = train(MODEL, LOADERS, CFG, DEVICE)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Best model:   {CFG.best_model_path}\")\n",
        "print(f\"  Metrics file: {CFG.metrics_path}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==============================================================================\n",
        "# GOLD STANDARD EVALUATION (Thesis Results Section)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EVALUATING ON GOLD STANDARD (GROUND TRUTH LABELS)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load best checkpoint for gold evaluation\n",
        "MODEL.load_state_dict(torch.load(CFG.best_model_path, map_location=DEVICE))\n",
        "\n",
        "# Run evaluation on manually-annotated gold dataset\n",
        "gold_metrics = evaluate_on_gold(MODEL, CFG, LOADERS[\"tokenizer\"], DEVICE)\n",
        "\n",
        "# Save gold evaluation results to JSON\n",
        "import json\n",
        "os.makedirs(os.path.dirname(CFG.gold_results_path) or \".\", exist_ok=True)\n",
        "with open(CFG.gold_results_path, \"w\") as f:\n",
        "    json.dump(gold_metrics, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ“ Gold evaluation saved to: {CFG.gold_results_path}\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ALL EVALUATIONS COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nðŸ“Š RESULTS SUMMARY:\")\n",
        "print(f\"  Weak Test Set:   {CFG.metrics_path}\")\n",
        "print(f\"  Gold Test Set:   {CFG.gold_results_path}\")\n",
        "print(f\"  Best Model:      {CFG.best_model_path}\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
