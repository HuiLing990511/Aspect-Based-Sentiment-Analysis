{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca1b1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n================================================================================\\nXLM-RoBERTa Aspect-Based Sentiment Classification\\n================================================================================\\nPROJECT : NLP-Driven ABSA for Gastronomy Tourism Insights in Malaysia\\nPIPELINE: Pipelined-ABSA (Decoupled) — Step 3: Sentiment Classification\\nINPUT   : Dataset/aspect_categorization.pkl   (output of Notebook 5)\\nOUTPUT  : models/xlm_roberta_absa_best.pt     (best checkpoint)\\n          results/training_metrics.json        (loss/acc curves)\\n\\nACADEMIC JUSTIFICATION\\n----------------------\\n- XLM-RoBERTa (Conneau et al., 2020): Pre-trained on 100 languages including\\n  Malay and Chinese. Superior zero/few-shot cross-lingual transfer vs.\\n  monolingual BERT, critical for Manglish code-switching.\\n- Aspect-Conditioned Input (Sun et al., 2019): We prepend the aspect category\\n  to the segment text as \"[aspect] [SEP] [segment]\". This forces the model to\\n  learn aspect-specific sentiment representations rather than general polarity.\\n- Class-Weighted Loss (Japkowicz & Stephen, 2002): Our dataset is severely\\n  imbalanced (89% positive). We use the inverse-frequency weights computed in\\n  Notebook 4 to prevent the model from trivially predicting \"positive\".\\n- Weak Supervision (Ratner et al., 2016): Star ratings are noisy proxies for\\n  sentiment. The consistency filtering in Notebook 4 already removed the worst\\n  offenders (4.1% noise). Residual noise is tolerable for fine-tuning.\\n================================================================================\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "================================================================================\n",
    "XLM-RoBERTa Aspect-Based Sentiment Classification\n",
    "================================================================================\n",
    "PROJECT : NLP-Driven ABSA for Gastronomy Tourism Insights in Malaysia\n",
    "PIPELINE: Pipelined-ABSA (Decoupled) — Step 3: Sentiment Classification\n",
    "INPUT   : Dataset/aspect_categorization.pkl   (output of Notebook 5)\n",
    "OUTPUT  : models/xlm_roberta_absa_best.pt     (best checkpoint)\n",
    "          results/training_metrics.json        (loss/acc curves)\n",
    "\n",
    "ACADEMIC JUSTIFICATION\n",
    "----------------------\n",
    "- XLM-RoBERTa (Conneau et al., 2020): Pre-trained on 100 languages including\n",
    "  Malay and Chinese. Superior zero/few-shot cross-lingual transfer vs.\n",
    "  monolingual BERT, critical for Manglish code-switching.\n",
    "- Aspect-Conditioned Input (Sun et al., 2019): We prepend the aspect category\n",
    "  to the segment text as \"[aspect] [SEP] [segment]\". This forces the model to\n",
    "  learn aspect-specific sentiment representations rather than general polarity.\n",
    "- Class-Weighted Loss (Japkowicz & Stephen, 2002): Our dataset is severely\n",
    "  imbalanced (89% positive). We use the inverse-frequency weights computed in\n",
    "  Notebook 4 to prevent the model from trivially predicting \"positive\".\n",
    "- Weak Supervision (Ratner et al., 2016): Star ratings are noisy proxies for\n",
    "  sentiment. The consistency filtering in Notebook 4 already removed the worst\n",
    "  offenders (4.1% noise). Residual noise is tolerable for fine-tuning.\n",
    "================================================================================\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ea3c5",
   "metadata": {},
   "source": [
    "# STAGE 0: Environment & Dependency Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f2fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4fec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix PyTorch DLL loading issue on Windows\n",
    "import os\n",
    "import platform\n",
    "if platform.system() == \"Windows\":\n",
    "    import ctypes\n",
    "    from importlib.util import find_spec\n",
    "    try:\n",
    "        if (spec := find_spec(\"torch\")) and spec.origin and os.path.exists(\n",
    "            dll_path := os.path.join(os.path.dirname(spec.origin), \"lib\", \"c10.dll\")\n",
    "        ):\n",
    "            ctypes.CDLL(os.path.normpath(dll_path))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a870b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENVIRONMENT CHECK\n",
      "======================================================================\n",
      "  ✓  PyTorch                        v2.8.0+cpu\n",
      "  ✓  HuggingFace Transformers       v4.41.2\n",
      "  ✓  Pandas                         v2.2.3\n",
      "  ✓  NumPy                          v2.2.0\n",
      "  ✓  Scikit-Learn                   v1.5.1\n",
      "\n",
      "  GPU Available: False  →  CPU only\n",
      "  Python:        3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "REQUIRED = {\n",
    "    \"torch\": \"PyTorch\",\n",
    "    \"transformers\": \"HuggingFace Transformers\",\n",
    "    \"pandas\": \"Pandas\",\n",
    "    \"numpy\": \"NumPy\",\n",
    "    \"sklearn\": \"Scikit-Learn\",\n",
    "}\n",
    "\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Verify all required packages are installed and print versions.\n",
    "\n",
    "    Why:\n",
    "        Explicit environment checks prevent cryptic import errors mid-training,\n",
    "        which is especially costly when running on GPU with long epoch times.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ENVIRONMENT CHECK\")\n",
    "    print(\"=\" * 70)\n",
    "    all_ok = True\n",
    "    for module_name, display_name in REQUIRED.items():\n",
    "        try:\n",
    "            mod = importlib.import_module(module_name)\n",
    "            version = getattr(mod, \"__version__\", \"unknown\")\n",
    "            print(f\"  ✓  {display_name:<30} v{version}\")\n",
    "        except ImportError:\n",
    "            print(f\"  ✗  {display_name:<30} NOT INSTALLED\")\n",
    "            all_ok = False\n",
    "\n",
    "    # Special check: torch CUDA availability\n",
    "    import torch\n",
    "\n",
    "    cuda_avail = torch.cuda.is_available()\n",
    "    device_name = torch.cuda.get_device_name(0) if cuda_avail else \"CPU only\"\n",
    "    print(f\"\\n  GPU Available: {cuda_avail}  →  {device_name}\")\n",
    "    print(f\"  Python:        {sys.version}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not all_ok:\n",
    "        raise RuntimeError(\n",
    "            \"Some packages are missing. Install them before continuing.\"\n",
    "        )\n",
    "    return torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
    "\n",
    "\n",
    "DEVICE = check_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d863039",
   "metadata": {},
   "source": [
    "# STAGE 1: Hyperparameter Configuration (Single Source of Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5f8a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Config loaded. Model: xlm-roberta-base | Epochs: 5 | Batch: 32 | LR: 2e-05\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Central configuration object for the entire training run.\n",
    "\n",
    "    Why a dataclass:\n",
    "        Keeps hyperparameters serializable (can be logged to JSON for\n",
    "        reproducibility) and gives IDE auto-completion — important when\n",
    "        iterating quickly on a GPU budget.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Model -----------------------------------------------------------\n",
    "    model_name: str = \"xlm-roberta-base\"\n",
    "    # \"xlm-roberta-base\" (278M params) is the default.\n",
    "    # Switch to \"xlm-roberta-large\" (550M) if GPU memory allows (≥16 GB).\n",
    "\n",
    "    num_labels: int = 2  # 0 = negative, 1 = positive\n",
    "\n",
    "    # --- Data ------------------------------------------------------------\n",
    "    data_path: str = r\"C:/Users/Ong Hui Ling/Dropbox/PC/Documents/Github/Aspect-Based-Sentiment-Analysis/Dataset/aspect_categorization.pkl\"\n",
    "    max_seq_length: int = 128\n",
    "    # 128 covers ~95th percentile of your segment lengths (median ~54 words).\n",
    "    # Increase to 256 only if you see significant truncation in logs.\n",
    "\n",
    "    test_size: float = 0.15  # 15% held out for evaluation\n",
    "    val_size: float = 0.10   # 10% for early-stopping validation\n",
    "    random_seed: int = 42\n",
    "\n",
    "    # --- Training --------------------------------------------------------\n",
    "    batch_size: int = 32       # Reduce to 16 if OOM on your GPU\n",
    "    learning_rate: float = 2e-5  # Classic BERT fine-tuning sweet spot\n",
    "    num_epochs: int = 5\n",
    "    warmup_ratio: float = 0.1  # 10% of total steps used for LR warm-up\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "    # --- Class Weights (from Notebook 4: compute_class_weight) -----------\n",
    "    # Negative (class 0) weight: 4.5448\n",
    "    # Positive (class 1) weight: 0.5618\n",
    "    class_weights: List[float] = field(default_factory=lambda: [4.5448, 0.5618])\n",
    "\n",
    "    # --- Output ----------------------------------------------------------\n",
    "    output_dir: str = \"models\"\n",
    "    best_model_path: str = \"models/xlm_roberta_absa_best.pt\"\n",
    "    metrics_path: str = \"results/training_metrics.json\"\n",
    "\n",
    "\n",
    "CFG = TrainingConfig()\n",
    "print(f\"\\n✓ Config loaded. Model: {CFG.model_name} | Epochs: {CFG.num_epochs} | \"\n",
    "      f\"Batch: {CFG.batch_size} | LR: {CFG.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7a186",
   "metadata": {},
   "source": [
    "# STAGE 2: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9546dc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING & SPLITTING DATA\n",
      "======================================================================\n",
      "  Raw segments loaded: 132,637\n",
      "  Single-aspect segments (for training): 100,557\n",
      "  Multi-aspect segments (dropped for training): 32,080\n",
      "\n",
      "  Label distribution:\n",
      "label\n",
      "  0 (negative)     7673\n",
      "  1 (positive)    92884\n",
      "\n",
      "  Split sizes:\n",
      "    train :  75,417 rows | pos: 69,662 (92.4%) | neg: 5,755 (7.6%)\n",
      "    val   :  10,056 rows | pos: 9,289 (92.4%) | neg: 767 (7.6%)\n",
      "    test  :  15,084 rows | pos: 13,933 (92.4%) | neg: 1,151 (7.6%)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Transforms aspect_categorization.pkl → train/val/test splits\n",
    "# ready for the PyTorch DataLoader.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Label encoding map (explicit is better than implicit)\n",
    "LABEL2ID = {\"negative\": 0, \"positive\": 1}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "\n",
    "def load_and_prepare_data(cfg: TrainingConfig) -> dict:\n",
    "    \"\"\"Load aspect segments and split into train / val / test.\n",
    "\n",
    "    Why stratified split:\n",
    "        With 89/11 class imbalance, a random split can accidentally create\n",
    "        a val/test set with zero or near-zero negative samples. Stratification\n",
    "        guarantees each split mirrors the overall class ratio.\n",
    "\n",
    "    Why we filter out multi-aspect segments for training:\n",
    "        When a segment maps to multiple aspects (e.g., [FOOD, AMBIENCE]),\n",
    "        the weak label (derived from the whole review) is even noisier for\n",
    "        that segment. We keep only single-aspect segments for cleaner\n",
    "        supervision signal. Multi-aspect segments can still be predicted\n",
    "        at inference time.\n",
    "\n",
    "    Args:\n",
    "        cfg: TrainingConfig instance.\n",
    "\n",
    "    Returns:\n",
    "        Dict with keys: 'train', 'val', 'test' — each a DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LOADING & SPLITTING DATA\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    df = pd.read_pickle(cfg.data_path)\n",
    "    print(f\"  Raw segments loaded: {len(df):,}\")\n",
    "\n",
    "    # --- Filter to single-aspect segments for cleaner weak supervision ----\n",
    "    df[\"num_aspects\"] = df[\"Aspect_Labels\"].apply(len)\n",
    "    df_single = df[df[\"num_aspects\"] == 1].copy()\n",
    "    df_single[\"aspect\"] = df_single[\"Aspect_Labels\"].apply(lambda x: x[0])\n",
    "    print(f\"  Single-aspect segments (for training): {len(df_single):,}\")\n",
    "    print(f\"  Multi-aspect segments (dropped for training): \"\n",
    "          f\"{len(df) - len(df_single):,}\")\n",
    "\n",
    "    # --- Encode labels ---------------------------------------------------\n",
    "    df_single[\"label\"] = df_single[\"Sentiment_Label\"].map(LABEL2ID)\n",
    "\n",
    "    # Sanity check: no NaN labels\n",
    "    assert df_single[\"label\"].isna().sum() == 0, (\n",
    "        \"Found NaN labels! Check Sentiment_Label column values.\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n  Label distribution:\")\n",
    "    print(df_single[\"label\"].value_counts().sort_index().to_string(\n",
    "        index=True).replace(\"0\", \"  0 (negative)\").replace(\"1\", \"  1 (positive)\")\n",
    "    )\n",
    "\n",
    "    # --- Train / Val / Test split (two-stage stratified) -----------------\n",
    "    # Stage 1: Separate test set\n",
    "    df_trainval, df_test = train_test_split(\n",
    "        df_single,\n",
    "        test_size=cfg.test_size,\n",
    "        stratify=df_single[\"label\"],\n",
    "        random_state=cfg.random_seed,\n",
    "    )\n",
    "    # Stage 2: Split remainder into train + val\n",
    "    # Adjust val_size relative to the remaining data\n",
    "    adjusted_val_size = cfg.val_size / (1.0 - cfg.test_size)\n",
    "    df_train, df_val = train_test_split(\n",
    "        df_trainval,\n",
    "        test_size=adjusted_val_size,\n",
    "        stratify=df_trainval[\"label\"],\n",
    "        random_state=cfg.random_seed,\n",
    "    )\n",
    "\n",
    "    splits = {\"train\": df_train, \"val\": df_val, \"test\": df_test}\n",
    "\n",
    "    print(f\"\\n  Split sizes:\")\n",
    "    for name, split_df in splits.items():\n",
    "        pos = (split_df[\"label\"] == 1).sum()\n",
    "        neg = (split_df[\"label\"] == 0).sum()\n",
    "        print(f\"    {name:<6}: {len(split_df):>7,} rows \"\n",
    "              f\"| pos: {pos:,} ({pos/len(split_df)*100:.1f}%) \"\n",
    "              f\"| neg: {neg:,} ({neg/len(split_df)*100:.1f}%)\")\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "DATA = load_and_prepare_data(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802384a",
   "metadata": {},
   "source": [
    "# STAGE 3: PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68796217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BUILDING DATALOADERS\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06a765913e243bf8138a96c0481cf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Hui Ling\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ong Hui Ling\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Ong Hui Ling\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61ff89f0b9048618580b73577faaf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c304358f9534fd49fef9ef9741f9933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99e9802584f41b7a65bba8a7357b2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Tokenizer loaded: xlm-roberta-base\n",
      "    Vocab size: 250,002\n",
      "\n",
      "  Dataset sizes & batches:\n",
      "    train :  75,417 samples → 2,357 batches (batch_size=32)\n",
      "    val   :  10,056 samples →  315 batches (batch_size=32)\n",
      "    test  :  15,084 samples →  472 batches (batch_size=32)\n",
      "\n",
      "  Sample input (decoded):\n",
      "    \"<s> FOOD</s></s> blue cheese panini</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\"\n",
      "    Label: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Hui Ling\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Wraps a DataFrame split into a tokenized, aspect-conditioned dataset.\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    \"\"\"Aspect-conditioned sentiment dataset for XLM-RoBERTa.\n",
    "\n",
    "    Input Format (Sun et al., 2019 — Aspect-Based Sentiment):\n",
    "        Input:  \"[aspect] </s></s> [segment text]\"\n",
    "        Label:  0 (negative) or 1 (positive)\n",
    "\n",
    "    Why \"</s></s>\" (double SEP):\n",
    "        XLM-RoBERTa uses </s> as its separator token (unlike BERT's [SEP]).\n",
    "        The double </s></s> pattern is the standard way RoBERTa-family models\n",
    "        denote a sentence boundary — this is baked into its pre-training.\n",
    "\n",
    "    Example:\n",
    "        Input text:  \"FOOD </s></s> the nasi lemak was incredibly sedap\"\n",
    "        Tokenized:   <s> FOOD </s> </s> the nasi lemak was incredibly sedap </s>\n",
    "        Label:       1 (positive)\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns ['aspect', 'Segment', 'label'].\n",
    "        tokenizer: HuggingFace tokenizer for xlm-roberta.\n",
    "        max_length: Maximum token length (default 128).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 128,\n",
    "    ):\n",
    "        self.texts = df[\"Segment\"].tolist()\n",
    "        self.aspects = df[\"aspect\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"Tokenize a single (aspect, segment) pair on-the-fly.\n",
    "\n",
    "        Why on-the-fly tokenization (not pre-tokenized):\n",
    "            For datasets of this size (~100K), pre-tokenizing and caching\n",
    "            in memory is faster but uses ~2-3 GB RAM. On-the-fly keeps\n",
    "            memory footprint low and simplifies the code. If training\n",
    "            speed becomes the bottleneck, switch to a pre-tokenized cache.\n",
    "\n",
    "        Returns:\n",
    "            Dict with 'input_ids', 'attention_mask', 'labels' tensors.\n",
    "        \"\"\"\n",
    "        aspect = self.aspects[idx]\n",
    "        segment = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # --- Construct aspect-conditioned input --------------------------\n",
    "        # Format: \"ASPECT_LABEL </s></s> segment_text\"\n",
    "        # The aspect is uppercased to visually distinguish it as a\n",
    "        # \"prompt token\" — the model learns to treat it as a conditioning\n",
    "        # signal rather than natural language.\n",
    "        conditioned_text = f\"{aspect.upper()} </s></s> {segment}\"\n",
    "\n",
    "        # --- Tokenize ----------------------------------------------------\n",
    "        encoding = self.tokenizer(\n",
    "            conditioned_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",       # Pad all sequences to max_length\n",
    "            truncation=True,            # Truncate if longer\n",
    "            return_tensors=\"pt\",        # Return PyTorch tensors directly\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),       # (max_length,)\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),  # (max_length,)\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),     # scalar\n",
    "        }\n",
    "\n",
    "\n",
    "def build_dataloaders(data: dict, cfg: TrainingConfig) -> dict:\n",
    "    \"\"\"Instantiate tokenizer, datasets, and DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        data: Dict with 'train', 'val', 'test' DataFrames.\n",
    "        cfg: TrainingConfig.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'train', 'val', 'test' DataLoaders and 'tokenizer'.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BUILDING DATALOADERS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load tokenizer (downloads ~1 MB vocab file on first run)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "    print(f\"  ✓ Tokenizer loaded: {cfg.model_name}\")\n",
    "    print(f\"    Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "    loaders = {}\n",
    "    datasets_info = {}\n",
    "\n",
    "    for split_name, df in data.items():\n",
    "        dataset = ABSADataset(df, tokenizer, cfg.max_seq_length)\n",
    "\n",
    "        # Training set uses shuffle; val/test do not\n",
    "        is_train = split_name == \"train\"\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=is_train,\n",
    "            num_workers=0,        # 0 = main process (safe on Windows)\n",
    "            pin_memory=True,      # Faster CPU→GPU transfer\n",
    "        )\n",
    "        loaders[split_name] = loader\n",
    "        datasets_info[split_name] = len(dataset)\n",
    "\n",
    "    loaders[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    print(f\"\\n  Dataset sizes & batches:\")\n",
    "    for name, size in datasets_info.items():\n",
    "        n_batches = size // cfg.batch_size + (1 if size % cfg.batch_size else 0)\n",
    "        print(f\"    {name:<6}: {size:>7,} samples → {n_batches:>4,} batches \"\n",
    "              f\"(batch_size={cfg.batch_size})\")\n",
    "\n",
    "    # --- Quick sanity check: decode one sample ---------------------------\n",
    "    sample_batch = next(iter(loaders[\"train\"]))\n",
    "    sample_text = tokenizer.decode(\n",
    "        sample_batch[\"input_ids\"][0], skip_special_tokens=False\n",
    "    )\n",
    "    print(f\"\\n  Sample input (decoded):\")\n",
    "    print(f\"    \\\"{sample_text}\\\"\")\n",
    "    print(f\"    Label: {ID2LABEL[sample_batch['labels'][0].item()]}\")\n",
    "\n",
    "    return loaders\n",
    "\n",
    "\n",
    "LOADERS = build_dataloaders(DATA, CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770bc9e",
   "metadata": {},
   "source": [
    "# STAGE 4: Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971bcec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BUILDING MODEL\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0131 14:57:07.220000 20524 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f02141f552543f99168ca0b07f73e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Model loaded on cpu\n",
      "    Total params:      278,045,186\n",
      "    Trainable params:  278,045,186\n",
      "    Class weights:    [4.5448, 0.5618]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# XLM-RoBERTa + a 2-class classification head + class-weighted loss.\n",
    "# ==============================================================================\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ABSASentimentClassifier(nn.Module):\n",
    "    \"\"\"XLM-RoBERTa with a classification head for binary sentiment.\n",
    "\n",
    "    Why AutoModelForSequenceClassification instead of raw AutoModel:\n",
    "        The \"ForSequenceClassification\" variant already includes:\n",
    "          - The [CLS] token pooling (first token representation)\n",
    "          - A dropout layer\n",
    "          - A linear projection to num_labels\n",
    "        Building these manually adds no value and risks subtle bugs\n",
    "        (e.g., forgetting dropout → overfitting).\n",
    "\n",
    "    Why we store class_weights on the model:\n",
    "        This ensures the weights move to the correct device (CPU/GPU)\n",
    "        alongside the model when .to(device) is called. Forgetting this\n",
    "        is one of the most common PyTorch bugs.\n",
    "\n",
    "    Args:\n",
    "        cfg: TrainingConfig.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModelForSequenceClassification.from_pretrained(\n",
    "            cfg.model_name,\n",
    "            num_labels=cfg.num_labels,\n",
    "        )\n",
    "        # Register class weights as a buffer (not a parameter —\n",
    "        # it won't be updated by the optimizer, but WILL move with .to())\n",
    "        self.register_buffer(\n",
    "            \"class_weights\",\n",
    "            torch.tensor(cfg.class_weights, dtype=torch.float),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        labels: torch.Tensor = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len).\n",
    "            attention_mask: 1 for real tokens, 0 for padding (batch_size, seq_len).\n",
    "            labels: Ground truth labels (batch_size,). Optional — if None,\n",
    "                    only logits are returned (useful for inference).\n",
    "\n",
    "        Returns:\n",
    "            Dict with 'loss' (if labels provided) and 'logits'.\n",
    "        \"\"\"\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        logits = outputs.logits  # (batch_size, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "def build_model(cfg: TrainingConfig) -> ABSASentimentClassifier:\n",
    "    \"\"\"Instantiate model and move to device.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BUILDING MODEL\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    model = ABSASentimentClassifier(cfg)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"  ✓ Model loaded on {DEVICE}\")\n",
    "    print(f\"    Total params:     {total_params:>12,}\")\n",
    "    print(f\"    Trainable params: {trainable_params:>12,}\")\n",
    "    print(f\"    Class weights:    {cfg.class_weights}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL = build_model(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3468a",
   "metadata": {},
   "source": [
    "# STAGE 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60febb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "def compute_metrics(predictions: list, labels: list) -> dict:\n",
    "    \"\"\"Compute accuracy, macro-F1, and per-class F1.\n",
    "\n",
    "    Why macro-F1 over accuracy:\n",
    "        With 89/11 imbalance, a model predicting \"positive\" always gets\n",
    "        89% accuracy. Macro-F1 weights both classes equally, so it\n",
    "        actually measures whether the model learned the minority class.\n",
    "\n",
    "    Args:\n",
    "        predictions: List of predicted class IDs.\n",
    "        labels: List of ground-truth class IDs.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'accuracy', 'macro_f1', 'neg_f1', 'pos_f1'.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "    per_class_f1 = f1_score(labels, predictions, average=None)  # [neg_f1, pos_f1]\n",
    "    return {\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"macro_f1\": round(macro_f1, 4),\n",
    "        \"neg_f1\": round(per_class_f1[0], 4),\n",
    "        \"pos_f1\": round(per_class_f1[1], 4),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device) -> dict:\n",
    "    \"\"\"Run one full training epoch.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'loss' (average over all batches).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Gradient clipping: prevents exploding gradients in transformers\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return {\"loss\": round(total_loss / num_batches, 6)}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device) -> dict:\n",
    "    \"\"\"Run evaluation on val or test set (no gradient computation).\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'loss', 'accuracy', 'macro_f1', 'neg_f1', 'pos_f1'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        total_loss += outputs[\"loss\"].item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Argmax → predicted class\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy().tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics[\"loss\"] = round(total_loss / num_batches, 6)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train(model, loaders, cfg, device) -> dict:\n",
    "    \"\"\"Full training loop with early stopping on val macro-F1.\n",
    "\n",
    "    Why early stopping on macro-F1 (not loss):\n",
    "        Validation loss can continue decreasing even as the model starts\n",
    "        overfitting to the majority class. Macro-F1 directly measures\n",
    "        what we care about: balanced performance on both classes.\n",
    "\n",
    "    Args:\n",
    "        model: ABSASentimentClassifier (already on device).\n",
    "        loaders: Dict with 'train', 'val', 'test' DataLoaders.\n",
    "        cfg: TrainingConfig.\n",
    "        device: torch.device.\n",
    "\n",
    "    Returns:\n",
    "        Dict with full training history (for plotting / logging).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # --- Optimizer: AdamW (standard for transformer fine-tuning) --------\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.learning_rate,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "    )\n",
    "\n",
    "    # --- Learning rate scheduler: linear warm-up then linear decay -------\n",
    "    # Why: Transformers are sensitive to LR. A warm-up phase prevents\n",
    "    # catastrophic early updates to pre-trained weights.\n",
    "    total_steps = len(loaders[\"train\"]) * cfg.num_epochs\n",
    "    warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "    print(f\"  Total training steps:  {total_steps:,}\")\n",
    "    print(f\"  Warmup steps:          {warmup_steps:,}\")\n",
    "\n",
    "    # --- Output directory -------------------------------------------------\n",
    "    os.makedirs(os.path.dirname(cfg.best_model_path) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.metrics_path) or \".\", exist_ok=True)\n",
    "\n",
    "    # --- Training history -------------------------------------------------\n",
    "    history = {\"train\": [], \"val\": [], \"test\": None}\n",
    "    best_val_f1 = -1.0\n",
    "    patience = 2   # Stop if val macro-F1 doesn't improve for 2 epochs\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, cfg.num_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # --- Train ---\n",
    "        train_metrics = train_epoch(model, loaders[\"train\"], optimizer,\n",
    "                                    scheduler, device)\n",
    "        history[\"train\"].append(train_metrics)\n",
    "\n",
    "        # --- Validate ---\n",
    "        val_metrics = evaluate(model, loaders[\"val\"], device)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        # --- Log -------------------------------------------------------------\n",
    "        print(f\"\\n  Epoch {epoch}/{cfg.num_epochs}  ({epoch_time:.1f}s)\")\n",
    "        print(f\"    Train Loss:      {train_metrics['loss']:.6f}\")\n",
    "        print(f\"    Val  Loss:       {val_metrics['loss']:.6f}\")\n",
    "        print(f\"    Val  Accuracy:   {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"    Val  Macro-F1:   {val_metrics['macro_f1']:.4f}  \"\n",
    "              f\"(neg: {val_metrics['neg_f1']:.4f} | \"\n",
    "              f\"pos: {val_metrics['pos_f1']:.4f})\")\n",
    "\n",
    "        # --- Early stopping & best-model checkpoint -------------------------\n",
    "        if val_metrics[\"macro_f1\"] > best_val_f1:\n",
    "            best_val_f1 = val_metrics[\"macro_f1\"]\n",
    "            patience_counter = 0\n",
    "            # Save only the model state_dict (not the whole object)\n",
    "            torch.save(model.state_dict(), cfg.best_model_path)\n",
    "            print(f\"    ★ New best model saved  (macro-F1: {best_val_f1:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"    ✗ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n  ⚡ Early stopping at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "    # --- Final evaluation on TEST set (only once, after training) -----------\n",
    "    # Load the best checkpoint before evaluating\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"  LOADING BEST MODEL FOR FINAL TEST EVALUATION\")\n",
    "    print(\"-\" * 70)\n",
    "    model.load_state_dict(torch.load(cfg.best_model_path, map_location=device))\n",
    "\n",
    "    test_metrics = evaluate(model, loaders[\"test\"], device)\n",
    "    history[\"test\"] = test_metrics\n",
    "\n",
    "    print(f\"\\n  ★ FINAL TEST RESULTS (best checkpoint):\")\n",
    "    print(f\"    Test Loss:       {test_metrics['loss']:.6f}\")\n",
    "    print(f\"    Test Accuracy:   {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"    Test Macro-F1:   {test_metrics['macro_f1']:.4f}\")\n",
    "    print(f\"      Negative F1:   {test_metrics['neg_f1']:.4f}\")\n",
    "    print(f\"      Positive  F1:  {test_metrics['pos_f1']:.4f}\")\n",
    "\n",
    "    # --- Full classification report (for the report / thesis) --------------\n",
    "    # Re-run test set to collect all predictions for the report\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loaders[\"test\"]:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(batch[\"labels\"].numpy().tolist())\n",
    "\n",
    "    print(f\"\\n  Classification Report:\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_preds,\n",
    "        target_names=[\"Negative\", \"Positive\"],\n",
    "    ))\n",
    "\n",
    "    # --- Save training history as JSON ------------------------------------\n",
    "    with open(cfg.metrics_path, \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"  ✓ Training metrics saved to: {cfg.metrics_path}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835fdf30",
   "metadata": {},
   "source": [
    "# STAGE 6: Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2a9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING\n",
      "======================================================================\n",
      "  Total training steps:  11,785\n",
      "  Warmup steps:          1,178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Hui Ling\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "HISTORY = train(MODEL, LOADERS, CFG, DEVICE)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Best model:   {CFG.best_model_path}\")\n",
    "print(f\"  Metrics file: {CFG.metrics_path}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
