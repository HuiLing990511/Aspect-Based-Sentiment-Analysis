{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38297865",
   "metadata": {},
   "source": [
    "# Traditional ML Baseline for ABSA Sentiment Classification\n",
    "\n",
    "**Objective:** Train Random Forest classifier with TF-IDF features as baseline to compare with XLM-RoBERTa\n",
    "\n",
    "**Academic Justification:**\n",
    "- Establishes baseline performance using classical ML (Scikit-learn Random Forest)\n",
    "- TF-IDF captures term importance without contextual embeddings\n",
    "- Comparison validates whether transformer pre-training provides value for Manglish code-switching\n",
    "- Following best practices: same train/test split, same evaluation protocol, same class imbalance handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e89709",
   "metadata": {},
   "source": [
    "# Stage 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to google drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# 1. Mount Google Drive (To save the model checkpoints)\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e635b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ML & Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ“ All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caaa0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ NLTK resources downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Ong Hui\n",
      "[nltk_data]     Ling\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"âœ“ NLTK resources downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b61515",
   "metadata": {},
   "source": [
    "# Stage 1: Configuration & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8600f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  Data path: C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\aspect_categorization_after_filtering.pkl\n",
      "  Gold path: C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\Final_Gold_Standard.csv\n",
      "  Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "#DATA_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/aspect_categorization_after_filtering.pkl'  \n",
    "#GOLD_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/Final_Gold_Standard.csv'\n",
    "DATA_PATH = r'C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\aspect_categorization_after_filtering.pkl' \n",
    "GOLD_PATH = r'C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\Final_Gold_Standard.csv'\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.10\n",
    "\n",
    "# Label encoding\n",
    "LABEL2ID = {\"negative\": 0, \"positive\": 1}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "print(f\"âœ“ Configuration loaded\")\n",
    "print(f\"  Data path: {DATA_PATH}\")\n",
    "print(f\"  Gold path: {GOLD_PATH}\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a2e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING & PREPARING DATA\n",
      "======================================================================\n",
      "  Raw segments loaded: 128,778\n",
      "\n",
      "  âš ï¸  DATA LEAKAGE PREVENTION:\n",
      "  Loading gold standard to identify held-out review IDs...\n",
      "  âœ“ Gold dataset loaded: 645 annotations\n",
      "  âœ“ Unique review IDs in gold: 131\n",
      "  âœ“ Filtered out 996 segments from gold reviews (0.8%)\n",
      "  âœ“ Training segments remaining: 127,782\n",
      "\n",
      "  FILTERING STRATEGY:\n",
      "    Single-aspect segments:   97,911 ( 76.6%) â†’ KEPT\n",
      "    Multi-aspect segments:    29,871 ( 23.4%) â†’ DROPPED\n",
      "\n",
      "  Label distribution:\n",
      "    negative  :   6,498 (  6.6%)\n",
      "    positive  :  91,413 ( 93.4%)\n",
      "\n",
      "âœ“ Data preparation complete: 97,911 segments ready for training\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING & PREPARING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load training data\n",
    "df = pd.read_pickle(DATA_PATH)\n",
    "print(f\"  Raw segments loaded: {len(df):,}\")\n",
    "\n",
    "# --- PREVENT DATA LEAKAGE: Exclude gold standard review IDs ----------\n",
    "print(f\"\\n  âš ï¸  DATA LEAKAGE PREVENTION:\")\n",
    "print(f\"  Loading gold standard to identify held-out review IDs...\")\n",
    "\n",
    "try:\n",
    "    gold_df = pd.read_csv(GOLD_PATH)\n",
    "    \n",
    "    # Extract unique Original_Review_IDs from gold dataset\n",
    "    if 'Original_Review_ID' in gold_df.columns:\n",
    "        gold_review_ids = set(gold_df['Original_Review_ID'].unique())\n",
    "    elif 'Review_ID' in gold_df.columns:\n",
    "        gold_review_ids = set(gold_df['Review_ID'].unique())\n",
    "    else:\n",
    "        print(f\"  âš ï¸  Warning: Could not find review ID column in gold dataset\")\n",
    "        gold_review_ids = set()\n",
    "    \n",
    "    print(f\"  âœ“ Gold dataset loaded: {len(gold_df):,} annotations\")\n",
    "    print(f\"  âœ“ Unique review IDs in gold: {len(gold_review_ids):,}\")\n",
    "    \n",
    "    # Filter out segments from gold review IDs\n",
    "    n_before = len(df)\n",
    "    df = df[~df['Original_Review_ID'].isin(gold_review_ids)].copy()\n",
    "    n_after = len(df)\n",
    "    n_removed = n_before - n_after\n",
    "    \n",
    "    print(f\"  âœ“ Filtered out {n_removed:,} segments from gold reviews ({n_removed/n_before*100:.1f}%)\")\n",
    "    print(f\"  âœ“ Training segments remaining: {n_after:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  âœ— Error loading gold dataset: {e}\")\n",
    "\n",
    "# Filter to single-aspect segments\n",
    "df[\"num_aspects\"] = df[\"Aspect_Labels\"].apply(len)\n",
    "df_single = df[df[\"num_aspects\"] == 1].copy()\n",
    "df_single[\"aspect\"] = df_single[\"Aspect_Labels\"].apply(lambda x: x[0])\n",
    "\n",
    "n_multi = len(df) - len(df_single)\n",
    "pct_retained = (len(df_single) / len(df)) * 100\n",
    "\n",
    "print(f\"\\n  FILTERING STRATEGY:\")\n",
    "print(f\"    Single-aspect segments:  {len(df_single):>7,} ({pct_retained:>5.1f}%) â†’ KEPT\")\n",
    "print(f\"    Multi-aspect segments:   {n_multi:>7,} ({100-pct_retained:>5.1f}%) â†’ DROPPED\")\n",
    "\n",
    "# Encode labels\n",
    "df_single[\"label\"] = df_single[\"Sentiment_Label\"].map(LABEL2ID)\n",
    "\n",
    "print(f\"\\n  Label distribution:\")\n",
    "for label_name, label_id in LABEL2ID.items():\n",
    "    count = (df_single[\"label\"] == label_id).sum()\n",
    "    pct = count / len(df_single) * 100\n",
    "    print(f\"    {label_name:<10}: {count:>7,} ({pct:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ“ Data preparation complete: {len(df_single):,} segments ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d5195",
   "metadata": {},
   "source": [
    "# Stage 2: Text Preprocessing (Conventional NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884e0529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  The nasi lemak was incredibly sedap but the service was lambat!\n",
      "Processed: nasi lemak incredibly sedap service lambat\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Apply conventional NLP preprocessing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Lowercase conversion\n",
    "    2. Remove special characters (keep letters, numbers, spaces)\n",
    "    3. Tokenization\n",
    "    4. Remove English stopwords\n",
    "    5. Lemmatization\n",
    "    \n",
    "    Why:\n",
    "        Traditional ML models (Random Forest, SVM) lack contextual understanding.\n",
    "        Preprocessing reduces noise and dimensionality for TF-IDF vectorization.\n",
    "    \n",
    "    Note:\n",
    "        We do NOT remove Manglish terms (sedap, mamak) as they carry sentiment.\n",
    "        Stopwords removal is conservative to preserve sentiment-bearing phrases.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join back\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = \"The nasi lemak was incredibly sedap but the service was lambat!\"\n",
    "print(f\"Original:  {sample_text}\")\n",
    "print(f\"Processed: {preprocess_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e8d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPROCESSING TEXT DATA\n",
      "======================================================================\n",
      "Processing 97,911 segments...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREPROCESSING TEXT DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply preprocessing to all segments\n",
    "print(f\"Processing {len(df_single):,} segments...\")\n",
    "df_single['processed_text'] = df_single['Segment'].apply(preprocess_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b5e51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EMPTY PROCESSED TEXTS ORIGNIAL SEGMENTS\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segment\n",
       "again                 46\n",
       "once again             4\n",
       "over again             2\n",
       "again)                 1\n",
       "again not è±¬æ²¹æ¸£          1\n",
       "again â¤ï¸ðŸ˜Š              1\n",
       "here again             1\n",
       "was again              1\n",
       "because again          1\n",
       "will be here again     1\n",
       "i was here again       1\n",
       "so soÃ¢â‚¬Â¦               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for empty processed texts\n",
    "print(\"=\"*70)\n",
    "print(\"EMPTY PROCESSED TEXTS ORIGNIAL SEGMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_single[df_single['processed_text'].str.len() == 0]['Segment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c97e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âš ï¸  Warning: 61 segments became empty after preprocessing\n",
      "  âœ“ Removed empty segments. Remaining: 97,850\n",
      "\n",
      "âœ“ Text preprocessing complete\n",
      "\n",
      "Sample processed segments:\n",
      "\n",
      "  Original:  coconut cream - the perfect finish to the meal...\n",
      "  Processed: coconut cream perfect finish meal...\n",
      "\n",
      "  Original:  cooked perfectly...\n",
      "  Processed: cooked perfectly...\n",
      "\n",
      "  Original:  donâ€™t miss the sweet appam with brown sugar...\n",
      "  Processed: dont miss sweet appam brown sugar...\n"
     ]
    }
   ],
   "source": [
    "# Check for empty processed texts\n",
    "n_empty = (df_single['processed_text'].str.len() == 0).sum()\n",
    "\n",
    "if n_empty > 0:\n",
    "    print(f\"  âš ï¸  Warning: {n_empty} segments became empty after preprocessing\")\n",
    "    df_single = df_single[df_single['processed_text'].str.len() > 0].copy()\n",
    "    print(f\"  âœ“ Removed empty segments. Remaining: {len(df_single):,}\")\n",
    "\n",
    "print(f\"\\nâœ“ Text preprocessing complete\")\n",
    "print(f\"\\nSample processed segments:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n  Original:  {df_single.iloc[i]['Segment'][:80]}...\")\n",
    "    print(f\"  Processed: {df_single.iloc[i]['processed_text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846e566",
   "metadata": {},
   "source": [
    "# Stage 3: Train/Val/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e32ef73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAIN/VAL/TEST SPLIT\n",
      "======================================================================\n",
      "\n",
      "Split sizes:\n",
      "  Train :  73,387 rows | pos: 68,519 ( 93.4%) | neg:  4,868 (  6.6%)\n",
      "  Val   :   9,785 rows | pos:  9,136 ( 93.4%) | neg:    649 (  6.6%)\n",
      "  Test  :  14,678 rows | pos: 13,704 ( 93.4%) | neg:    974 (  6.6%)\n",
      "\n",
      "âœ“ Data split complete\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAIN/VAL/TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Stage 1: Separate test set (stratified)\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df_single,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df_single[\"label\"],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Stage 2: Split remainder into train + val\n",
    "adjusted_val_size = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=adjusted_val_size,\n",
    "    stratify=df_trainval[\"label\"],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "for name, split_df in [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]:\n",
    "    pos = (split_df[\"label\"] == 1).sum()\n",
    "    neg = (split_df[\"label\"] == 0).sum()\n",
    "    print(f\"  {name:<6}: {len(split_df):>7,} rows | \"\n",
    "          f\"pos: {pos:>6,} ({pos/len(split_df)*100:>5.1f}%) | \"\n",
    "          f\"neg: {neg:>6,} ({neg/len(split_df)*100:>5.1f}%)\")\n",
    "\n",
    "# Extract X (processed text) and y (labels)\n",
    "X_train = df_train['processed_text'].values\n",
    "X_val = df_val['processed_text'].values\n",
    "X_test = df_test['processed_text'].values\n",
    "\n",
    "y_train = df_train['label'].values\n",
    "y_val = df_val['label'].values\n",
    "y_test = df_test['label'].values\n",
    "\n",
    "print(f\"\\nâœ“ Data split complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efa955",
   "metadata": {},
   "source": [
    "# Stage 4: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e488170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TF-IDF VECTORIZATION\n",
      "======================================================================\n",
      "TF-IDF Configuration:\n",
      "  Max features:  5,000\n",
      "  N-gram range:  (1, 2)\n",
      "  Min doc freq:  2\n",
      "  Max doc freq:  0.8\n",
      "\n",
      "Fitting TF-IDF on 73,387 training samples...\n",
      "\n",
      "âœ“ TF-IDF vectorization complete\n",
      "  Train shape: (73387, 5000)\n",
      "  Val shape:   (9785, 5000)\n",
      "  Test shape:  (14678, 5000)\n",
      "  Vocabulary size: 5,000 terms\n",
      "\n",
      "Top 20 features by TF-IDF score:\n",
      "  food                 2059.51\n",
      "  service              1339.57\n",
      "  good                 1215.44\n",
      "  delicious            1155.81\n",
      "  friendly             1032.15\n",
      "  staff                913.08\n",
      "  chicken              826.22\n",
      "  taste                804.06\n",
      "  price                803.12\n",
      "  nice                 774.98\n",
      "  fresh                739.35\n",
      "  tasty                663.22\n",
      "  great                639.46\n",
      "  place                630.93\n",
      "  attentive            630.19\n",
      "  dish                 605.45\n",
      "  restaurant           541.72\n",
      "  come                 531.13\n",
      "  clean                509.03\n",
      "  rice                 504.46\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TF-IDF VECTORIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,     # Limit to top 5000 features (prevent overfitting)\n",
    "    ngram_range=(1, 2),    # Unigrams + bigrams (capture phrases like \"nasi lemak\")\n",
    "    min_df=2,              # Ignore terms appearing in < 2 documents\n",
    "    max_df=0.8,            # Ignore terms appearing in > 80% of documents\n",
    "    sublinear_tf=True      # Apply sublinear tf scaling (1 + log(tf))\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF Configuration:\")\n",
    "print(f\"  Max features:  {vectorizer.max_features:,}\")\n",
    "print(f\"  N-gram range:  {vectorizer.ngram_range}\")\n",
    "print(f\"  Min doc freq:  {vectorizer.min_df}\")\n",
    "print(f\"  Max doc freq:  {vectorizer.max_df}\")\n",
    "\n",
    "# Fit on training data and transform all splits\n",
    "print(f\"\\nFitting TF-IDF on {len(X_train):,} training samples...\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\nâœ“ TF-IDF vectorization complete\")\n",
    "print(f\"  Train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"  Val shape:   {X_val_tfidf.shape}\")\n",
    "print(f\"  Test shape:  {X_test_tfidf.shape}\")\n",
    "print(f\"  Vocabulary size: {len(vectorizer.vocabulary_):,} terms\")\n",
    "\n",
    "# Show top features by TF-IDF score\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = X_train_tfidf.sum(axis=0).A1\n",
    "top_indices = tfidf_scores.argsort()[-20:][::-1]\n",
    "\n",
    "print(f\"\\nTop 20 features by TF-IDF score:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"  {feature_names[idx]:<20} {tfidf_scores[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905141c",
   "metadata": {},
   "source": [
    "# Stage 5: Multi-Model Training \n",
    "\n",
    "**Academic Justification:**\n",
    "- Compare 5 traditional ML classifiers: Logistic Regression, SVM, Naive Bayes, XGBoost, Random Forest\n",
    "- Evaluate on validation set to identify best model(s) for hyperparameter tuning\n",
    "- Different models capture different patterns: linear (LR, SVM) vs non-linear (RF, XGB) vs probabilistic (NB)\n",
    "- Best practice: broad comparison before expensive hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537af86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING MULTIPLE ML MODELS\n",
      "======================================================================\n",
      "Class imbalance handling:\n",
      "  Negative weight: 7.5377\n",
      "  Positive weight: 0.5355\n",
      "  XGBoost scale_pos_weight: 14.0754\n",
      "\n",
      "======================================================================\n",
      "TRAINING 5 MODELS\n",
      "======================================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "  âœ“ Complete in 2.78 seconds\n",
      "\n",
      "Training SVM (Linear)...\n",
      "  âœ“ Complete in 307.85 seconds\n",
      "\n",
      "Training Naive Bayes...\n",
      "  âœ“ Complete in 0.01 seconds\n",
      "\n",
      "Training XGBoost...\n",
      "  âœ“ Complete in 6.69 seconds\n",
      "\n",
      "Training Random Forest...\n",
      "  âœ“ Complete in 3.35 seconds\n",
      "\n",
      "âœ“ All models trained successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING MULTIPLE ML MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute class weights (for models that support it)\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = {i: class_weights_array[i] for i in range(len(class_weights_array))}\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost (ratio of negative to positive)\n",
    "scale_pos_weight = class_weights_array[0] / class_weights_array[1]\n",
    "\n",
    "print(f\"Class imbalance handling:\")\n",
    "print(f\"  Negative weight: {class_weights_dict[0]:.4f}\")\n",
    "print(f\"  Positive weight: {class_weights_dict[1]:.4f}\")\n",
    "print(f\"  XGBoost scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# Define 5 models with reasonable default parameters\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=class_weights_dict,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"SVM (Linear)\": SVC(\n",
    "        kernel='linear',\n",
    "        class_weight=class_weights_dict,\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    \"Naive Bayes\": MultinomialNB(\n",
    "        alpha=1.0  \n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        class_weight=class_weights_dict,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TRAINING {len(models)} MODELS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Train all models and store results\n",
    "trained_models = {}\n",
    "training_times = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    trained_models[model_name] = model\n",
    "    training_times[model_name] = training_time\n",
    "    \n",
    "    print(f\"  âœ“ Complete in {training_time:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"âœ“ All models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9acccc",
   "metadata": {},
   "source": [
    "# Stage 6: Model Comparison on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8383a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARING MODELS ON VALIDATION SET\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "VALIDATION SET RESULTS (Ranked by Macro-F1)\n",
      "======================================================================\n",
      "\n",
      "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
      "Logistic Regression    0.8239    0.6236       0.3491       0.8982           2.781038\n",
      "       SVM (Linear)    0.8056    0.6062       0.3260       0.8864         307.845950\n",
      "        Naive Bayes    0.9388    0.5905       0.2129       0.9682           0.011084\n",
      "            XGBoost    0.9359    0.5203       0.0739       0.9668           6.688418\n",
      "      Random Forest    0.6003    0.4787       0.2269       0.7305           3.353922\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARING MODELS ON VALIDATION SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate all models on validation set\n",
    "results = []\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute same metrics as BERT for fair comparison.\"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    per_class_f1 = f1_score(y_true, y_pred, average=None, labels=[0, 1])\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"macro_f1\": round(macro_f1, 4),\n",
    "        \"neg_f1\": round(per_class_f1[0], 4),\n",
    "        \"pos_f1\": round(per_class_f1[1], 4),\n",
    "    }\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    # Predict on validation set\n",
    "    y_pred_val = model.predict(X_val_tfidf)\n",
    "    \n",
    "    # Compute metrics\n",
    "    val_metrics = compute_metrics(y_val, y_pred_val)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': val_metrics['accuracy'],\n",
    "        'Macro-F1': val_metrics['macro_f1'],\n",
    "        'Negative F1': val_metrics['neg_f1'],\n",
    "        'Positive F1': val_metrics['pos_f1'],\n",
    "        'Training Time (s)': training_times[model_name]\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"VALIDATION SET RESULTS (Ranked by Macro-F1)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffcb075",
   "metadata": {},
   "source": [
    "# Stage 7: Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341e09de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARING MODELS ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST SET RESULTS (Ranked by Macro-F1)\n",
      "======================================================================\n",
      "\n",
      "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
      "Logistic Regression    0.8193    0.6233       0.3516       0.8950           2.781038\n",
      "        Naive Bayes    0.9419    0.6137       0.2576       0.9698           0.011084\n",
      "       SVM (Linear)    0.8023    0.6097       0.3356       0.8839         307.845950\n",
      "            XGBoost    0.9368    0.5296       0.0920       0.9672           6.688418\n",
      "      Random Forest    0.6032    0.4809       0.2288       0.7329           3.353922\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARING MODELS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate all models on test set\n",
    "results = []\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    # Predict on test set\n",
    "    y_pred_test = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Compute metrics\n",
    "    test_metrics = compute_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': test_metrics['accuracy'],\n",
    "        'Macro-F1': test_metrics['macro_f1'],\n",
    "        'Negative F1': test_metrics['neg_f1'],\n",
    "        'Positive F1': test_metrics['pos_f1'],\n",
    "        'Training Time (s)': training_times[model_name]\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST SET RESULTS (Ranked by Macro-F1)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fa5ef",
   "metadata": {},
   "source": [
    "# Stage 8: Gold Standard Evaluation (Before Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51f17999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATING ON GOLD STANDARD (GROUND TRUTH)\n",
      "======================================================================\n",
      "  Gold dataset loaded: 645 rows\n",
      "  After exploding: 721 aspect-segment pairs\n",
      "\n",
      "Preprocessing gold segments...\n",
      "Vectorizing gold segments...\n",
      "Running inference on gold set using ...\n",
      "\n",
      "======================================================================\n",
      "GOLD SET RESULTS (Ranked by Macro-F1)\n",
      "======================================================================\n",
      "\n",
      "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
      "Logistic Regression    0.8350    0.8197       0.7671       0.8722           2.781038\n",
      "       SVM (Linear)    0.8280    0.8135       0.7615       0.8655         307.845950\n",
      "      Random Forest    0.7642    0.7591       0.7240       0.7942           3.353922\n",
      "        Naive Bayes    0.6755    0.5258       0.2595       0.7922           0.011084\n",
      "            XGBoost    0.6477    0.4601       0.1419       0.7784           6.688418\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATING ON GOLD STANDARD (GROUND TRUTH)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load gold standard\n",
    "gold_df = pd.read_csv(GOLD_PATH)\n",
    "print(f\"  Gold dataset loaded: {len(gold_df):,} rows\")\n",
    "\n",
    "# Prepare gold data (same as BERT)\n",
    "gold_df_prep = gold_df.copy()\n",
    "gold_df_prep.rename(columns={\n",
    "    \"Manual_Aspect\": \"aspect\",\n",
    "    \"Manual_Sentiment\": \"Sentiment_Label\",\n",
    "}, inplace=True)\n",
    "\n",
    "# Normalize sentiment labels to lowercase\n",
    "gold_df_prep[\"Sentiment_Label\"] = gold_df_prep[\"Sentiment_Label\"].str.lower()\n",
    "\n",
    "# Handle multi-aspect segments: explode into separate rows\n",
    "import ast\n",
    "def parse_aspect(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            else:\n",
    "                return [parsed]\n",
    "        except (ValueError, SyntaxError):\n",
    "            return [val]\n",
    "    elif isinstance(val, list):\n",
    "        return val\n",
    "    else:\n",
    "        return [str(val)]\n",
    "\n",
    "gold_df_prep[\"aspect\"] = gold_df_prep[\"aspect\"].apply(parse_aspect)\n",
    "gold_df_exploded = gold_df_prep.explode(\"aspect\").reset_index(drop=True)\n",
    "\n",
    "print(f\"  After exploding: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
    "\n",
    "# Encode labels\n",
    "gold_df_exploded[\"label\"] = gold_df_exploded[\"Sentiment_Label\"].map(LABEL2ID)\n",
    "\n",
    "# Preprocess gold text\n",
    "print(f\"\\nPreprocessing gold segments...\")\n",
    "gold_df_exploded['processed_text'] = gold_df_exploded['Segment'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize gold text\n",
    "print(f\"Vectorizing gold segments...\")\n",
    "X_gold = gold_df_exploded['processed_text'].values\n",
    "X_gold_tfidf = vectorizer.transform(X_gold)\n",
    "y_gold = gold_df_exploded['label'].values\n",
    "\n",
    "# Predict on gold using best model \n",
    "print(f\"Running inference on gold set using ...\")\n",
    "\n",
    "# Evaluate all models on gold data set\n",
    "results = []\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    # Predict on test set\n",
    "    y_pred_gold = model.predict(X_gold_tfidf)\n",
    "    \n",
    "    # Compute metrics\n",
    "    gold_metrics = compute_metrics(y_gold, y_pred_gold)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': gold_metrics['accuracy'],\n",
    "        'Macro-F1': gold_metrics['macro_f1'],\n",
    "        'Negative F1': gold_metrics['neg_f1'],\n",
    "        'Positive F1': gold_metrics['pos_f1'],\n",
    "        'Training Time (s)': training_times[model_name]\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"GOLD SET RESULTS (Ranked by Macro-F1)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "196035b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ† BEST MODEL: Logistic Regression\n",
      "â˜… GOLD TEST SET RESULTS - Logistic Regression (Before Tuning)\n",
      "â˜… Total samples: 721 aspect-segment pairs\n",
      "======================================================================\n",
      "\n",
      "OVERALL PERFORMANCE:\n",
      "  Accuracy:  0.8350\n",
      "  Macro-F1:  0.8197\n",
      "    Negative F1: 0.7671\n",
      "    Positive F1: 0.8722\n",
      "\n",
      "âš ï¸  Note: This is baseline performance before hyperparameter tuning\n"
     ]
    }
   ],
   "source": [
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_macro_f1 = results_df.iloc[0]['Macro-F1']\n",
    "\n",
    "# Store best model for hyperparameter tuning\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ† BEST MODEL: {best_model_name}\")\n",
    "\n",
    "# Best model prediction results\n",
    "y_pred_gold = best_model.predict(X_gold_tfidf)\n",
    "    \n",
    "# Compute metrics\n",
    "gold_metrics = compute_metrics(y_gold, y_pred_gold)\n",
    "\n",
    "print(f\"â˜… GOLD TEST SET RESULTS - {best_model_name} (Before Tuning)\")\n",
    "print(f\"â˜… Total samples: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nOVERALL PERFORMANCE:\")\n",
    "print(f\"  Accuracy:  {gold_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Macro-F1:  {gold_metrics['macro_f1']:.4f}\")\n",
    "print(f\"    Negative F1: {gold_metrics['neg_f1']:.4f}\")\n",
    "print(f\"    Positive F1: {gold_metrics['pos_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  Note: This is baseline performance before hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca082d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PER-ASPECT BREAKDOWN (for comparison with BERT):\n",
      "Aspect                Samples   Accuracy   Macro-F1\n",
      "-------------------- -------- ---------- ----------\n",
      "AMBIENCE                   62     0.8387     0.7567\n",
      "AUTHENTICITY & LOCAL VIBE       16     0.8750     0.7143\n",
      "FOOD                      302     0.8510     0.8267\n",
      "HALAL COMPLIANCE            2     1.0000     1.0000\n",
      "LOCATION                   21     0.6190     0.5714\n",
      "LOYALTY (RETURN INTENT)       91     0.8022     0.7831\n",
      "NON-HALAL ELEMENTS         10     0.7000     0.6703\n",
      "SERVICE                   128     0.8984     0.8981\n",
      "VALUE                      88     0.7727     0.7726\n",
      "nan                         1     1.0000     1.0000\n"
     ]
    }
   ],
   "source": [
    "# Per-aspect breakdown\n",
    "print(f\"\\nPER-ASPECT BREAKDOWN (for comparison with BERT):\")\n",
    "print(f\"{'Aspect':<20} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
    "print(f\"{'-'*20} {'-'*8} {'-'*10} {'-'*10}\")\n",
    "\n",
    "aspects_unique = sorted(gold_df_exploded[\"aspect\"].unique())\n",
    "for aspect in aspects_unique:\n",
    "    mask = gold_df_exploded[\"aspect\"] == aspect\n",
    "    y_aspect = y_gold[mask]\n",
    "    y_pred_aspect = y_pred_gold[mask]\n",
    "    \n",
    "    try:\n",
    "        metrics = compute_metrics(y_aspect, y_pred_aspect)\n",
    "        n_samples = mask.sum()\n",
    "        print(f\"{aspect:<20} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
    "    except:\n",
    "        print(f\"{aspect:<20} {mask.sum():>8} {'N/A':>10} {'N/A':>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "928fd992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FULL CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8305    0.7127    0.7671       275\n",
      "    Positive     0.8371    0.9103    0.8722       446\n",
      "\n",
      "    accuracy                         0.8350       721\n",
      "   macro avg     0.8338    0.8115    0.8197       721\n",
      "weighted avg     0.8346    0.8350    0.8321       721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFULL CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_gold, y_pred_gold, target_names=[\"Negative\", \"Positive\"], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cec374",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "- Best model: Logistic Regression will be used for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf903be5",
   "metadata": {},
   "source": [
    "# Stage 9: Hyperparameter Tuning for Best Model\n",
    "\n",
    "**Academic Justification:**\n",
    "- TF-IDF parameters (n-gram range, feature count, min/max df) affect feature space\n",
    "- Random Forest parameters (n_estimators, max_depth, min_samples) affect model capacity\n",
    "- Joint tuning ensures optimal feature-model combination\n",
    "- Using RandomizedSearchCV for efficiency (tests subset of parameter space)\n",
    "- 3-fold CV on training set prevents overfitting to validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"HYPERPARAMETER TUNING FOR BEST MODEL: {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define parameter distributions based on best model type\n",
    "if best_model_name == \"Random Forest\":\n",
    "    param_distributions = {\n",
    "        'tfidf__max_features': [3000, 5000, 7000, 10000],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'tfidf__min_df': [2, 3, 5],\n",
    "        'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "        'tfidf__sublinear_tf': [True, False],\n",
    "        'clf__n_estimators': [100, 200, 300, 500],\n",
    "        'clf__max_depth': [10, 20, 30, None],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    model_for_tuning = RandomForestClassifier(class_weight=class_weights_dict, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    \n",
    "elif best_model_name == \"XGBoost\":\n",
    "    param_distributions = {\n",
    "        'tfidf__max_features': [3000, 5000, 7000, 10000],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'tfidf__min_df': [2, 3, 5],\n",
    "        'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "        'tfidf__sublinear_tf': [True, False],\n",
    "        'clf__n_estimators': [100, 200, 300, 500],\n",
    "        'clf__max_depth': [3, 5, 7, 9],\n",
    "        'clf__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'clf__subsample': [0.6, 0.8, 1.0],\n",
    "        'clf__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    model_for_tuning = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=RANDOM_SEED, n_jobs=-1, eval_metric='logloss')\n",
    "    \n",
    "elif best_model_name == \"Logistic Regression\":\n",
    "    param_distributions = {\n",
    "        'tfidf__max_features': [3000, 5000, 7000, 10000],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'tfidf__min_df': [2, 3, 5],\n",
    "        'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "        'tfidf__sublinear_tf': [True, False],\n",
    "        'clf__C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    model_for_tuning = LogisticRegression(max_iter=1000, class_weight=class_weights_dict, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    \n",
    "elif best_model_name == \"SVM (Linear)\":\n",
    "    param_distributions = {\n",
    "        'tfidf__max_features': [3000, 5000, 7000, 10000],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'tfidf__min_df': [2, 3, 5],\n",
    "        'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "        'tfidf__sublinear_tf': [True, False],\n",
    "        'clf__C': [0.1, 1.0, 10.0, 100.0],\n",
    "        'clf__kernel': ['linear', 'rbf']\n",
    "    }\n",
    "    model_for_tuning = SVC(class_weight=class_weights_dict, random_state=RANDOM_SEED)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', model_for_tuning)\n",
    "])\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "print(f\"\\nInitializing RandomizedSearchCV...\")\n",
    "print(f\"  Parameter combinations to test: 50\")\n",
    "print(f\"  Cross-validation folds: 3\")\n",
    "print(f\"  Scoring metric: Macro-F1\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting hyperparameter search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "tuning_time = time.time() - start_time\n",
    "print(f\"\\nâœ“ Hyperparameter tuning complete in {tuning_time:.2f} seconds ({tuning_time/60:.2f} minutes)\")\n",
    "\n",
    "# Extract best parameters\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BEST PARAMETERS FOUND (via 3-fold CV on training set)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best CV Macro-F1 Score: {best_score:.4f}\\n\")\n",
    "\n",
    "print(f\"TF-IDF Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    if param.startswith('tfidf__'):\n",
    "        print(f\"  {param.replace('tfidf__', ''):<20}: {value}\")\n",
    "\n",
    "print(f\"\\n{best_model_name} Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    if param.startswith('clf__'):\n",
    "        print(f\"  {param.replace('clf__', ''):<20}: {value}\")\n",
    "\n",
    "# Show top 5 parameter combinations\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOP 5 PARAMETER COMBINATIONS\")\n",
    "print(f\"{'='*70}\")\n",
    "results_df = pd.DataFrame(random_search.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score')\n",
    "\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"\\nRank {i+1}: CV Macro-F1 = {row['mean_test_score']:.4f} (Â±{row['std_test_score']:.4f})\")\n",
    "    \n",
    "    # Print TF-IDF params\n",
    "    tfidf_params = {k: v for k, v in row.items() if k.startswith('param_tfidf__')}\n",
    "    print(f\"  TF-IDF: \", end=\"\")\n",
    "    print(\", \".join([f\"{k.replace('param_tfidf__', '')}={v}\" for k, v in list(tfidf_params.items())[:3]]))\n",
    "    \n",
    "    # Print model params\n",
    "    clf_params = {k: v for k, v in row.items() if k.startswith('param_clf__')}\n",
    "    print(f\"  {best_model_name}: \", end=\"\")\n",
    "    print(\", \".join([f\"{k.replace('param_clf__', '')}={v}\" for k, v in list(clf_params.items())[:3]]))\n",
    "\n",
    "# Save best estimator (pipeline) for later use\n",
    "best_pipeline = random_search.best_estimator_\n",
    "best_vectorizer_tuned = best_pipeline.named_steps['tfidf']\n",
    "best_clf_tuned = best_pipeline.named_steps['clf']\n",
    "\n",
    "print(f\"\\nâœ“ Tuned pipeline saved to: best_pipeline, best_vectorizer_tuned, best_clf_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6cc70",
   "metadata": {},
   "source": [
    "# Stage 10: Final Evaluation with Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(f\"FINAL EVALUATION: TUNED {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use tuned pipeline\n",
    "print(f\"Using tuned pipeline from Stage 8...\")\n",
    "print(f\"  âœ“ Model: {best_model_name}\")\n",
    "print(f\"  âœ“ Best CV Macro-F1: {random_search.best_score_:.4f}\")\n",
    "print(f\"  âœ“ Trained on {len(X_train):,} samples\")\n",
    "\n",
    "# Extract tuned components\n",
    "tuned_vectorizer = best_vectorizer_tuned\n",
    "tuned_clf = best_clf_tuned\n",
    "\n",
    "print(f\"\\nTuned Model Configuration:\")\n",
    "print(f\"  TF-IDF max_features:  {tuned_vectorizer.max_features}\")\n",
    "print(f\"  TF-IDF ngram_range:   {tuned_vectorizer.ngram_range}\")\n",
    "print(f\"  TF-IDF vocabulary:    {len(tuned_vectorizer.vocabulary_):,} terms\")\n",
    "\n",
    "# Show top 3 model-specific parameters\n",
    "model_params = {k: v for k, v in best_params.items() if k.startswith('clf__')}\n",
    "for i, (param, value) in enumerate(list(model_params.items())[:3]):\n",
    "    print(f\"  {param.replace('clf__', '')}:  {value}\")\n",
    "\n",
    "# === TEST SET EVALUATION ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST SET EVALUATION (After Tuning)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "y_pred_test_tuned = best_pipeline.predict(X_test)\n",
    "test_metrics_tuned = compute_metrics(y_test, y_pred_test_tuned)\n",
    "\n",
    "print(f\"\\nâ˜… TEST SET RESULTS - Tuned {best_model_name}:\")\n",
    "print(f\"  Accuracy:    {test_metrics_tuned['accuracy']:.4f}\")\n",
    "print(f\"  Macro-F1:    {test_metrics_tuned['macro_f1']:.4f}\")\n",
    "print(f\"    Negative F1: {test_metrics_tuned['neg_f1']:.4f}\")\n",
    "print(f\"    Positive F1: {test_metrics_tuned['pos_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_tuned, target_names=[\"Negative\", \"Positive\"], digits=4))\n",
    "\n",
    "# === GOLD STANDARD EVALUATION ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"GOLD STANDARD EVALUATION (After Tuning)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Load and prepare gold data (same as Stage 7)\n",
    "gold_df = pd.read_csv(GOLD_PATH)\n",
    "gold_df_prep = gold_df.copy()\n",
    "gold_df_prep.rename(columns={\n",
    "    \"Manual_Aspect\": \"aspect\",\n",
    "    \"Manual_Sentiment\": \"Sentiment_Label\",\n",
    "}, inplace=True)\n",
    "gold_df_prep[\"Sentiment_Label\"] = gold_df_prep[\"Sentiment_Label\"].str.lower()\n",
    "\n",
    "import ast\n",
    "def parse_aspect(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            return parsed if isinstance(parsed, list) else [parsed]\n",
    "        except (ValueError, SyntaxError):\n",
    "            return [val]\n",
    "    elif isinstance(val, list):\n",
    "        return val\n",
    "    else:\n",
    "        return [str(val)]\n",
    "\n",
    "gold_df_prep[\"aspect\"] = gold_df_prep[\"aspect\"].apply(parse_aspect)\n",
    "gold_df_exploded = gold_df_prep.explode(\"aspect\").reset_index(drop=True)\n",
    "gold_df_exploded[\"label\"] = gold_df_exploded[\"Sentiment_Label\"].map(LABEL2ID)\n",
    "gold_df_exploded['processed_text'] = gold_df_exploded['Segment'].apply(preprocess_text)\n",
    "\n",
    "# Predict on gold with tuned model\n",
    "y_gold = gold_df_exploded['label'].values\n",
    "y_pred_gold_tuned = best_pipeline.predict(gold_df_exploded['processed_text'].values)\n",
    "\n",
    "gold_metrics_tuned = compute_metrics(y_gold, y_pred_gold_tuned)\n",
    "\n",
    "print(f\"\\nâ˜… GOLD TEST SET RESULTS - Tuned {best_model_name}:\")\n",
    "print(f\"â˜… Total samples: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
    "print(f\"\\nOVERALL PERFORMANCE:\")\n",
    "print(f\"  Accuracy:  {gold_metrics_tuned['accuracy']:.4f}\")\n",
    "print(f\"  Macro-F1:  {gold_metrics_tuned['macro_f1']:.4f}\")\n",
    "print(f\"    Negative F1: {gold_metrics_tuned['neg_f1']:.4f}\")\n",
    "print(f\"    Positive F1: {gold_metrics_tuned['pos_f1']:.4f}\")\n",
    "\n",
    "# Per-aspect breakdown\n",
    "print(f\"\\nPER-ASPECT BREAKDOWN:\")\n",
    "print(f\"{'Aspect':<20} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
    "print(f\"{'-'*20} {'-'*8} {'-'*10} {'-'*10}\")\n",
    "\n",
    "aspects_unique = sorted(gold_df_exploded[\"aspect\"].unique())\n",
    "for aspect in aspects_unique:\n",
    "    mask = gold_df_exploded[\"aspect\"] == aspect\n",
    "    y_aspect = y_gold[mask]\n",
    "    y_pred_aspect = y_pred_gold_tuned[mask]\n",
    "    \n",
    "    try:\n",
    "        metrics = compute_metrics(y_aspect, y_pred_aspect)\n",
    "        n_samples = mask.sum()\n",
    "        print(f\"{aspect:<20} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
    "    except:\n",
    "        print(f\"{aspect:<20} {mask.sum():>8} {'N/A':>10} {'N/A':>10}\")\n",
    "\n",
    "print(f\"\\nFULL CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_gold, y_pred_gold_tuned, target_names=[\"Negative\", \"Positive\"], digits=4))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ FINAL MODEL EVALUATION COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
