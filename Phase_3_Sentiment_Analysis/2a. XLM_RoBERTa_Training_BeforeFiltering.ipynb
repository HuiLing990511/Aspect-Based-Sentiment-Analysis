{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ffec8e71",
      "metadata": {},
      "source": [
        "# XLM-RoBERTa Aspect-Based Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09899414",
      "metadata": {},
      "source": [
        "- INPUT     : Dataset/aspect_categorization.pkl\n",
        "- OUTPUT    : \n",
        "    - models/xlm_roberta_absa_best.pt     (best checkpoint)\n",
        "    - results/training_metrics.json        (loss/acc curves)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a797005c",
      "metadata": {},
      "source": [
        "## ACADEMIC JUSTIFICATION\n",
        "- XLM-RoBERTa (Conneau et al., 2020): Pre-trained on 100 languages including Malay and Chinese. Superior zero/few-shot cross-lingual transfer vs. monolingual BERT, critical for Manglish code-switching.\n",
        "- Aspect-Conditioned Input (Sun et al., 2019): We prepend the aspect category to the segment text as \"[aspect] [SEP] [segment]\". This forces the model to learn aspect-specific sentiment representations rather than general polarity.\n",
        "- Class-Weighted Loss (Japkowicz & Stephen, 2002): Our dataset is severely imbalanced. We use the inverse-frequency weights method to prevent the model from trivially predicting \"positive\".\n",
        "- Weak Supervision (Ratner et al., 2016): Star ratings are noisy proxies for sentiment. \n",
        "\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18ea3c5",
      "metadata": {
        "id": "b18ea3c5"
      },
      "source": [
        "# STAGE 0: Environment & Dependency Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eCJk5vDPVr_5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCJk5vDPVr_5",
        "outputId": "bebd6ca4-7644-44c3-d194-28d6d97a78ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis\n",
            "ðŸ”„ Updating your repo...\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Aspect-Based-Sentiment-Analysis\n",
            "Already up to date.\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis\n",
            "/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Aspect-Based-Sentiment-Analysis\n",
            "\n",
            "READY! We are now running on Google's Computer.\n"
          ]
        }
      ],
      "source": [
        "# Connect to google drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive (To save the model checkpoints)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Install Libraries \n",
        "!pip install transformers accelerate tokenizers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f2fb06",
      "metadata": {
        "id": "c4f2fb06"
      },
      "outputs": [],
      "source": [
        "# pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4fec26",
      "metadata": {
        "id": "7a4fec26"
      },
      "outputs": [],
      "source": [
        "# Fix PyTorch DLL loading issue on Windows\n",
        "import os\n",
        "import platform\n",
        "if platform.system() == \"Windows\":\n",
        "    import ctypes\n",
        "    from importlib.util import find_spec\n",
        "    try:\n",
        "        if (spec := find_spec(\"torch\")) and spec.origin and os.path.exists(\n",
        "            dll_path := os.path.join(os.path.dirname(spec.origin), \"lib\", \"c10.dll\")\n",
        "        ):\n",
        "            ctypes.CDLL(os.path.normpath(dll_path))\n",
        "    except Exception:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a870b8d3",
      "metadata": {
        "id": "a870b8d3",
        "outputId": "67149857-b061-4410-f37a-0a776bae560f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ENVIRONMENT CHECK\n",
            "======================================================================\n",
            "  âœ“  PyTorch                        v2.8.0+cpu\n",
            "  âœ“  HuggingFace Transformers       v4.41.2\n",
            "  âœ“  Pandas                         v2.2.3\n",
            "  âœ“  NumPy                          v2.2.0\n",
            "  âœ“  Scikit-Learn                   v1.5.1\n",
            "\n",
            "  GPU Available: False  â†’  CPU only\n",
            "  Python:        3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import importlib\n",
        "\n",
        "REQUIRED = {\n",
        "    \"torch\": \"PyTorch\",\n",
        "    \"transformers\": \"HuggingFace Transformers\",\n",
        "    \"pandas\": \"Pandas\",\n",
        "    \"numpy\": \"NumPy\",\n",
        "    \"sklearn\": \"Scikit-Learn\",\n",
        "}\n",
        "\n",
        "\n",
        "def check_environment():\n",
        "    \"\"\"Verify all required packages are installed and print versions.\n",
        "\n",
        "    Why:\n",
        "        Explicit environment checks prevent cryptic import errors mid-training,\n",
        "        which is especially costly when running on GPU with long epoch times.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ENVIRONMENT CHECK\")\n",
        "    print(\"=\" * 70)\n",
        "    all_ok = True\n",
        "    for module_name, display_name in REQUIRED.items():\n",
        "        try:\n",
        "            mod = importlib.import_module(module_name)\n",
        "            version = getattr(mod, \"__version__\", \"unknown\")\n",
        "            print(f\"  âœ“  {display_name:<30} v{version}\")\n",
        "        except ImportError:\n",
        "            print(f\"  âœ—  {display_name:<30} NOT INSTALLED\")\n",
        "            all_ok = False\n",
        "\n",
        "    # Special check: torch CUDA availability\n",
        "    import torch\n",
        "\n",
        "    cuda_avail = torch.cuda.is_available()\n",
        "    device_name = torch.cuda.get_device_name(0) if cuda_avail else \"CPU only\"\n",
        "    print(f\"\\n  GPU Available: {cuda_avail}  â†’  {device_name}\")\n",
        "    print(f\"  Python:        {sys.version}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if not all_ok:\n",
        "        raise RuntimeError(\n",
        "            \"Some packages are missing. Install them before continuing.\"\n",
        "        )\n",
        "    return torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
        "\n",
        "\n",
        "DEVICE = check_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d863039",
      "metadata": {
        "id": "5d863039"
      },
      "source": [
        "# STAGE 1: Hyperparameter Configuration (Single Source of Truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5f8a24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5f8a24",
        "outputId": "2d32863d-bf51-4507-bfa9-fca6f96e80d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Config loaded. Model: xlm-roberta-base | Epochs: 5 | Batch: 32 | LR: 2e-05\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Central configuration object for the entire training run.\n",
        "\n",
        "    Why a dataclass:\n",
        "        Keeps hyperparameters serializable (can be logged to JSON for\n",
        "        reproducibility) and gives IDE auto-completion â€” important when\n",
        "        iterating quickly on a GPU budget.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Model -----------------------------------------------------------\n",
        "    model_name: str = \"xlm-roberta-base\"\n",
        "    # \"xlm-roberta-base\" (278M params) is the default.\n",
        "    # Switch to \"xlm-roberta-large\" (550M) if GPU memory allows (â‰¥16 GB).\n",
        "\n",
        "    num_labels: int = 2  # 0 = negative, 1 = positive\n",
        "\n",
        "    # --- Data ------------------------------------------------------------\n",
        "    data_path: str = r\"/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/aspect_categorization_before_filtering.pkl\"\n",
        "    max_seq_length: int = 128\n",
        "    # 128 covers ~95th percentile of the segment lengths (median ~54 words).\n",
        "    # Increase to 256 only if you see significant truncation in logs.\n",
        "\n",
        "    test_size: float = 0.15  # 15% held out for evaluation\n",
        "    val_size: float = 0.10   # 10% for early-stopping validation\n",
        "    random_seed: int = 42\n",
        "\n",
        "    # --- Training --------------------------------------------------------\n",
        "    batch_size: int = 32      \n",
        "    learning_rate: float = 1e-5  # Reduced from 2e-5 for better minority class learning\n",
        "    num_epochs: int = 7        # Increased from 5 to give model more time\n",
        "    warmup_ratio: float = 0.1  # 10% of total steps used for LR warm-up\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # --- Class Weights (computed dynamically from training data) ---------\n",
        "    # Manual override for stronger minority class emphasis\n",
        "    class_weights: Optional[List[float]] = field(default_factory=lambda: [6.0, 1.0])\n",
        "    # Using field(default_factory) because lists are mutable and can't be direct defaults\n",
        "    # Formula when None: n_samples / (n_classes * np.bincount(y))\n",
        "    # This ensures minority class gets higher weight to balance gradients\n",
        "\n",
        "    # --- Output ----------------------------------------------------------\n",
        "    output_dir: str = \"models\"\n",
        "    best_model_path: str = \"/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/xlm_roberta_absa_best_before_filtering.pt\"\n",
        "    metrics_path: str = \"/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/results/training_metrics_before_filtering.json\"\n",
        "\n",
        "    # --- Gold Standard Dataset (for final evaluation) --------------------\n",
        "    # Manually-annotated ground truth \n",
        "    gold_data_path: str = r\"/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/Dataset/Final_Gold_Standard.csv\"\n",
        "    gold_results_path: str = \"/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/results/gold_evaluation.json\"\n",
        "\n",
        "\n",
        "CFG = TrainingConfig()\n",
        "print(f\"\\nâœ“ Config loaded. Model: {CFG.model_name} | Epochs: {CFG.num_epochs} | \"\n",
        "\n",
        "      f\"Batch: {CFG.batch_size} | LR: {CFG.learning_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be7a186",
      "metadata": {
        "id": "0be7a186"
      },
      "source": [
        "# STAGE 2: Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9546dc82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9546dc82",
        "outputId": "6f2f70c7-fba3-4cdf-b851-f74efd63dd1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING & SPLITTING DATA\n",
            "======================================================================\n",
            "  Raw segments loaded: 132,637\n",
            "  Single-aspect segments (for training): 100,557\n",
            "  Multi-aspect segments (dropped for training): 32,080\n",
            "\n",
            "  Label distribution:\n",
            "label\n",
            "  0 (negative)     7673\n",
            "  1 (positive)    92884\n",
            "\n",
            "  Split sizes:\n",
            "    train :  75,417 rows | pos: 69,662 (92.4%) | neg: 5,755 (7.6%)\n",
            "    val   :  10,056 rows | pos: 9,289 (92.4%) | neg: 767 (7.6%)\n",
            "    test  :  15,084 rows | pos: 13,933 (92.4%) | neg: 1,151 (7.6%)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Transforms aspect_categorization.pkl â†’ train/val/test splits\n",
        "# ready for the PyTorch DataLoader.\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Label encoding map (explicit is better than implicit)\n",
        "LABEL2ID = {\"negative\": 0, \"positive\": 1}\n",
        "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "\n",
        "def load_and_prepare_data(cfg: TrainingConfig) -> dict:\n",
        "    \"\"\"Load aspect segments and split into train / val / test.\n",
        "\n",
        "    Why stratified split:\n",
        "        With 89/11 class imbalance, a random split can accidentally create\n",
        "        a val/test set with zero or near-zero negative samples. Stratification\n",
        "        guarantees each split mirrors the overall class ratio.\n",
        "\n",
        "    Why we filter out multi-aspect segments for training:\n",
        "        When a segment maps to multiple aspects (e.g., [FOOD, AMBIENCE]),\n",
        "        the weak label (derived from the whole review) is even noisier for\n",
        "        that segment. We keep only single-aspect segments for cleaner\n",
        "        supervision signal. Multi-aspect segments can still be predicted\n",
        "        at inference time.\n",
        "\n",
        "    Why we exclude gold standard review IDs:\n",
        "        To prevent data leakage, we filter out any training segments that\n",
        "        come from reviews already in the gold standard dataset. This ensures\n",
        "        the model never sees text from gold reviews during training, making\n",
        "        gold evaluation truly independent.\n",
        "\n",
        "    Args:\n",
        "        cfg: TrainingConfig instance.\n",
        "\n",
        "    Returns:\n",
        "        Dict with keys: 'train', 'val', 'test' â€” each a DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LOADING & SPLITTING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df = pd.read_pickle(cfg.data_path)\n",
        "    print(f\"  Raw segments loaded: {len(df):,}\")\n",
        "\n",
        "    # --- PREVENT DATA LEAKAGE: Exclude gold standard review IDs ----------\n",
        "    print(f\"\\n  âš ï¸  DATA LEAKAGE PREVENTION:\")\n",
        "    print(f\"  Loading gold standard to identify held-out review IDs...\")\n",
        "    \n",
        "    try:\n",
        "        gold_df = pd.read_csv(cfg.gold_data_path)\n",
        "        \n",
        "        # Extract unique Original_Review_IDs from gold dataset\n",
        "        # Gold dataset may have 'Review_ID' or 'Original_Review_ID' column\n",
        "        if 'Original_Review_ID' in gold_df.columns:\n",
        "            gold_review_ids = set(gold_df['Original_Review_ID'].unique())\n",
        "        elif 'Review_ID' in gold_df.columns:\n",
        "            gold_review_ids = set(gold_df['Review_ID'].unique())\n",
        "        else:\n",
        "            print(f\"  âš ï¸  Warning: Could not find review ID column in gold dataset\")\n",
        "            print(f\"     Available columns: {list(gold_df.columns)}\")\n",
        "            print(f\"     Proceeding without filtering (may cause data leakage!)\")\n",
        "            gold_review_ids = set()\n",
        "        \n",
        "        print(f\"  âœ“ Gold dataset loaded: {len(gold_df):,} annotations\")\n",
        "        print(f\"  âœ“ Unique review IDs in gold: {len(gold_review_ids):,}\")\n",
        "        \n",
        "        # Filter out segments from gold review IDs\n",
        "        n_before = len(df)\n",
        "        df = df[~df['Original_Review_ID'].isin(gold_review_ids)].copy()\n",
        "        n_after = len(df)\n",
        "        n_removed = n_before - n_after\n",
        "        \n",
        "        print(f\"  âœ“ Filtered out {n_removed:,} segments from gold reviews ({n_removed/n_before*100:.1f}%)\")\n",
        "        print(f\"  âœ“ Training segments remaining: {n_after:,}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error loading gold dataset: {e}\")\n",
        "        print(f\"     Proceeding without filtering (may cause data leakage!)\")\n",
        "    \n",
        "    print(f\"\\n  Final training data: {len(df):,} segments\")\n",
        "\n",
        "    # --- Filter to single-aspect segments for cleaner weak supervision ----\n",
        "    df[\"num_aspects\"] = df[\"Aspect_Labels\"].apply(len)\n",
        "    df_single = df[df[\"num_aspects\"] == 1].copy()\n",
        "    df_single[\"aspect\"] = df_single[\"Aspect_Labels\"].apply(lambda x: x[0])\n",
        "    \n",
        "    n_multi = len(df) - len(df_single)\n",
        "    pct_retained = (len(df_single) / len(df)) * 100\n",
        "    \n",
        "    print(f\"\\n  âš ï¸  FILTERING STRATEGY (for training only):\")\n",
        "    print(f\"    Single-aspect segments:  {len(df_single):>7,} ({pct_retained:>5.1f}%) â†’ KEPT for training\")\n",
        "    print(f\"    Multi-aspect segments:   {n_multi:>7,} ({100-pct_retained:>5.1f}%) â†’ DROPPED from training\")\n",
        "    print(f\"    Segments remaining:      {len(df_single):>7,}\")\n",
        "    print(f\"\\n  ðŸ“Š For inference/visualization: Use the FULL dataset (all segments)\")\n",
        "    print(f\"     including multi-aspect ones to get complete review coverage.\")\n",
        "\n",
        "    # --- Encode labels ---------------------------------------------------\n",
        "    df_single[\"label\"] = df_single[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "    # Sanity check: no NaN labels\n",
        "    assert df_single[\"label\"].isna().sum() == 0, (\n",
        "        \"Found NaN labels! Check Sentiment_Label column values.\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  Label distribution:\")\n",
        "    print(df_single[\"label\"].value_counts().sort_index().to_string(\n",
        "        index=True).replace(\"0\", \"  0 (negative)\").replace(\"1\", \"  1 (positive)\")\n",
        "    )\n",
        "\n",
        "    # --- Train / Val / Test split (two-stage stratified) -----------------\n",
        "    # Stage 1: Separate test set\n",
        "    df_trainval, df_test = train_test_split(\n",
        "        df_single,\n",
        "        test_size=cfg.test_size,\n",
        "        stratify=df_single[\"label\"],\n",
        "        random_state=cfg.random_seed,\n",
        "    )\n",
        "    # Stage 2: Split remainder into train + val\n",
        "    # Adjust val_size relative to the remaining data\n",
        "    adjusted_val_size = cfg.val_size / (1.0 - cfg.test_size)\n",
        "    df_train, df_val = train_test_split(\n",
        "        df_trainval,\n",
        "        test_size=adjusted_val_size,\n",
        "        stratify=df_trainval[\"label\"],\n",
        "        random_state=cfg.random_seed,\n",
        "    )\n",
        "\n",
        "    splits = {\"train\": df_train, \"val\": df_val, \"test\": df_test}\n",
        "\n",
        "    print(f\"\\n  Split sizes:\")\n",
        "    for name, split_df in splits.items():\n",
        "        pos = (split_df[\"label\"] == 1).sum()\n",
        "        neg = (split_df[\"label\"] == 0).sum()\n",
        "        print(f\"    {name:<6}: {len(split_df):>7,} rows \"\n",
        "              f\"| pos: {pos:,} ({pos/len(split_df)*100:.1f}%) \"\n",
        "              f\"| neg: {neg:,} ({neg/len(split_df)*100:.1f}%)\")\n",
        "\n",
        "    # --- Compute Class Weights Dynamically -------------------------------\n",
        "    # Check if class weights are manually set in config\n",
        "    if cfg.class_weights is None:\n",
        "        # Using inverse frequency weighting: n_samples / (n_classes * np.bincount(y))\n",
        "        print(f\"\\n  Computing class weights from training data...\")\n",
        "        class_weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.unique(df_train[\"label\"]),\n",
        "            y=df_train[\"label\"]\n",
        "        )\n",
        "        cfg.class_weights = class_weights.tolist()\n",
        "        print(f\"  Class weights (auto-computed balanced):\")\n",
        "    else:\n",
        "        print(f\"\\n  Using manually-set class weights from config:\")\n",
        "    \n",
        "    for class_id, weight in enumerate(cfg.class_weights):\n",
        "        print(f\"    {ID2LABEL[class_id]:<10} (class {class_id}): {weight:.4f}\")\n",
        "    \n",
        "    print(f\"\\n  Interpretation:\")\n",
        "    print(f\"    Higher weight = minority class â†’ model penalized more for errors\")\n",
        "    if cfg.class_weights == [6.0, 1.0]:\n",
        "        print(f\"    âš ï¸  AGGRESSIVE WEIGHTING: Negative class errors cost 6x more!\")\n",
        "    print(f\"    Formula (when auto): n_samples / (n_classes Ã— count_per_class)\")\n",
        "\n",
        "\n",
        "\n",
        "    return splits\n",
        "\n",
        "DATA = load_and_prepare_data(CFG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f802384a",
      "metadata": {
        "id": "f802384a"
      },
      "source": [
        "# STAGE 3: PyTorch Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68796217",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68796217",
        "outputId": "7525bcfe-b7d3-43cf-f0ea-41e30743a0d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING DATALOADERS\n",
            "======================================================================\n",
            "  âœ“ Tokenizer loaded: xlm-roberta-base\n",
            "    Vocab size: 250,002\n",
            "\n",
            "  Dataset sizes & batches:\n",
            "    train :  75,417 samples â†’ 2,357 batches (batch_size=32)\n",
            "    val   :  10,056 samples â†’  315 batches (batch_size=32)\n",
            "    test  :  15,084 samples â†’  472 batches (batch_size=32)\n",
            "\n",
            "  Sample input (decoded):\n",
            "    \"<s> FOOD</s></s> cooked to perfection</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\"\n",
            "    Label: positive\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Wraps a DataFrame split into a tokenized, aspect-conditioned dataset.\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    \"\"\"Aspect-conditioned sentiment dataset for XLM-RoBERTa.\n",
        "\n",
        "    Input Format (Sun et al., 2019 â€” Aspect-Based Sentiment):\n",
        "        Input:  \"[aspect] </s></s> [segment text]\"\n",
        "        Label:  0 (negative) or 1 (positive)\n",
        "\n",
        "    Why \"</s></s>\" (double SEP):\n",
        "        XLM-RoBERTa uses </s> as its separator token (unlike BERT's [SEP]).\n",
        "        The double </s></s> pattern is the standard way RoBERTa-family models\n",
        "        denote a sentence boundary â€” this is baked into its pre-training.\n",
        "\n",
        "    Example:\n",
        "        Input text:  \"FOOD </s></s> the nasi lemak was incredibly sedap\"\n",
        "        Tokenized:   <s> FOOD </s> </s> the nasi lemak was incredibly sedap </s>\n",
        "        Label:       1 (positive)\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['aspect', 'Segment', 'label'].\n",
        "        tokenizer: HuggingFace tokenizer for xlm-roberta.\n",
        "        max_length: Maximum token length (default 128).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        max_length: int = 128,\n",
        "    ):\n",
        "        self.texts = df[\"Segment\"].tolist()\n",
        "        self.aspects = df[\"aspect\"].tolist()\n",
        "        self.labels = df[\"label\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        \"\"\"Tokenize a single (aspect, segment) pair on-the-fly.\n",
        "\n",
        "        Why on-the-fly tokenization (not pre-tokenized):\n",
        "            For datasets of this size (~100K), pre-tokenizing and caching\n",
        "            in memory is faster but uses ~2-3 GB RAM. On-the-fly keeps\n",
        "            memory footprint low and simplifies the code. If training\n",
        "            speed becomes the bottleneck, switch to a pre-tokenized cache.\n",
        "\n",
        "        Returns:\n",
        "            Dict with 'input_ids', 'attention_mask', 'labels' tensors.\n",
        "        \"\"\"\n",
        "        aspect = self.aspects[idx]\n",
        "        segment = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # --- Construct aspect-conditioned input --------------------------\n",
        "        # Format: \"ASPECT_LABEL </s></s> segment_text\"\n",
        "        # The aspect is uppercased to visually distinguish it as a\n",
        "        # \"prompt token\" â€” the model learns to treat it as a conditioning\n",
        "        # signal rather than natural language.\n",
        "        conditioned_text = f\"{aspect.upper()} </s></s> {segment}\"\n",
        "\n",
        "        # --- Tokenize ----------------------------------------------------\n",
        "        encoding = self.tokenizer(\n",
        "            conditioned_text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,           \n",
        "            padding=False,\n",
        "            return_tensors=None,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"],\n",
        "            \"attention_mask\": encoding[\"attention_mask\"],\n",
        "            \"labels\": label,\n",
        "        }\n",
        "\n",
        "\n",
        "def build_dataloaders(data: dict, cfg: TrainingConfig) -> dict:\n",
        "    \"\"\"Instantiate tokenizer, datasets, and DataLoaders.\n",
        "\n",
        "    Args:\n",
        "        data: Dict with 'train', 'val', 'test' DataFrames.\n",
        "        cfg: TrainingConfig.\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'train', 'val', 'test' DataLoaders and 'tokenizer'.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BUILDING DATALOADERS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Load tokenizer (downloads ~1 MB vocab file on first run)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "    print(f\"  âœ“ Tokenizer loaded: {cfg.model_name}\")\n",
        "    print(f\"    Vocab size: {tokenizer.vocab_size:,}\")\n",
        "\n",
        "    # OPTIMIZATION: Smart Collator handles dynamic padding\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    loaders = {}\n",
        "    datasets_info = {}\n",
        "\n",
        "    for split_name, df in data.items():\n",
        "        dataset = ABSADataset(df, tokenizer, cfg.max_seq_length)\n",
        "\n",
        "        # Training set uses shuffle; val/test do not\n",
        "        is_train = split_name == \"train\"\n",
        "\n",
        "        # OPTIMIZATION: Increase num_workers on Linux/Colab\n",
        "        # If on Windows, keep at 0. If on Colab, use 2.\n",
        "        import os\n",
        "        workers = 2 if os.name == 'posix' else 0\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=cfg.batch_size,\n",
        "            shuffle=is_train,\n",
        "            collate_fn=data_collator,\n",
        "            num_workers=workers,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        loaders[split_name] = loader\n",
        "        datasets_info[split_name] = len(dataset)\n",
        "\n",
        "    loaders[\"tokenizer\"] = tokenizer\n",
        "\n",
        "    print(f\"\\n  Dataset sizes & batches:\")\n",
        "    for name, size in datasets_info.items():\n",
        "        n_batches = size // cfg.batch_size + (1 if size % cfg.batch_size else 0)\n",
        "        print(f\"    {name:<6}: {size:>7,} samples â†’ {n_batches:>4,} batches \"\n",
        "              f\"(batch_size={cfg.batch_size})\")\n",
        "\n",
        "    # --- Quick sanity check: decode one sample ---------------------------\n",
        "    sample_batch = next(iter(loaders[\"train\"]))\n",
        "    sample_text = tokenizer.decode(\n",
        "        sample_batch[\"input_ids\"][0], skip_special_tokens=False\n",
        "    )\n",
        "    print(f\"\\n  Sample input (decoded):\")\n",
        "    print(f\"    \\\"{sample_text}\\\"\")\n",
        "    print(f\"    Label: {ID2LABEL[sample_batch['labels'][0].item()]}\")\n",
        "\n",
        "    return loaders\n",
        "\n",
        "\n",
        "LOADERS = build_dataloaders(DATA, CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a770bc9e",
      "metadata": {
        "id": "a770bc9e"
      },
      "source": [
        "# STAGE 4: Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "971bcec3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "971bcec3",
        "outputId": "609136f0-fa21-471c-de3f-d4cb11fa90ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "BUILDING MODEL\n",
            "======================================================================\n",
            "  âœ“ Model loaded on cuda\n",
            "    Total params:      278,045,186\n",
            "    Trainable params:  278,045,186\n",
            "    Class weights:    [4.5448, 0.5618]\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# XLM-RoBERTa + a 2-class classification head + class-weighted loss.\n",
        "# ==============================================================================\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for handling extreme class imbalance (Lin et al., 2017).\n",
        "    \n",
        "    Academic Justification:\n",
        "        Standard cross-entropy gives equal weight to all samples. With 89% positive\n",
        "        reviews, most gradients come from easy-to-classify positives, causing the\n",
        "        model to ignore hard negatives. Focal loss (Lin et al., 2017 - RetinaNet)\n",
        "        down-weights easy examples and focuses learning on hard cases.\n",
        "    \n",
        "    Formula:\n",
        "        FL(p_t) = -Î±(1-p_t)^Î³ * log(p_t)\n",
        "        where p_t = model confidence on true class\n",
        "    \n",
        "    Parameters:\n",
        "        - Î± (alpha): Weighting factor for class imbalance (0.25 = focus on minority)\n",
        "        - Î³ (gamma): Focusing parameter (2.0 = strongly down-weight easy samples)\n",
        "        - weight: Per-class weights (combines with focal mechanism)\n",
        "    \n",
        "    Why Î³=2.0:\n",
        "        - Easy sample (p_t=0.99): Weight = (1-0.99)^2 = 0.0001 (nearly ignored)\n",
        "        - Hard sample (p_t=0.51): Weight = (1-0.51)^2 = 0.24 (full attention)\n",
        "        This forces the model to learn from challenging negatives.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, weight=None):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        # Register weight as buffer so it moves to GPU with the model\n",
        "        # Use 'class_weight' to avoid conflict with nn.Module's 'weight' attribute\n",
        "        if weight is not None:\n",
        "            self.register_buffer('class_weight', weight)\n",
        "        else:\n",
        "            self.class_weight = None\n",
        "        \n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"Compute focal loss.\n",
        "        \n",
        "        Args:\n",
        "            inputs: Logits from model (batch_size, num_classes)\n",
        "            targets: Ground truth labels (batch_size,)\n",
        "            \n",
        "        Returns:\n",
        "            Scalar loss value\n",
        "        \"\"\"\n",
        "        # Standard cross-entropy loss (unreduced)\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.class_weight)\n",
        "        \n",
        "        # Get model confidence on true class: p_t = exp(-CE)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        \n",
        "        # Apply focal term: (1 - p_t)^gamma\n",
        "        # Easy samples (high p_t) get low weight, hard samples get high weight\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        \n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3255ac93",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ABSASentimentClassifier(nn.Module):\n",
        "    \"\"\"XLM-RoBERTa with a classification head for binary sentiment.\n",
        "\n",
        "    Why AutoModelForSequenceClassification instead of raw AutoModel:\n",
        "        The \"ForSequenceClassification\" variant already includes:\n",
        "          - The [CLS] token pooling (first token representation)\n",
        "          - A dropout layer\n",
        "          - A linear projection to num_labels\n",
        "        Building these manually adds no value and risks subtle bugs\n",
        "        (e.g., forgetting dropout â†’ overfitting).\n",
        "\n",
        "    Why we store class_weights on the model:\n",
        "        This ensures the weights move to the correct device (CPU/GPU)\n",
        "        alongside the model when .to(device) is called. Forgetting this\n",
        "        is one of the most common PyTorch bugs.\n",
        "\n",
        "    Args:\n",
        "        cfg: TrainingConfig.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: TrainingConfig):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModelForSequenceClassification.from_pretrained(\n",
        "            cfg.model_name,\n",
        "            num_labels=cfg.num_labels,\n",
        "        )\n",
        "        # Register class weights as a buffer (not a parameter â€”\n",
        "        # it won't be updated by the optimizer, but WILL move with .to())\n",
        "        self.register_buffer(\n",
        "            \"class_weights\",\n",
        "            torch.tensor(cfg.class_weights, dtype=torch.float),\n",
        "        )\n",
        "        # Use Focal Loss instead of standard CrossEntropyLoss\n",
        "        # alpha=0.25 focuses on minority class, gamma=2.0 down-weights easy examples\n",
        "        self.loss_fn = FocalLoss(alpha=0.25, gamma=2.0, weight=self.class_weights)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        labels: torch.Tensor = None,\n",
        "    ) -> dict:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs (batch_size, seq_len).\n",
        "            attention_mask: 1 for real tokens, 0 for padding (batch_size, seq_len).\n",
        "            labels: Ground truth labels (batch_size,). Optional â€” if None,\n",
        "                    only logits are returned (useful for inference).\n",
        "\n",
        "        Returns:\n",
        "            Dict with 'loss' (if labels provided) and 'logits'.\n",
        "        \"\"\"\n",
        "        outputs = self.backbone(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        logits = outputs.logits  # (batch_size, num_labels)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "\n",
        "def build_model(cfg: TrainingConfig) -> ABSASentimentClassifier:\n",
        "    \"\"\"Instantiate model and move to device.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BUILDING MODEL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model = ABSASentimentClassifier(cfg)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  âœ“ Model loaded on {DEVICE}\")\n",
        "    print(f\"    Total params:     {total_params:>12,}\")\n",
        "    print(f\"    Trainable params: {trainable_params:>12,}\")\n",
        "    print(f\"    Class weights:    {cfg.class_weights}\")\n",
        "    print(f\"    Loss function:    Focal Loss (alpha=0.25, gamma=2.0)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "MODEL = build_model(CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d3468a",
      "metadata": {
        "id": "75d3468a"
      },
      "source": [
        "# STAGE 5: Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60febb3f",
      "metadata": {
        "id": "60febb3f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.auto import tqdm  \n",
        "\n",
        "def compute_metrics(predictions: list, labels: list) -> dict:\n",
        "    \"\"\"Compute accuracy, macro-F1, and per-class F1.\n",
        "\n",
        "    Why macro-F1 over accuracy:\n",
        "        With 89/11 imbalance, a model predicting \"positive\" always gets\n",
        "        89% accuracy. Macro-F1 weights both classes equally, so it\n",
        "        actually measures whether the model learned the minority class.\n",
        "\n",
        "    Args:\n",
        "        predictions: List of predicted class IDs.\n",
        "        labels: List of ground-truth class IDs.\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'accuracy', 'macro_f1', 'neg_f1', 'pos_f1'.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
        "    # Explicitly specify labels=[0, 1] to ensure we always get both classes\n",
        "    # even if model predicts only one class (prevents IndexError)\n",
        "    per_class_f1 = f1_score(labels, predictions, average=None, labels=[0, 1])\n",
        "    return {\n",
        "        \"accuracy\": round(acc, 4),\n",
        "        \"macro_f1\": round(macro_f1, 4),\n",
        "        \"neg_f1\": round(per_class_f1[0], 4),\n",
        "        \"pos_f1\": round(per_class_f1[1], 4),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device) -> dict:\n",
        "    \"\"\"Run one full training epoch.\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'loss' (average over all batches).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # WRAP DATALOADER WITH TQDM FOR PROGRESS BAR\n",
        "    # This creates the visual bar: [=====>      ] 45%\n",
        "    progress_bar = tqdm(dataloader, desc=\"  Training\", leave=False)\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        loss = outputs[\"loss\"]\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Gradient clipping: prevents exploding gradients in transformers\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # UPDATE PROGRESS BAR\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return {\"loss\": round(total_loss / num_batches, 6)}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device) -> dict:\n",
        "    \"\"\"Run evaluation on val or test set (no gradient computation).\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'loss', 'accuracy', 'macro_f1', 'neg_f1', 'pos_f1'.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        total_loss += outputs[\"loss\"].item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Argmax â†’ predicted class\n",
        "        preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "        all_preds.extend(preds.cpu().numpy().tolist())\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    metrics = compute_metrics(all_preds, all_labels)\n",
        "    metrics[\"loss\"] = round(total_loss / num_batches, 6)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train(model, loaders, cfg, device) -> dict:\n",
        "    \"\"\"Full training loop with early stopping on val macro-F1.\n",
        "\n",
        "    Why early stopping on macro-F1 (not loss):\n",
        "        Validation loss can continue decreasing even as the model starts\n",
        "        overfitting to the majority class. Macro-F1 directly measures\n",
        "        what we care about: balanced performance on both classes.\n",
        "\n",
        "    Args:\n",
        "        model: ABSASentimentClassifier (already on device).\n",
        "        loaders: Dict with 'train', 'val', 'test' DataLoaders.\n",
        "        cfg: TrainingConfig.\n",
        "        device: torch.device.\n",
        "\n",
        "    Returns:\n",
        "        Dict with full training history (for plotting / logging).\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # --- Optimizer: AdamW (standard for transformer fine-tuning) --------\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=cfg.learning_rate,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "    )\n",
        "\n",
        "    # OPTIMIZATION: Initialize Scaler for FP16\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # --- Learning rate scheduler: linear warm-up then linear decay -------\n",
        "    # Why: Transformers are sensitive to LR. A warm-up phase prevents\n",
        "    # catastrophic early updates to pre-trained weights.\n",
        "    total_steps = len(loaders[\"train\"]) * cfg.num_epochs\n",
        "    warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "    print(f\"  Total training steps:  {total_steps:,}\")\n",
        "    print(f\"  Warmup steps:          {warmup_steps:,}\")\n",
        "\n",
        "    # --- Output directory -------------------------------------------------\n",
        "    os.makedirs(os.path.dirname(cfg.best_model_path) or \".\", exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(cfg.metrics_path) or \".\", exist_ok=True)\n",
        "\n",
        "    # --- Training history -------------------------------------------------\n",
        "    history = {\"train\": [], \"val\": [], \"test\": None}\n",
        "    best_val_f1 = -1.0\n",
        "    patience = 5   # Increased from 2 to 5 - minority class needs more time\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, cfg.num_epochs + 1):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # --- Train ---\n",
        "        train_metrics = train_epoch(model, loaders[\"train\"], optimizer,\n",
        "                                    scheduler, device)\n",
        "        history[\"train\"].append(train_metrics)\n",
        "\n",
        "        # --- Validate ---\n",
        "        val_metrics = evaluate(model, loaders[\"val\"], device)\n",
        "        history[\"val\"].append(val_metrics)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        # --- Log -------------------------------------------------------------\n",
        "        print(f\"\\n  Epoch {epoch}/{cfg.num_epochs}  ({epoch_time:.1f}s)\")\n",
        "        print(f\"    Train Loss:      {train_metrics['loss']:.6f}\")\n",
        "        print(f\"    Val  Loss:       {val_metrics['loss']:.6f}\")\n",
        "        print(f\"    Val  Accuracy:   {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"    Val  Macro-F1:   {val_metrics['macro_f1']:.4f}  \"\n",
        "              f\"(neg: {val_metrics['neg_f1']:.4f} | \"\n",
        "              f\"pos: {val_metrics['pos_f1']:.4f})\")\n",
        "\n",
        "        # --- Early stopping & best-model checkpoint -------------------------\n",
        "        if val_metrics[\"macro_f1\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"macro_f1\"]\n",
        "            patience_counter = 0\n",
        "            # Save only the model state_dict (not the whole object)\n",
        "            torch.save(model.state_dict(), cfg.best_model_path)\n",
        "            print(f\"    â˜… New best model saved  (macro-F1: {best_val_f1:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"    âœ— No improvement. Patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\n  âš¡ Early stopping at epoch {epoch}.\")\n",
        "                break\n",
        "\n",
        "    # --- Final evaluation on WEAK TEST set (only once, after training) ------\n",
        "    # Load the best checkpoint before evaluating\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"  LOADING BEST MODEL FOR WEAK TEST SET EVALUATION\")\n",
        "    print(\"-\" * 70)\n",
        "    model.load_state_dict(torch.load(cfg.best_model_path, map_location=device))\n",
        "\n",
        "    test_metrics = evaluate(model, loaders[\"test\"], device)\n",
        "    history[\"test\"] = test_metrics\n",
        "\n",
        "    print(f\"\\n  â˜… WEAK TEST SET RESULTS (15% split from training data):\")\n",
        "    print(f\"    Test Loss:       {test_metrics['loss']:.6f}\")\n",
        "    print(f\"    Test Accuracy:   {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"    Test Macro-F1:   {test_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"      Negative F1:   {test_metrics['neg_f1']:.4f}\")\n",
        "    print(f\"      Positive  F1:  {test_metrics['pos_f1']:.4f}\")\n",
        "\n",
        "    # ------------ Full classification report --------------\n",
        "    # Re-run test set to collect all predictions for the report\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loaders[\"test\"]:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    print(f\"\\n  Classification Report:\")\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=[\"Negative\", \"Positive\"],\n",
        "    ))\n",
        "\n",
        "    # --- Save training history as JSON ------------------------------------\n",
        "    with open(cfg.metrics_path, \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "    print(f\"  âœ“ Training metrics saved to: {cfg.metrics_path}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "835fdf30",
      "metadata": {
        "id": "835fdf30"
      },
      "source": [
        "# STAGE 6: Execute Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7d100a",
      "metadata": {},
      "outputs": [],
      "source": [
        "HISTORY = train(MODEL, LOADERS, CFG, DEVICE)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Best model:   {CFG.best_model_path}\")\n",
        "print(f\"  Metrics file: {CFG.metrics_path}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a54a7a4",
      "metadata": {},
      "source": [
        "# STAGE 7: Model Evaluation on Gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff6ca80",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_on_gold(model, cfg: TrainingConfig, tokenizer, device) -> dict:\n",
        "    \"\"\"Evaluate trained model on the Gold Standard (manually-annotated) dataset.\n",
        "\n",
        "    Why separate gold evaluation:\n",
        "        - Training uses WEAK labels (star ratings) with inherent noise\n",
        "        - Gold standard has HUMAN-ANNOTATED labels (ground truth)\n",
        "        - We evaluate on gold separately to measure TRUE model performance\n",
        "\n",
        "    Academic Justification:\n",
        "        Following the evaluation protocol of Sun et al. (2019) and Pontiki et al.\n",
        "        (2016), we report performance on a gold standard test set annotated by\n",
        "        domain experts. This accounts for label noise in the weak supervision\n",
        "        training set and provides trustworthy F1 scores for the final thesis.\n",
        "\n",
        "    Args:\n",
        "        model: Trained ABSASentimentClassifier (already on best checkpoint)\n",
        "        cfg: TrainingConfig with gold_data_path\n",
        "        tokenizer: XLM-RoBERTa tokenizer\n",
        "        device: torch.device (GPU or CPU)\n",
        "\n",
        "    Returns:\n",
        "        Dict with overall metrics + per-aspect breakdown for thesis reporting\n",
        "    \"\"\"\n",
        "    print(f\"\\n  Loading gold standard from: {cfg.gold_data_path}\")\n",
        "\n",
        "    # --- Load and preprocess gold data ---\n",
        "    try:\n",
        "        gold_df = pd.read_csv(cfg.gold_data_path)\n",
        "        print(f\"  âœ“ Gold dataset loaded: {len(gold_df):,} rows (before exploding multi-aspect)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error loading gold data: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    # Rename columns to match training format\n",
        "    # Input columns: Segment, Manual_Aspect, Manual_Sentiment\n",
        "    gold_df_prep = gold_df.copy()\n",
        "    gold_df_prep.rename(columns={\n",
        "        \"Manual_Aspect\": \"aspect\",\n",
        "        \"Manual_Sentiment\": \"Sentiment_Label\",\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Normalize sentiment labels to lowercase (handle 'POSITIVE'/'NEGATIVE' vs 'positive'/'negative')\n",
        "    gold_df_prep[\"Sentiment_Label\"] = gold_df_prep[\"Sentiment_Label\"].str.lower()\n",
        "\n",
        "    # --- Handle multi-aspect segments: explode into separate rows -------\n",
        "    # Aspect is stored as string representation of list (e.g., \"['FOOD', 'VALUE']\"),\n",
        "    # convert to actual list\n",
        "    import ast\n",
        "    def parse_aspect(val):\n",
        "        \"\"\"Parse aspect column - handle both strings and lists.\"\"\"\n",
        "        if isinstance(val, str):\n",
        "            try:\n",
        "                # Try to parse as Python literal (handles \"['FOOD', 'VALUE']\")\n",
        "                parsed = ast.literal_eval(val)\n",
        "                if isinstance(parsed, list):\n",
        "                    return parsed\n",
        "                else:\n",
        "                    return [parsed]  # Single aspect as string\n",
        "            except (ValueError, SyntaxError):\n",
        "                # Already a plain string like \"FOOD\"\n",
        "                return [val]\n",
        "        elif isinstance(val, list):\n",
        "            return val\n",
        "        else:\n",
        "            return [str(val)]\n",
        "\n",
        "    gold_df_prep[\"aspect\"] = gold_df_prep[\"aspect\"].apply(parse_aspect)\n",
        "    \n",
        "    # Count single vs multi-aspect rows BEFORE exploding\n",
        "    n_single_aspect = sum(len(aspects) == 1 for aspects in gold_df_prep[\"aspect\"])\n",
        "    n_multi_aspect = len(gold_df_prep) - n_single_aspect\n",
        "    \n",
        "    # Explode: one row per aspect (same segment can appear multiple times)\n",
        "    gold_df_exploded = gold_df_prep.explode(\"aspect\").reset_index(drop=True)\n",
        "    \n",
        "    print(f\"    Original rows:        {len(gold_df):,}\")\n",
        "    print(f\"      Single-aspect:      {n_single_aspect:,}\")\n",
        "    print(f\"      Multi-aspect:       {n_multi_aspect:,}\")\n",
        "    print(f\"    After exploding:      {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "\n",
        "    # Encode sentiment labels to numeric format\n",
        "    gold_df_exploded[\"label\"] = gold_df_exploded[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "    # Sanity check: warn if any labels couldn't be mapped\n",
        "    n_unmapped = gold_df_exploded[\"label\"].isna().sum()\n",
        "    if n_unmapped > 0:\n",
        "        print(f\"  âš ï¸  Warning: {n_unmapped} rows with unmapped sentiment labels\")\n",
        "        print(f\"     Available values: {gold_df_exploded['Sentiment_Label'].unique()}\")\n",
        "        print(f\"     Expected: {list(LABEL2ID.keys())}\")\n",
        "        # Drop unmapped rows\n",
        "        gold_df_exploded = gold_df_exploded.dropna(subset=[\"label\"])\n",
        "        print(f\"     Proceeding with {len(gold_df_exploded):,} valid samples\")\n",
        "\n",
        "    print(f\"\\n  Gold dataset label distribution:\")\n",
        "    for label_id in sorted(gold_df_exploded[\"label\"].unique()):\n",
        "        count = (gold_df_exploded[\"label\"] == label_id).sum()\n",
        "        pct = count / len(gold_df_exploded) * 100\n",
        "        print(f\"    {ID2LABEL[label_id]:<10}: {count:>5,} samples ({pct:>5.1f}%)\")\n",
        "    \n",
        "    # Check aspect distribution in gold data\n",
        "    print(f\"\\n  Gold dataset aspect distribution:\")\n",
        "    aspect_counts = gold_df_exploded[\"aspect\"].value_counts()\n",
        "    for aspect, count in aspect_counts.items():\n",
        "        print(f\"    {aspect:<16}: {count:>4} samples\")\n",
        "    \n",
        "    # Use the exploded dataframe for evaluation\n",
        "    gold_df_prep = gold_df_exploded\n",
        "\n",
        "    # --- Create dataset and dataloader ---\n",
        "    gold_dataset = ABSADataset(gold_df_prep, tokenizer, cfg.max_seq_length)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    gold_loader = torch.utils.data.DataLoader(\n",
        "        gold_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    # --- Inference on gold set (no gradients) ---\n",
        "    print(f\"\\n  Running inference on gold set...\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_aspects = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(gold_loader):\n",
        "            if (batch_idx + 1) % max(1, len(gold_loader) // 5) == 0:\n",
        "                print(f\"    Progress: {batch_idx + 1}/{len(gold_loader)} batches\")\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            # Get predicted classes (argmax over logits)\n",
        "            preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    # Get corresponding aspects for per-aspect breakdown\n",
        "    all_aspects = gold_df_prep[\"aspect\"].tolist()\n",
        "\n",
        "    # --- DIAGNOSTIC: Check prediction distribution ---\n",
        "    from collections import Counter\n",
        "    pred_counts = Counter(all_preds)\n",
        "    label_counts = Counter(all_labels)\n",
        "    \n",
        "    print(f\"\\n  âš ï¸  PREDICTION DIAGNOSTIC:\")\n",
        "    print(f\"    Ground truth distribution:\")\n",
        "    for label_id in sorted(label_counts.keys()):\n",
        "        count = label_counts[label_id]\n",
        "        pct = count / len(all_labels) * 100\n",
        "        print(f\"      {ID2LABEL[label_id]:<10}: {count:>4} ({pct:>5.1f}%)\")\n",
        "    \n",
        "    print(f\"\\n    Model prediction distribution:\")\n",
        "    for label_id in sorted(pred_counts.keys()):\n",
        "        count = pred_counts[label_id]\n",
        "        pct = count / len(all_preds) * 100\n",
        "        print(f\"      {ID2LABEL[label_id]:<10}: {count:>4} ({pct:>5.1f}%)\")\n",
        "    \n",
        "    if len(pred_counts) == 1:\n",
        "        only_class = list(pred_counts.keys())[0]\n",
        "        print(f\"\\n    âŒ PROBLEM: Model predicts ONLY {ID2LABEL[only_class].upper()}!\")\n",
        "        print(f\"       This indicates severe overfitting to the majority class.\")\n",
        "        print(f\"       Possible causes:\")\n",
        "        print(f\"         1. Domain shift: Gold data differs from training data\")\n",
        "        print(f\"         2. Aspect mismatch: Check if aspects in gold match training\")\n",
        "        print(f\"         3. Decision threshold: Model confidence too skewed\")\n",
        "\n",
        "    # --- Compute overall metrics ---\n",
        "    overall_metrics = compute_metrics(all_preds, all_labels)\n",
        "\n",
        "    # --- Per-aspect breakdown ---\n",
        "    aspects_unique = sorted(set(all_aspects))\n",
        "    per_aspect_metrics = {}\n",
        "\n",
        "    for aspect in aspects_unique:\n",
        "        # Filter to samples of this aspect\n",
        "        mask = [i for i, a in enumerate(all_aspects) if a == aspect]\n",
        "        if not mask:\n",
        "            continue\n",
        "\n",
        "        aspect_preds = [all_preds[i] for i in mask]\n",
        "        aspect_labels = [all_labels[i] for i in mask]\n",
        "        \n",
        "        # Try to compute metrics; skip if aspect has too few samples or predictions\n",
        "        try:\n",
        "            per_aspect_metrics[aspect] = compute_metrics(aspect_preds, aspect_labels)\n",
        "        except (IndexError, ValueError) as e:\n",
        "            # Aspect has predictions of only one class - compute what we can\n",
        "            acc = accuracy_score(aspect_labels, aspect_preds)\n",
        "            per_aspect_metrics[aspect] = {\n",
        "                \"accuracy\": round(acc, 4),\n",
        "                \"macro_f1\": 0.0,  # Can't compute macro-F1 with single class\n",
        "                \"neg_f1\": 0.0,\n",
        "                \"pos_f1\": 0.0,\n",
        "                \"note\": f\"Single-class predictions ({len(mask)} samples)\"\n",
        "            }\n",
        "\n",
        "    # --- Format results for saving ---\n",
        "    gold_results = {\n",
        "        \"overall\": overall_metrics,\n",
        "        \"per_aspect\": per_aspect_metrics,\n",
        "        \"n_samples\": len(gold_df_prep),\n",
        "        \"aspects\": aspects_unique,\n",
        "    }\n",
        "\n",
        "    # --- Print results for immediate feedback ---\n",
        "    print(f\"\\n  {'='*70}\")\n",
        "    print(f\"  â˜… GOLD TEST SET RESULTS (Human-Annotated Ground Truth)\")\n",
        "    print(f\"  â˜… Total samples: {len(gold_df_prep):,} aspect-segment pairs\")\n",
        "    print(f\"  {'='*70}\")\n",
        "    print(f\"\\n  OVERALL PERFORMANCE:\")\n",
        "    print(f\"    Accuracy:  {overall_metrics['accuracy']:.4f}\")\n",
        "    print(f\"    Macro-F1:  {overall_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"      Negative F1 (Recall on negative class):  {overall_metrics['neg_f1']:.4f}\")\n",
        "    print(f\"      Positive F1 (Recall on positive class):  {overall_metrics['pos_f1']:.4f}\")\n",
        "\n",
        "    print(f\"\\n  PER-ASPECT BREAKDOWN (for Kano Model analysis):\")\n",
        "    print(f\"  {'Aspect':<16} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
        "    print(f\"  {'-'*16} {'-'*8} {'-'*10} {'-'*10}\")\n",
        "    for aspect in aspects_unique:\n",
        "        metrics = per_aspect_metrics[aspect]\n",
        "        n_samples = sum(1 for a in all_aspects if a == aspect)\n",
        "        print(f\"  {aspect:<16} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
        "\n",
        "    print(f\"\\n  FULL CLASSIFICATION REPORT (for thesis):\")\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=[\"Negative\", \"Positive\"],\n",
        "        digits=4,\n",
        "    ))\n",
        "\n",
        "    return gold_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a840c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# GOLD STANDARD EVALUATION \n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EVALUATING ON GOLD STANDARD (GROUND TRUTH LABELS)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load best checkpoint for gold evaluation\n",
        "MODEL.load_state_dict(torch.load(CFG.best_model_path, map_location=DEVICE))\n",
        "\n",
        "# Run evaluation on manually-annotated gold dataset\n",
        "gold_metrics = evaluate_on_gold(MODEL, CFG, LOADERS[\"tokenizer\"], DEVICE)\n",
        "\n",
        "# Save gold evaluation results to JSON\n",
        "import json\n",
        "os.makedirs(os.path.dirname(CFG.gold_results_path) or \".\", exist_ok=True)\n",
        "with open(CFG.gold_results_path, \"w\") as f:\n",
        "    json.dump(gold_metrics, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ“ Gold evaluation saved to: {CFG.gold_results_path}\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ALL EVALUATIONS COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nðŸ“Š RESULTS SUMMARY:\")\n",
        "print(f\"  Weak Test Set:   {CFG.metrics_path}\")\n",
        "print(f\"  Gold Test Set:   {CFG.gold_results_path}\")\n",
        "print(f\"  Best Model:      {CFG.best_model_path}\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
