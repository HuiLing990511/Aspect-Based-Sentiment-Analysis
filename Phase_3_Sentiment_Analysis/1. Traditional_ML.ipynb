{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38297865",
      "metadata": {
        "id": "38297865"
      },
      "source": [
        "# Traditional ML Baseline for ABSA Sentiment Classification\n",
        "\n",
        "**Objective:** Train traditional ML classifier (LR, SVM, NB, XG Boost, Random Forest) with TF-IDF features as baseline to compare with XLM-RoBERTa\n",
        "\n",
        "**Academic Justification:**\n",
        "- Establishes baseline performance using classical ML (LR, SVM, NB, XG Boost, Random Forest)\n",
        "- TF-IDF captures term importance without contextual embeddings\n",
        "- Comparison validates whether transformer pre-training provides value for Manglish code-switching\n",
        "- Following best practices: same train/test split, same evaluation protocol, same class imbalance handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e89709",
      "metadata": {
        "id": "b0e89709"
      },
      "source": [
        "# Stage 0: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e31676a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e31676a4",
        "outputId": "0bac2c40-e2d5-4f47-a379-697c52c4a348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to google drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive (To save the model checkpoints)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9e635b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9e635b0",
        "outputId": "4a979f90-5743-4b6e-b339-a2c9165e3395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All libraries loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP Processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# ML & Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"‚úì All libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "caaa0290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caaa0290",
        "outputId": "1630e411-3a48-41a9-d824-af47e1012ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì NLTK resources downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"‚úì NLTK resources downloaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b61515",
      "metadata": {
        "id": "84b61515"
      },
      "source": [
        "# Stage 1: Configuration & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8600f635",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8600f635",
        "outputId": "f8e5265e-25f4-4f2b-eeae-04cdb1e01899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Configuration loaded\n",
            "  Data path: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/aspect_categorization_refined.pkl\n",
            "  Gold path: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/Final_Gold_Standard.csv\n",
            "  Random seed: 42\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATA_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/aspect_categorization_refined.pkl'\n",
        "GOLD_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Dataset/Final_Gold_Standard.csv'\n",
        "#DATA_PATH = r'C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\aspect_categorization_refined.pkl'\n",
        "#GOLD_PATH = r'C:\\Users\\Ong Hui Ling\\Dropbox\\PC\\Documents\\Github\\Aspect-Based-Sentiment-Analysis\\Dataset\\Final_Gold_Standard.csv'\n",
        "\n",
        "# Output path\n",
        "OUTPUT_PATH = r'/content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.15\n",
        "VAL_SIZE = 0.10\n",
        "\n",
        "# Label encoding\n",
        "LABEL2ID = {\"negative\": 0, \"positive\": 1}\n",
        "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "print(f\"‚úì Configuration loaded\")\n",
        "print(f\"  Data path: {DATA_PATH}\")\n",
        "print(f\"  Gold path: {GOLD_PATH}\")\n",
        "print(f\"  Random seed: {RANDOM_SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "77a2e808",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77a2e808",
        "outputId": "e44141a6-b0b4-4018-c105-c5612505bb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LOADING & PREPARING DATA\n",
            "======================================================================\n",
            "  Raw segments loaded: 129,034\n",
            "\n",
            "  ‚ö†Ô∏è  DATA LEAKAGE PREVENTION:\n",
            "  Loading gold standard to identify held-out review IDs...\n",
            "  ‚úì Gold dataset loaded: 799 annotations\n",
            "  ‚úì Unique review IDs in gold: 158\n",
            "  ‚úì Filtered out 1,249 segments from gold reviews (1.0%)\n",
            "  ‚úì Training segments remaining: 127,785\n",
            "\n",
            "  FILTERING STRATEGY:\n",
            "    Single-aspect segments:   98,946 ( 77.4%) ‚Üí KEPT\n",
            "    Multi-aspect segments:    28,839 ( 22.6%) ‚Üí DROPPED\n",
            "\n",
            "  Label distribution:\n",
            "    negative  :   6,559 (  6.6%)\n",
            "    positive  :  92,387 ( 93.4%)\n",
            "\n",
            "‚úì Data preparation complete: 98,946 segments ready for training\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING & PREPARING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load training data\n",
        "df = pd.read_pickle(DATA_PATH)\n",
        "print(f\"  Raw segments loaded: {len(df):,}\")\n",
        "\n",
        "# --- PREVENT DATA LEAKAGE: Exclude gold standard review IDs ----------\n",
        "print(f\"\\n  ‚ö†Ô∏è  DATA LEAKAGE PREVENTION:\")\n",
        "print(f\"  Loading gold standard to identify held-out review IDs...\")\n",
        "\n",
        "try:\n",
        "    gold_df = pd.read_csv(GOLD_PATH)\n",
        "\n",
        "    # Extract unique Original_Review_IDs from gold dataset\n",
        "    if 'Original_Review_ID' in gold_df.columns:\n",
        "        gold_review_ids = set(gold_df['Original_Review_ID'].unique())\n",
        "    elif 'Review_ID' in gold_df.columns:\n",
        "        gold_review_ids = set(gold_df['Review_ID'].unique())\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  Warning: Could not find review ID column in gold dataset\")\n",
        "        gold_review_ids = set()\n",
        "\n",
        "    print(f\"  ‚úì Gold dataset loaded: {len(gold_df):,} annotations\")\n",
        "    print(f\"  ‚úì Unique review IDs in gold: {len(gold_review_ids):,}\")\n",
        "\n",
        "    # Filter out segments from gold review IDs\n",
        "    n_before = len(df)\n",
        "    df = df[~df['Original_Review_ID'].isin(gold_review_ids)].copy()\n",
        "    n_after = len(df)\n",
        "    n_removed = n_before - n_after\n",
        "\n",
        "    print(f\"  ‚úì Filtered out {n_removed:,} segments from gold reviews ({n_removed/n_before*100:.1f}%)\")\n",
        "    print(f\"  ‚úì Training segments remaining: {n_after:,}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ‚úó Error loading gold dataset: {e}\")\n",
        "\n",
        "# Filter to single-aspect segments\n",
        "df[\"num_aspects\"] = df[\"Aspect_Labels\"].apply(len)\n",
        "df_single = df[df[\"num_aspects\"] == 1].copy()\n",
        "df_single[\"aspect\"] = df_single[\"Aspect_Labels\"].apply(lambda x: x[0])\n",
        "\n",
        "n_multi = len(df) - len(df_single)\n",
        "pct_retained = (len(df_single) / len(df)) * 100\n",
        "\n",
        "print(f\"\\n  FILTERING STRATEGY:\")\n",
        "print(f\"    Single-aspect segments:  {len(df_single):>7,} ({pct_retained:>5.1f}%) ‚Üí KEPT\")\n",
        "print(f\"    Multi-aspect segments:   {n_multi:>7,} ({100-pct_retained:>5.1f}%) ‚Üí DROPPED\")\n",
        "\n",
        "# Encode labels\n",
        "df_single[\"label\"] = df_single[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "print(f\"\\n  Label distribution:\")\n",
        "for label_name, label_id in LABEL2ID.items():\n",
        "    count = (df_single[\"label\"] == label_id).sum()\n",
        "    pct = count / len(df_single) * 100\n",
        "    print(f\"    {label_name:<10}: {count:>7,} ({pct:>5.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚úì Data preparation complete: {len(df_single):,} segments ready for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "873d5195",
      "metadata": {
        "id": "873d5195"
      },
      "source": [
        "# Stage 2: Text Preprocessing (Conventional NLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "884e0529",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "884e0529",
        "outputId": "3b6611e8-4a24-4efd-bcd3-0562c1720568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  The nasi lemak was incredibly sedap but the service was lambat!\n",
            "Processed: nasi lemak incredibly sedap service lambat\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Apply conventional NLP preprocessing pipeline.\n",
        "\n",
        "    Steps:\n",
        "    1. Lowercase conversion\n",
        "    2. Remove special characters (keep letters, numbers, spaces)\n",
        "    3. Tokenization\n",
        "    4. Remove English stopwords\n",
        "    5. Lemmatization\n",
        "\n",
        "    Why:\n",
        "        Traditional ML models (Random Forest, SVM) lack contextual understanding.\n",
        "        Preprocessing reduces noise and dimensionality for TF-IDF vectorization.\n",
        "\n",
        "    Note:\n",
        "        We do NOT remove Manglish terms (sedap, mamak) as they carry sentiment.\n",
        "        Stopwords removal is conservative to preserve sentiment-bearing phrases.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters but keep spaces\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join back\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Test preprocessing\n",
        "sample_text = \"The nasi lemak was incredibly sedap but the service was lambat!\"\n",
        "print(f\"Original:  {sample_text}\")\n",
        "print(f\"Processed: {preprocess_text(sample_text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "70e8d45f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70e8d45f",
        "outputId": "c6c76849-eaa0-4cc7-d8dc-94f8d840f66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PREPROCESSING TEXT DATA\n",
            "======================================================================\n",
            "Processing 98,946 segments...\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PREPROCESSING TEXT DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Apply preprocessing to all segments\n",
        "print(f\"Processing {len(df_single):,} segments...\")\n",
        "df_single['processed_text'] = df_single['Segment'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b2b5e51c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "b2b5e51c",
        "outputId": "05cb6c50-cf8a-42eb-a319-56bb2826d879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EMPTY PROCESSED TEXTS ORIGNIAL SEGMENTS\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Segment\n",
              "again                 46\n",
              "once again             4\n",
              "over again             2\n",
              "again)                 1\n",
              "again not Ë±¨Ê≤πÊ∏£          1\n",
              "again ‚ù§Ô∏èüòä              1\n",
              "here again             1\n",
              "was again              1\n",
              "because again          1\n",
              "will be here again     1\n",
              "i was here again       1\n",
              "so so√¢‚Ç¨¬¶               1\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Segment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>again</th>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>once again</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>over again</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>again)</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>again not Ë±¨Ê≤πÊ∏£</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>again ‚ù§Ô∏èüòä</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>here again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>was again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>because again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>will be here again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i was here again</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>so so√¢‚Ç¨¬¶</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Check for empty processed texts\n",
        "print(\"=\"*70)\n",
        "print(\"EMPTY PROCESSED TEXTS ORIGNIAL SEGMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_single[df_single['processed_text'].str.len() == 0]['Segment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "93c97e67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c97e67",
        "outputId": "a3295ad1-d888-4028-9b29-83789bd6224b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚ö†Ô∏è  Warning: 61 segments became empty after preprocessing\n",
            "  ‚úì Removed empty segments. Remaining: 98,885\n",
            "\n",
            "‚úì Text preprocessing complete\n",
            "\n",
            "Sample processed segments:\n",
            "\n",
            "  Original:  aaliya famansara serves some of the best crab curry...\n",
            "  Processed: aaliya famansara serf best crab curry...\n",
            "\n",
            "  Original:  coconut cream - the perfect finish to the meal...\n",
            "  Processed: coconut cream perfect finish meal...\n",
            "\n",
            "  Original:  cooked perfectly...\n",
            "  Processed: cooked perfectly...\n"
          ]
        }
      ],
      "source": [
        "# Check for empty processed texts\n",
        "n_empty = (df_single['processed_text'].str.len() == 0).sum()\n",
        "\n",
        "if n_empty > 0:\n",
        "    print(f\"  ‚ö†Ô∏è  Warning: {n_empty} segments became empty after preprocessing\")\n",
        "    df_single = df_single[df_single['processed_text'].str.len() > 0].copy()\n",
        "    print(f\"  ‚úì Removed empty segments. Remaining: {len(df_single):,}\")\n",
        "\n",
        "print(f\"\\n‚úì Text preprocessing complete\")\n",
        "print(f\"\\nSample processed segments:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\n  Original:  {df_single.iloc[i]['Segment'][:80]}...\")\n",
        "    print(f\"  Processed: {df_single.iloc[i]['processed_text'][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3846e566",
      "metadata": {
        "id": "3846e566"
      },
      "source": [
        "# Stage 3: Train/Val/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3e32ef73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e32ef73",
        "outputId": "48d5463f-86e6-48dd-baa7-01f8a959de6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAIN/VAL/TEST SPLIT\n",
            "======================================================================\n",
            "\n",
            "Split sizes:\n",
            "  Train :  74,163 rows | pos: 69,249 ( 93.4%) | neg:  4,914 (  6.6%)\n",
            "  Val   :   9,889 rows | pos:  9,234 ( 93.4%) | neg:    655 (  6.6%)\n",
            "  Test  :  14,833 rows | pos: 13,850 ( 93.4%) | neg:    983 (  6.6%)\n",
            "\n",
            "‚úì Data split complete\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TRAIN/VAL/TEST SPLIT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Stage 1: Separate test set (stratified)\n",
        "df_trainval, df_test = train_test_split(\n",
        "    df_single,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=df_single[\"label\"],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Stage 2: Split remainder into train + val\n",
        "adjusted_val_size = VAL_SIZE / (1.0 - TEST_SIZE)\n",
        "df_train, df_val = train_test_split(\n",
        "    df_trainval,\n",
        "    test_size=adjusted_val_size,\n",
        "    stratify=df_trainval[\"label\"],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit sizes:\")\n",
        "for name, split_df in [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]:\n",
        "    pos = (split_df[\"label\"] == 1).sum()\n",
        "    neg = (split_df[\"label\"] == 0).sum()\n",
        "    print(f\"  {name:<6}: {len(split_df):>7,} rows | \"\n",
        "          f\"pos: {pos:>6,} ({pos/len(split_df)*100:>5.1f}%) | \"\n",
        "          f\"neg: {neg:>6,} ({neg/len(split_df)*100:>5.1f}%)\")\n",
        "\n",
        "# Extract X (processed text) and y (labels)\n",
        "X_train = df_train['processed_text'].values\n",
        "X_val = df_val['processed_text'].values\n",
        "X_test = df_test['processed_text'].values\n",
        "\n",
        "y_train = df_train['label'].values\n",
        "y_val = df_val['label'].values\n",
        "y_test = df_test['label'].values\n",
        "\n",
        "print(f\"\\n‚úì Data split complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96efa955",
      "metadata": {
        "id": "96efa955"
      },
      "source": [
        "# Stage 4: TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e488170a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e488170a",
        "outputId": "07785950-fe5f-4321-8eba-bd6e6149a6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TF-IDF VECTORIZATION\n",
            "======================================================================\n",
            "TF-IDF Configuration:\n",
            "  Max features:  5,000\n",
            "  N-gram range:  (1, 2)\n",
            "  Min doc freq:  2\n",
            "  Max doc freq:  0.8\n",
            "\n",
            "Fitting TF-IDF on 74,163 training samples...\n",
            "\n",
            "‚úì TF-IDF vectorization complete\n",
            "  Train shape: (74163, 5000)\n",
            "  Val shape:   (9889, 5000)\n",
            "  Test shape:  (14833, 5000)\n",
            "  Vocabulary size: 5,000 terms\n",
            "\n",
            "Top 20 features by TF-IDF score:\n",
            "  food                 2074.25\n",
            "  service              1372.29\n",
            "  good                 1215.74\n",
            "  delicious            1152.46\n",
            "  friendly             1044.19\n",
            "  staff                937.57\n",
            "  chicken              837.92\n",
            "  taste                784.83\n",
            "  price                774.84\n",
            "  fresh                741.10\n",
            "  dish                 683.63\n",
            "  great                637.04\n",
            "  attentive            634.21\n",
            "  tasty                632.00\n",
            "  place                604.43\n",
            "  nice                 575.18\n",
            "  restaurant           537.36\n",
            "  also                 525.34\n",
            "  come                 520.60\n",
            "  rice                 514.69\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TF-IDF VECTORIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,     # Limit to top 5000 features (prevent overfitting)\n",
        "    ngram_range=(1, 2),    # Unigrams + bigrams (capture phrases like \"nasi lemak\")\n",
        "    min_df=2,              # Ignore terms appearing in < 2 documents\n",
        "    max_df=0.8,            # Ignore terms appearing in > 80% of documents\n",
        "    sublinear_tf=True      # Apply sublinear tf scaling (1 + log(tf))\n",
        ")\n",
        "\n",
        "print(f\"TF-IDF Configuration:\")\n",
        "print(f\"  Max features:  {vectorizer.max_features:,}\")\n",
        "print(f\"  N-gram range:  {vectorizer.ngram_range}\")\n",
        "print(f\"  Min doc freq:  {vectorizer.min_df}\")\n",
        "print(f\"  Max doc freq:  {vectorizer.max_df}\")\n",
        "\n",
        "# Fit on training data and transform all splits\n",
        "print(f\"\\nFitting TF-IDF on {len(X_train):,} training samples...\")\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"\\n‚úì TF-IDF vectorization complete\")\n",
        "print(f\"  Train shape: {X_train_tfidf.shape}\")\n",
        "print(f\"  Val shape:   {X_val_tfidf.shape}\")\n",
        "print(f\"  Test shape:  {X_test_tfidf.shape}\")\n",
        "print(f\"  Vocabulary size: {len(vectorizer.vocabulary_):,} terms\")\n",
        "\n",
        "# Show top features by TF-IDF score\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = X_train_tfidf.sum(axis=0).A1\n",
        "top_indices = tfidf_scores.argsort()[-20:][::-1]\n",
        "\n",
        "print(f\"\\nTop 20 features by TF-IDF score:\")\n",
        "for idx in top_indices:\n",
        "    print(f\"  {feature_names[idx]:<20} {tfidf_scores[idx]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0905141c",
      "metadata": {
        "id": "0905141c"
      },
      "source": [
        "# Stage 5: Multi-Model Training\n",
        "\n",
        "**Academic Justification:**\n",
        "- Compare 5 traditional ML classifiers: Logistic Regression, SVM, Naive Bayes, XGBoost, Random Forest\n",
        "- Evaluate on validation set to identify best model(s) for hyperparameter tuning\n",
        "- Different models capture different patterns: linear (LR, SVM) vs non-linear (RF, XGB) vs probabilistic (NB)\n",
        "- Best practice: broad comparison before expensive hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "537af86a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "537af86a",
        "outputId": "725d953c-1d94-49d7-82de-209f5f894704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING MULTIPLE ML MODELS\n",
            "======================================================================\n",
            "Class imbalance handling:\n",
            "  Negative weight: 7.5461\n",
            "  Positive weight: 0.5355\n",
            "  XGBoost scale_pos_weight: 14.0922\n",
            "\n",
            "======================================================================\n",
            "TRAINING 5 MODELS\n",
            "======================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "  ‚úì Complete in 1.64 seconds\n",
            "\n",
            "Training SVM (Linear)...\n",
            "  ‚úì Complete in 236.81 seconds\n",
            "\n",
            "Training Naive Bayes...\n",
            "  ‚úì Complete in 0.01 seconds\n",
            "\n",
            "Training XGBoost...\n",
            "  ‚úì Complete in 5.01 seconds\n",
            "\n",
            "Training Random Forest...\n",
            "  ‚úì Complete in 1.14 seconds\n",
            "\n",
            "‚úì All models trained successfully\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING MULTIPLE ML MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compute class weights (for models that support it)\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = {i: class_weights_array[i] for i in range(len(class_weights_array))}\n",
        "\n",
        "# Calculate scale_pos_weight for XGBoost (ratio of negative to positive)\n",
        "scale_pos_weight = class_weights_array[0] / class_weights_array[1]\n",
        "\n",
        "print(f\"Class imbalance handling:\")\n",
        "print(f\"  Negative weight: {class_weights_dict[0]:.4f}\")\n",
        "print(f\"  Positive weight: {class_weights_dict[1]:.4f}\")\n",
        "print(f\"  XGBoost scale_pos_weight: {scale_pos_weight:.4f}\")\n",
        "\n",
        "# Define 5 models with reasonable default parameters\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    \"SVM (Linear)\": SVC(\n",
        "        kernel='linear',\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=RANDOM_SEED\n",
        "    ),\n",
        "    \"Naive Bayes\": MultinomialNB(\n",
        "        alpha=1.0\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='logloss'\n",
        "    ),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"TRAINING {len(models)} MODELS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Train all models and store results\n",
        "trained_models = {}\n",
        "training_times = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    trained_models[model_name] = model\n",
        "    training_times[model_name] = training_time\n",
        "\n",
        "    print(f\"  ‚úì Complete in {training_time:.2f} seconds\\n\")\n",
        "\n",
        "print(f\"‚úì All models trained successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9acccc",
      "metadata": {
        "id": "8d9acccc"
      },
      "source": [
        "# Stage 6: Model Comparison on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f8383a56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8383a56",
        "outputId": "d63359ce-3c32-4f7c-fdbd-b86c39ff5950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "COMPARING MODELS ON VALIDATION SET\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "VALIDATION SET RESULTS (Ranked by Macro-F1)\n",
            "======================================================================\n",
            "\n",
            "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
            "Logistic Regression    0.8235    0.6293       0.3610       0.8976           1.642203\n",
            "       SVM (Linear)    0.8015    0.6104       0.3375       0.8833         236.813104\n",
            "        Naive Bayes    0.9401    0.6030       0.2371       0.9688           0.008898\n",
            "            XGBoost    0.9360    0.5159       0.0650       0.9669           5.012386\n",
            "      Random Forest    0.6375    0.4989       0.2354       0.7624           1.141841\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPARING MODELS ON VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate all models on validation set\n",
        "results = []\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute same metrics as BERT for fair comparison.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    per_class_f1 = f1_score(y_true, y_pred, average=None, labels=[0, 1])\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": round(acc, 4),\n",
        "        \"macro_f1\": round(macro_f1, 4),\n",
        "        \"neg_f1\": round(per_class_f1[0], 4),\n",
        "        \"pos_f1\": round(per_class_f1[1], 4),\n",
        "    }\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    # Predict on validation set\n",
        "    y_pred_val = model.predict(X_val_tfidf)\n",
        "\n",
        "    # Compute metrics\n",
        "    val_metrics = compute_metrics(y_val, y_pred_val)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': val_metrics['accuracy'],\n",
        "        'Macro-F1': val_metrics['macro_f1'],\n",
        "        'Negative F1': val_metrics['neg_f1'],\n",
        "        'Positive F1': val_metrics['pos_f1'],\n",
        "        'Training Time (s)': training_times[model_name]\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"VALIDATION SET RESULTS (Ranked by Macro-F1)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ffcb075",
      "metadata": {
        "id": "3ffcb075"
      },
      "source": [
        "# Stage 7: Model Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "341e09de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "341e09de",
        "outputId": "32b0ae8b-8663-45b8-a8a4-5dec0303ecfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "COMPARING MODELS ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "TEST SET RESULTS (Ranked by Macro-F1)\n",
            "======================================================================\n",
            "\n",
            "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
            "Logistic Regression    0.8201    0.6246       0.3537       0.8955           1.642203\n",
            "       SVM (Linear)    0.7986    0.6092       0.3372       0.8812         236.813104\n",
            "        Naive Bayes    0.9407    0.6014       0.2337       0.9692           0.008898\n",
            "            XGBoost    0.9360    0.5159       0.0650       0.9668           5.012386\n",
            "      Random Forest    0.6393    0.5007       0.2377       0.7638           1.141841\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPARING MODELS ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate all models on test set\n",
        "results = []\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    # Predict on test set\n",
        "    y_pred_test = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_metrics = compute_metrics(y_test, y_pred_test)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': test_metrics['accuracy'],\n",
        "        'Macro-F1': test_metrics['macro_f1'],\n",
        "        'Negative F1': test_metrics['neg_f1'],\n",
        "        'Positive F1': test_metrics['pos_f1'],\n",
        "        'Training Time (s)': training_times[model_name]\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"TEST SET RESULTS (Ranked by Macro-F1)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7fa5ef",
      "metadata": {
        "id": "be7fa5ef"
      },
      "source": [
        "# Stage 8: Gold Standard Evaluation (Before Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "51f17999",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51f17999",
        "outputId": "5bba7622-0bbe-4763-f849-65252e9f0b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EVALUATING ON GOLD STANDARD (GROUND TRUTH)\n",
            "======================================================================\n",
            "  Gold dataset loaded: 799 rows\n",
            "  After exploding: 899 aspect-segment pairs\n",
            "\n",
            "Preprocessing gold segments...\n",
            "Vectorizing gold segments...\n",
            "Running inference on gold set using ...\n",
            "\n",
            "======================================================================\n",
            "GOLD SET RESULTS (Ranked by Macro-F1)\n",
            "======================================================================\n",
            "\n",
            "              Model  Accuracy  Macro-F1  Negative F1  Positive F1  Training Time (s)\n",
            "Logistic Regression    0.8309    0.8146       0.7595       0.8696           1.642203\n",
            "       SVM (Linear)    0.8220    0.8054       0.7484       0.8623         236.813104\n",
            "      Random Forest    0.7553    0.7493       0.7105       0.7881           1.141841\n",
            "        Naive Bayes    0.6696    0.5164       0.2443       0.7886           0.008898\n",
            "            XGBoost    0.6396    0.4442       0.1148       0.7737           5.012386\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"EVALUATING ON GOLD STANDARD (GROUND TRUTH)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load gold standard\n",
        "gold_df = pd.read_csv(GOLD_PATH)\n",
        "print(f\"  Gold dataset loaded: {len(gold_df):,} rows\")\n",
        "\n",
        "# Prepare gold data (same as BERT)\n",
        "gold_df_prep = gold_df.copy()\n",
        "gold_df_prep.rename(columns={\n",
        "    \"Manual_Aspect\": \"aspect\",\n",
        "    \"Manual_Sentiment\": \"Sentiment_Label\",\n",
        "}, inplace=True)\n",
        "\n",
        "# Normalize sentiment labels to lowercase\n",
        "gold_df_prep[\"Sentiment_Label\"] = gold_df_prep[\"Sentiment_Label\"].str.lower()\n",
        "\n",
        "# Handle multi-aspect segments: explode into separate rows\n",
        "import ast\n",
        "def parse_aspect(val):\n",
        "    if isinstance(val, str):\n",
        "        try:\n",
        "            parsed = ast.literal_eval(val)\n",
        "            if isinstance(parsed, list):\n",
        "                return parsed\n",
        "            else:\n",
        "                return [parsed]\n",
        "        except (ValueError, SyntaxError):\n",
        "            return [val]\n",
        "    elif isinstance(val, list):\n",
        "        return val\n",
        "    else:\n",
        "        return [str(val)]\n",
        "\n",
        "gold_df_prep[\"aspect\"] = gold_df_prep[\"aspect\"].apply(parse_aspect)\n",
        "gold_df_exploded = gold_df_prep.explode(\"aspect\").reset_index(drop=True)\n",
        "\n",
        "print(f\"  After exploding: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "\n",
        "# Encode labels\n",
        "gold_df_exploded[\"label\"] = gold_df_exploded[\"Sentiment_Label\"].map(LABEL2ID)\n",
        "\n",
        "# Preprocess gold text\n",
        "print(f\"\\nPreprocessing gold segments...\")\n",
        "gold_df_exploded['processed_text'] = gold_df_exploded['Segment'].apply(preprocess_text)\n",
        "\n",
        "# Vectorize gold text\n",
        "print(f\"Vectorizing gold segments...\")\n",
        "X_gold = gold_df_exploded['processed_text'].values\n",
        "X_gold_tfidf = vectorizer.transform(X_gold)\n",
        "y_gold = gold_df_exploded['label'].values\n",
        "\n",
        "# Predict on gold using best model\n",
        "print(f\"Running inference on gold set using ...\")\n",
        "\n",
        "# Evaluate all models on gold data set\n",
        "results = []\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    # Predict on test set\n",
        "    y_pred_gold = model.predict(X_gold_tfidf)\n",
        "\n",
        "    # Compute metrics\n",
        "    gold_metrics = compute_metrics(y_gold, y_pred_gold)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': gold_metrics['accuracy'],\n",
        "        'Macro-F1': gold_metrics['macro_f1'],\n",
        "        'Negative F1': gold_metrics['neg_f1'],\n",
        "        'Positive F1': gold_metrics['pos_f1'],\n",
        "        'Training Time (s)': training_times[model_name]\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('Macro-F1', ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"GOLD SET RESULTS (Ranked by Macro-F1)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "196035b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "196035b6",
        "outputId": "5d0b38c9-c924-44b9-8bb4-d2e21a2895f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üèÜ BEST MODEL: Logistic Regression\n",
            "‚òÖ GOLD TEST SET RESULTS - Logistic Regression\n",
            "‚òÖ Total samples: 899 aspect-segment pairs\n",
            "======================================================================\n",
            "\n",
            "OVERALL PERFORMANCE:\n",
            "  Accuracy:  0.8309\n",
            "  Macro-F1:  0.8146\n",
            "    Negative F1: 0.7595\n",
            "    Positive F1: 0.8696\n",
            "\n",
            "‚ö†Ô∏è  Note: This is baseline performance before hyperparameter tuning\n"
          ]
        }
      ],
      "source": [
        "# Identify best model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_macro_f1 = results_df.iloc[0]['Macro-F1']\n",
        "\n",
        "# Store best model for hyperparameter tuning\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
        "\n",
        "# Best model prediction results\n",
        "y_pred_gold = best_model.predict(X_gold_tfidf)\n",
        "\n",
        "# Compute metrics\n",
        "gold_metrics = compute_metrics(y_gold, y_pred_gold)\n",
        "\n",
        "print(f\"‚òÖ GOLD TEST SET RESULTS - {best_model_name}\")\n",
        "print(f\"‚òÖ Total samples: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "print(f\"  Accuracy:  {gold_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:  {gold_metrics['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {gold_metrics['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {gold_metrics['pos_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Note: This is baseline performance before hyperparameter tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HYPERPARAMETER TUNING: LOGISTIC REGRESSION (LIGHTWEIGHT)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Safety check: ensure required data splits exist\n",
        "required_vars = [\"df_trainval\", \"df_test\", \"gold_df_exploded\", \"class_weights_dict\", \"RANDOM_SEED\"]\n",
        "missing_vars = [var for var in required_vars if var not in globals()]\n",
        "if missing_vars:\n",
        "    raise ValueError(f\"Missing required variables before tuning: {missing_vars}\")\n",
        "\n",
        "# Build a full pipeline so CV is leakage-safe within each fold\n",
        "lr_tuning_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=0.8,\n",
        "        sublinear_tf=True\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight=class_weights_dict,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Reduced search space for faster runtime:\n",
        "# 2 solvers √ó 1 penalty √ó 4 C √ó 1 max_iter = 8 candidates\n",
        "param_grid = [\n",
        "    {\n",
        "        \"clf__solver\": [\"liblinear\", \"saga\"],\n",
        "        \"clf__penalty\": [\"l2\"],\n",
        "        \"clf__C\": [0.1, 1.0, 3.0, 10.0],\n",
        "        \"clf__max_iter\": [1000],\n",
        "    }\n",
        "]\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "grid_search_lr = GridSearchCV(\n",
        "    estimator=lr_tuning_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "print(\"\\nRunning lightweight GridSearchCV on train+val (Macro-F1)...\")\n",
        "print(\"Expected workload: 8 candidates √ó 3 folds = 24 fits\")\n",
        "grid_search_lr.fit(\n",
        "    df_trainval[\"processed_text\"].values,\n",
        "    df_trainval[\"label\"].values\n",
        ")\n",
        "\n",
        "best_lr_tuned_pipeline = grid_search_lr.best_estimator_\n",
        "\n",
        "print(\"\\n‚úì Hyperparameter tuning complete\")\n",
        "print(f\"Best CV Macro-F1: {grid_search_lr.best_score_:.4f}\")\n",
        "print(\"Best params:\")\n",
        "for key, value in grid_search_lr.best_params_.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtoMLQUAoM-1",
        "outputId": "c1985a8a-70ec-4b53-a1f5-1197274c5e6f"
      },
      "id": "UtoMLQUAoM-1",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "HYPERPARAMETER TUNING: LOGISTIC REGRESSION (LIGHTWEIGHT)\n",
            "======================================================================\n",
            "\n",
            "Running lightweight GridSearchCV on train+val (Macro-F1)...\n",
            "Expected workload: 8 candidates √ó 3 folds = 24 fits\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "\n",
            "‚úì Hyperparameter tuning complete\n",
            "Best CV Macro-F1: 0.6228\n",
            "Best params:\n",
            "  clf__C: 0.1\n",
            "  clf__max_iter: 1000\n",
            "  clf__penalty: l2\n",
            "  clf__solver: saga\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"EVALUATING TUNED LOGISTIC REGRESSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate on TEST (held-out)\n",
        "y_pred_test_tuned_lr = best_lr_tuned_pipeline.predict(df_test[\"processed_text\"].values)\n",
        "test_metrics_tuned_lr = compute_metrics(df_test[\"label\"].values, y_pred_test_tuned_lr)\n",
        "\n",
        "# Evaluate on GOLD (manual ground truth)\n",
        "y_pred_gold_tuned_lr = best_lr_tuned_pipeline.predict(gold_df_exploded[\"processed_text\"].values)\n",
        "gold_metrics_tuned_lr = compute_metrics(gold_df_exploded[\"label\"].values, y_pred_gold_tuned_lr)\n",
        "\n",
        "print(\"\\nTUNED LOGISTIC REGRESSION - TEST SET:\")\n",
        "print(f\"  Accuracy:  {test_metrics_tuned_lr['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:  {test_metrics_tuned_lr['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {test_metrics_tuned_lr['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {test_metrics_tuned_lr['pos_f1']:.4f}\")\n",
        "\n",
        "print(\"\\nTUNED LOGISTIC REGRESSION - GOLD SET:\")\n",
        "print(f\"  Accuracy:  {gold_metrics_tuned_lr['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:  {gold_metrics_tuned_lr['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {gold_metrics_tuned_lr['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {gold_metrics_tuned_lr['pos_f1']:.4f}\")\n",
        "\n",
        "# Compare with baseline best-model metrics from previous stage if available\n",
        "if \"gold_metrics\" in globals() and \"best_model_name\" in globals() and best_model_name == \"Logistic Regression\":\n",
        "    delta_macro_f1 = gold_metrics_tuned_lr[\"macro_f1\"] - gold_metrics[\"macro_f1\"]\n",
        "    delta_acc = gold_metrics_tuned_lr[\"accuracy\"] - gold_metrics[\"accuracy\"]\n",
        "    print(\"\\nŒî vs Baseline Logistic Regression (Gold):\")\n",
        "    print(f\"  Œî Accuracy: {delta_acc:+.4f}\")\n",
        "    print(f\"  Œî Macro-F1: {delta_macro_f1:+.4f}\")\n",
        "\n",
        "# Optional: use tuned LR as final model if it improves on gold macro-F1\n",
        "if \"gold_metrics\" in globals() and best_model_name == \"Logistic Regression\":\n",
        "    if gold_metrics_tuned_lr[\"macro_f1\"] >= gold_metrics[\"macro_f1\"]:\n",
        "        final_model = best_lr_tuned_pipeline\n",
        "        final_model_name = \"Logistic Regression (Tuned)\"\n",
        "    else:\n",
        "        final_model = best_model\n",
        "        final_model_name = best_model_name\n",
        "else:\n",
        "    final_model = best_lr_tuned_pipeline\n",
        "    final_model_name = \"Logistic Regression (Tuned)\"\n",
        "\n",
        "print(f\"\\nFinal selected model for deployment: {final_model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeiStbJNoQZn",
        "outputId": "a3cc15cf-37fe-4b0f-c5e7-bc8574e6d090"
      },
      "id": "CeiStbJNoQZn",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EVALUATING TUNED LOGISTIC REGRESSION\n",
            "======================================================================\n",
            "\n",
            "TUNED LOGISTIC REGRESSION - TEST SET:\n",
            "  Accuracy:  0.6889\n",
            "  Macro-F1:  0.5365\n",
            "    Negative F1: 0.2708\n",
            "    Positive F1: 0.8022\n",
            "\n",
            "TUNED LOGISTIC REGRESSION - GOLD SET:\n",
            "  Accuracy:  0.8031\n",
            "  Macro-F1:  0.7958\n",
            "    Negative F1: 0.7572\n",
            "    Positive F1: 0.8344\n",
            "\n",
            "Œî vs Baseline Logistic Regression (Gold):\n",
            "  Œî Accuracy: -0.0278\n",
            "  Œî Macro-F1: -0.0188\n",
            "\n",
            "Final selected model for deployment: Logistic Regression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ca082d8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca082d8a",
        "outputId": "c37e8490-9693-4bea-e30b-bdc3c280fd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PER-ASPECT BREAKDOWN (for comparison with BERT):\n",
            "Aspect                Samples   Accuracy   Macro-F1\n",
            "-------------------- -------- ---------- ----------\n",
            "AMBIENCE                   94     0.8298     0.7697\n",
            "AUTHENTICITY & LOCAL VIBE       24     0.7917     0.7052\n",
            "FOOD                      375     0.8773     0.8571\n",
            "HALAL COMPLIANCE            2     1.0000     1.0000\n",
            "LOCATION                   22     0.5000     0.4905\n",
            "LOYALTY (RETURN INTENT)      103     0.7670     0.7366\n",
            "NON-HALAL ELEMENTS         21     0.8095     0.6912\n",
            "SERVICE                   152     0.9013     0.9013\n",
            "VALUE                     105     0.7048     0.7030\n",
            "['AUTHENTICITY & LOCAL VIBE'        1     1.0000     1.0000\n"
          ]
        }
      ],
      "source": [
        "# Per-aspect breakdown\n",
        "print(f\"\\nPER-ASPECT BREAKDOWN (for comparison with BERT):\")\n",
        "print(f\"{'Aspect':<20} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
        "print(f\"{'-'*20} {'-'*8} {'-'*10} {'-'*10}\")\n",
        "\n",
        "aspects_unique = sorted(gold_df_exploded[\"aspect\"].unique())\n",
        "for aspect in aspects_unique:\n",
        "    mask = gold_df_exploded[\"aspect\"] == aspect\n",
        "    y_aspect = y_gold[mask]\n",
        "    y_pred_aspect = y_pred_gold[mask]\n",
        "\n",
        "    try:\n",
        "        metrics = compute_metrics(y_aspect, y_pred_aspect)\n",
        "        n_samples = mask.sum()\n",
        "        print(f\"{aspect:<20} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
        "    except:\n",
        "        print(f\"{aspect:<20} {mask.sum():>8} {'N/A':>10} {'N/A':>10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "928fd992",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "928fd992",
        "outputId": "f7d5f052-4328-4b53-d481-f0359466f791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FULL CLASSIFICATION REPORT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.8362    0.6957    0.7595       345\n",
            "    Positive     0.8284    0.9152    0.8696       554\n",
            "\n",
            "    accuracy                         0.8309       899\n",
            "   macro avg     0.8323    0.8054    0.8146       899\n",
            "weighted avg     0.8314    0.8309    0.8274       899\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nFULL CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_gold, y_pred_gold, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dgvHMbh0cd_z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgvHMbh0cd_z",
        "outputId": "85741941-5f3b-4ddf-be87-3678de2d136b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved TF-IDF Vectorizer to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/tfidf_vectorizer.pkl\n",
            "‚úÖ Saved model 'Logistic Regression' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/Logistic_Regression.pkl\n",
            "‚úÖ Saved model 'SVM (Linear)' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/SVM_(Linear).pkl\n",
            "‚úÖ Saved model 'Naive Bayes' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/Naive_Bayes.pkl\n",
            "‚úÖ Saved model 'XGBoost' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/XGBoost.pkl\n",
            "‚úÖ Saved model 'Random Forest' to: /content/drive/MyDrive/Aspect-Based-Sentiment-Analysis/Modelling/models/Random_Forest.pkl\n",
            "\n",
            "All saving operations completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "\n",
        "# Save the Vectorizer\n",
        "vectorizer_path = os.path.join(OUTPUT_PATH, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(vectorizer, vectorizer_path)\n",
        "print(f\"‚úÖ Saved TF-IDF Vectorizer to: {vectorizer_path}\")\n",
        "\n",
        "# Save each model from the 'trained_models' dictionary\n",
        "for model_name, model in trained_models.items():\n",
        "    # create a safe filename (replace spaces with underscores)\n",
        "    safe_name = model_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
        "    file_path = os.path.join(OUTPUT_PATH, f\"{safe_name}.pkl\")\n",
        "\n",
        "    # Save the model object\n",
        "    joblib.dump(model, file_path)\n",
        "    print(f\"‚úÖ Saved model '{model_name}' to: {file_path}\")\n",
        "\n",
        "print(\"\\nAll saving operations completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NQojnFHQaD34",
      "metadata": {
        "id": "NQojnFHQaD34"
      },
      "source": [
        "# Stage 9: VADER Baseline Comparison\n",
        "\n",
        "**Academic Justification:**\n",
        "- VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool\n",
        "- Establishes rule-based baseline for 3-way architecture comparison: Rule-Based (VADER) vs Traditional ML vs Deep Learning (BERT)\n",
        "- VADER requires no training - applies directly to raw text\n",
        "- Evaluates on **same data splits** (validation, test, gold) for fair comparison\n",
        "- Expected to underperform on Manglish code-switching (lacks cultural context and mixed-language support)\n",
        "- Demonstrates value of ML/DL approaches for low-resource languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "68MJfD4iaEZS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68MJfD4iaEZS",
        "outputId": "d16fb2dd-200d-4715-d17d-79001d267bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing VADER...\n",
            "‚úì VADER installed successfully\n",
            "\n",
            "======================================================================\n",
            "VADER SENTIMENT ANALYSIS - RULE-BASED BASELINE\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  VADER Characteristics:\n",
            "  ‚Ä¢ Rule-based lexicon approach (no training required)\n",
            "  ‚Ä¢ Designed for English social media text\n",
            "  ‚Ä¢ Returns compound score: [-1.0, 1.0]\n",
            "  ‚Ä¢ Decision rule: compound >= 0.05 ‚Üí positive, < 0.05 ‚Üí negative\n",
            "  ‚Ä¢ Expected challenge: Manglish code-switching (e.g., 'sedap', 'mamak')\n"
          ]
        }
      ],
      "source": [
        "# Install and import VADER\n",
        "try:\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    print(\"‚úì VADER already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing VADER...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'vaderSentiment'])\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    print(\"‚úì VADER installed successfully\")\n",
        "\n",
        "# Initialize VADER analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VADER SENTIMENT ANALYSIS - RULE-BASED BASELINE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  VADER Characteristics:\")\n",
        "print(\"  ‚Ä¢ Rule-based lexicon approach (no training required)\")\n",
        "print(\"  ‚Ä¢ Designed for English social media text\")\n",
        "print(\"  ‚Ä¢ Returns compound score: [-1.0, 1.0]\")\n",
        "print(\"  ‚Ä¢ Decision rule: compound >= 0.05 ‚Üí positive, < 0.05 ‚Üí negative\")\n",
        "print(\"  ‚Ä¢ Expected challenge: Manglish code-switching (e.g., 'sedap', 'mamak')\")\n",
        "\n",
        "def vader_predict(text):\n",
        "    \"\"\"\n",
        "    Apply VADER sentiment analysis and convert to binary classification.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text segment\n",
        "\n",
        "    Returns:\n",
        "        int: 0 for negative, 1 for positive\n",
        "\n",
        "    Why compound score?\n",
        "        VADER's compound score normalizes sentiment across text length.\n",
        "        Standard threshold: >= 0.05 is positive, < 0.05 is negative.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return 1  # Default to positive if empty\n",
        "\n",
        "    scores = vader_analyzer.polarity_scores(text)\n",
        "    compound = scores['compound']\n",
        "\n",
        "    # Binary classification using standard threshold\n",
        "    return 1 if compound >= 0.05 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "BV--7N2maLHF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV--7N2maLHF",
        "outputId": "1058c24d-b9ab-484d-9d7c-b3695daf8a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "VADER EVALUATION ON VALIDATION SET\n",
            "======================================================================\n",
            "\n",
            "Applying VADER to 9,889 validation segments...\n",
            "\n",
            "‚òÖ VADER VALIDATION SET RESULTS:\n",
            "  Accuracy:    0.5258\n",
            "  Macro-F1:    0.4357\n",
            "    Negative F1: 0.2102\n",
            "    Positive F1: 0.6612\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.1181    0.9527    0.2102       655\n",
            "    Positive     0.9933    0.4956    0.6612      9234\n",
            "\n",
            "    accuracy                         0.5258      9889\n",
            "   macro avg     0.5557    0.7241    0.4357      9889\n",
            "weighted avg     0.9353    0.5258    0.6314      9889\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"VADER EVALUATION ON VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use ORIGINAL text (not preprocessed) for VADER\n",
        "X_val_original = df_val['Segment'].values\n",
        "\n",
        "print(f\"\\nApplying VADER to {len(X_val_original):,} validation segments...\")\n",
        "y_pred_val_vader = np.array([vader_predict(text) for text in X_val_original])\n",
        "\n",
        "# Compute metrics\n",
        "val_metrics_vader = compute_metrics(y_val, y_pred_val_vader)\n",
        "\n",
        "print(f\"\\n‚òÖ VADER VALIDATION SET RESULTS:\")\n",
        "print(f\"  Accuracy:    {val_metrics_vader['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:    {val_metrics_vader['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {val_metrics_vader['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {val_metrics_vader['pos_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred_val_vader, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "Kiz-ZklMaNIW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiz-ZklMaNIW",
        "outputId": "44ee2f2a-4032-499f-cc71-5d178577d1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "VADER EVALUATION ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "Applying VADER to 14,833 test segments...\n",
            "\n",
            "‚òÖ VADER TEST SET RESULTS:\n",
            "  Accuracy:    0.5302\n",
            "  Macro-F1:    0.4396\n",
            "    Negative F1: 0.2144\n",
            "    Positive F1: 0.6649\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.1206    0.9674    0.2144       983\n",
            "    Positive     0.9954    0.4991    0.6649     13850\n",
            "\n",
            "    accuracy                         0.5302     14833\n",
            "   macro avg     0.5580    0.7333    0.4396     14833\n",
            "weighted avg     0.9374    0.5302    0.6350     14833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"VADER EVALUATION ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use ORIGINAL text (not preprocessed) for VADER\n",
        "X_test_original = df_test['Segment'].values\n",
        "\n",
        "print(f\"\\nApplying VADER to {len(X_test_original):,} test segments...\")\n",
        "y_pred_test_vader = np.array([vader_predict(text) for text in X_test_original])\n",
        "\n",
        "# Compute metrics\n",
        "test_metrics_vader = compute_metrics(y_test, y_pred_test_vader)\n",
        "\n",
        "print(f\"\\n‚òÖ VADER TEST SET RESULTS:\")\n",
        "print(f\"  Accuracy:    {test_metrics_vader['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:    {test_metrics_vader['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {test_metrics_vader['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {test_metrics_vader['pos_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_test_vader, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5Aj3OXGXbWew",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aj3OXGXbWew",
        "outputId": "25702ad3-a50c-4e8c-971d-4bc5554c503b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "VADER EVALUATION ON GOLD STANDARD\n",
            "======================================================================\n",
            "\n",
            "Applying VADER to 899 gold standard segments...\n",
            "\n",
            "‚òÖ VADER GOLD STANDARD RESULTS:\n",
            "‚òÖ Total samples: 899 aspect-segment pairs\n",
            "\n",
            "OVERALL PERFORMANCE:\n",
            "  Accuracy:  0.7297\n",
            "  Macro-F1:  0.7282\n",
            "    Negative F1: 0.7083\n",
            "    Positive F1: 0.7482\n",
            "\n",
            "PER-ASPECT BREAKDOWN:\n",
            "Aspect                Samples   Accuracy   Macro-F1\n",
            "-------------------- -------- ---------- ----------\n",
            "AMBIENCE                   94     0.7660     0.7466\n",
            "AUTHENTICITY & LOCAL VIBE       24     0.6250     0.6190\n",
            "FOOD                      375     0.7387     0.7333\n",
            "HALAL COMPLIANCE            2     0.5000     0.3333\n",
            "LOCATION                   22     0.5909     0.5448\n",
            "LOYALTY (RETURN INTENT)      103     0.6699     0.6661\n",
            "NON-HALAL ELEMENTS         21     0.7143     0.6971\n",
            "SERVICE                   152     0.7303     0.7268\n",
            "VALUE                     105     0.7810     0.7789\n",
            "['AUTHENTICITY & LOCAL VIBE'        1     1.0000     1.0000\n",
            "\n",
            "FULL CLASSIFICATION REPORT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative     0.6045    0.8551    0.7083       345\n",
            "    Positive     0.8783    0.6516    0.7482       554\n",
            "\n",
            "    accuracy                         0.7297       899\n",
            "   macro avg     0.7414    0.7533    0.7282       899\n",
            "weighted avg     0.7733    0.7297    0.7329       899\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"VADER EVALUATION ON GOLD STANDARD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use ORIGINAL gold segments\n",
        "X_gold_original = gold_df_exploded['Segment'].values\n",
        "y_gold = gold_df_exploded['label'].values\n",
        "\n",
        "print(f\"\\nApplying VADER to {len(X_gold_original):,} gold standard segments...\")\n",
        "y_pred_gold_vader = np.array([vader_predict(text) for text in X_gold_original])\n",
        "\n",
        "# Compute metrics\n",
        "gold_metrics_vader = compute_metrics(y_gold, y_pred_gold_vader)\n",
        "\n",
        "print(f\"\\n‚òÖ VADER GOLD STANDARD RESULTS:\")\n",
        "print(f\"‚òÖ Total samples: {len(gold_df_exploded):,} aspect-segment pairs\")\n",
        "print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "print(f\"  Accuracy:  {gold_metrics_vader['accuracy']:.4f}\")\n",
        "print(f\"  Macro-F1:  {gold_metrics_vader['macro_f1']:.4f}\")\n",
        "print(f\"    Negative F1: {gold_metrics_vader['neg_f1']:.4f}\")\n",
        "print(f\"    Positive F1: {gold_metrics_vader['pos_f1']:.4f}\")\n",
        "\n",
        "# Per-aspect breakdown\n",
        "print(f\"\\nPER-ASPECT BREAKDOWN:\")\n",
        "print(f\"{'Aspect':<20} {'Samples':>8} {'Accuracy':>10} {'Macro-F1':>10}\")\n",
        "print(f\"{'-'*20} {'-'*8} {'-'*10} {'-'*10}\")\n",
        "\n",
        "aspects_unique = sorted(gold_df_exploded[\"aspect\"].unique())\n",
        "for aspect in aspects_unique:\n",
        "    mask = gold_df_exploded[\"aspect\"] == aspect\n",
        "    y_aspect = y_gold[mask]\n",
        "    y_pred_aspect = y_pred_gold_vader[mask]\n",
        "\n",
        "    try:\n",
        "        metrics = compute_metrics(y_aspect, y_pred_aspect)\n",
        "        n_samples = mask.sum()\n",
        "        print(f\"{aspect:<20} {n_samples:>8} {metrics['accuracy']:>10.4f} {metrics['macro_f1']:>10.4f}\")\n",
        "    except:\n",
        "        print(f\"{aspect:<20} {mask.sum():>8} {'N/A':>10} {'N/A':>10}\")\n",
        "\n",
        "print(f\"\\nFULL CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_gold, y_pred_gold_vader, target_names=[\"Negative\", \"Positive\"], digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fpGgIu4Hbig8",
      "metadata": {
        "id": "fpGgIu4Hbig8"
      },
      "source": [
        "# Stage 12: Three-Way Architecture Comparison\n",
        "\n",
        "**Comparison Framework:**\n",
        "- **VADER (Rule-Based)**: Lexicon + rules, no training, English-centric\n",
        "- **Traditional ML (Tuned)**: TF-IDF + SVM/Logistic Regression, supervised learning\n",
        "- **XLM-RoBERTa (BERT)**: Transformer with cross-lingual pre-training, contextual embeddings\n",
        "\n",
        "**Evaluation Protocol:**\n",
        "- Same data splits (validation, test, gold standard)\n",
        "- Same metrics (accuracy, macro-F1, negative F1, positive F1)\n",
        "- Gold standard as final ground truth for thesis comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "BJVyfj-MbjCQ",
      "metadata": {
        "id": "BJVyfj-MbjCQ"
      },
      "outputs": [],
      "source": [
        "# print(\"=\"*70)\n",
        "# print(\"THREE-WAY MODEL COMPARISON\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Create comprehensive comparison table\n",
        "# comparison_data = []\n",
        "\n",
        "# # VADER (Rule-Based)\n",
        "# comparison_data.append({\n",
        "#     'Model': 'VADER (Rule-Based)',\n",
        "#     'Architecture': 'Lexicon + Rules',\n",
        "#     'Training': 'None',\n",
        "#     'Val Accuracy': val_metrics_vader['accuracy'],\n",
        "#     'Val Macro-F1': val_metrics_vader['macro_f1'],\n",
        "#     'Test Accuracy': test_metrics_vader['accuracy'],\n",
        "#     'Test Macro-F1': test_metrics_vader['macro_f1'],\n",
        "#     'Gold Accuracy': gold_metrics_vader['accuracy'],\n",
        "#     'Gold Macro-F1': gold_metrics_vader['macro_f1'],\n",
        "#     'Gold Neg F1': gold_metrics_vader['neg_f1'],\n",
        "#     'Gold Pos F1': gold_metrics_vader['pos_f1']\n",
        "# })\n",
        "\n",
        "# # Traditional ML (Tuned)\n",
        "# comparison_data.append({\n",
        "#     'Model': f'Traditional ML ({best_model_name})',\n",
        "#     'Architecture': 'TF-IDF + ML',\n",
        "#     'Training': f'{len(X_train):,} samples',\n",
        "#     'Val Accuracy': val_metrics['accuracy'],  # From Stage 6\n",
        "#     'Val Macro-F1': val_metrics['macro_f1'],\n",
        "#     'Test Accuracy': test_metrics_tuned['accuracy'],\n",
        "#     'Test Macro-F1': test_metrics_tuned['macro_f1'],\n",
        "#     'Gold Accuracy': gold_metrics_tuned['accuracy'],\n",
        "#     'Gold Macro-F1': gold_metrics_tuned['macro_f1'],\n",
        "#     'Gold Neg F1': gold_metrics_tuned['neg_f1'],\n",
        "#     'Gold Pos F1': gold_metrics_tuned['pos_f1']\n",
        "# })\n",
        "\n",
        "# # XLM-RoBERTa (from your completed BERT training)\n",
        "# # Note: Update these values with your actual BERT results\n",
        "# comparison_data.append({\n",
        "#     'Model': 'XLM-RoBERTa (BERT)',\n",
        "#     'Architecture': 'Transformer',\n",
        "#     'Training': f'{len(X_train):,} samples',\n",
        "#     'Val Accuracy': 0.0000,  # ‚Üê UPDATE with your BERT val accuracy\n",
        "#     'Val Macro-F1': 0.0000,  # ‚Üê UPDATE with your BERT val macro-F1\n",
        "#     'Test Accuracy': 0.0000,  # ‚Üê UPDATE with your BERT test accuracy\n",
        "#     'Test Macro-F1': 0.0000,  # ‚Üê UPDATE with your BERT test macro-F1\n",
        "#     'Gold Accuracy': 0.9240,  # Your reported BERT gold accuracy\n",
        "#     'Gold Macro-F1': 0.9170,  # Your reported BERT gold macro-F1\n",
        "#     'Gold Neg F1': 0.8940,    # Your reported BERT negative F1\n",
        "#     'Gold Pos F1': 0.9340     # Your reported BERT positive F1\n",
        "# })\n",
        "\n",
        "# comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(f\"COMPARISON TABLE: GOLD STANDARD (GROUND TRUTH)\")\n",
        "# print(f\"{'='*70}\\n\")\n",
        "\n",
        "# # Display gold standard comparison (most important)\n",
        "# gold_comparison = comparison_df[['Model', 'Architecture', 'Gold Accuracy', 'Gold Macro-F1', 'Gold Neg F1', 'Gold Pos F1']]\n",
        "# print(gold_comparison.to_string(index=False))\n",
        "\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(f\"FULL COMPARISON TABLE: ALL EVALUATION SETS\")\n",
        "# print(f\"{'='*70}\\n\")\n",
        "# print(comparison_df.to_string(index=False))\n",
        "\n",
        "# # Summary insights\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(f\"KEY FINDINGS\")\n",
        "# print(f\"{'='*70}\")\n",
        "\n",
        "# vader_gold_f1 = gold_metrics_vader['macro_f1']\n",
        "# ml_gold_f1 = gold_metrics_tuned['macro_f1']\n",
        "# bert_gold_f1 = 0.9170  # Update with your actual BERT value\n",
        "\n",
        "# print(f\"\\nüìä Gold Standard Macro-F1 Performance:\")\n",
        "# print(f\"  1. XLM-RoBERTa (BERT):     {bert_gold_f1:.4f} ü•á\")\n",
        "# print(f\"  2. Traditional ML:         {ml_gold_f1:.4f} ü•à\")\n",
        "# print(f\"  3. VADER (Rule-Based):     {vader_gold_f1:.4f} ü•â\")\n",
        "\n",
        "# improvement_ml_vs_vader = ((ml_gold_f1 - vader_gold_f1) / vader_gold_f1) * 100\n",
        "# improvement_bert_vs_ml = ((bert_gold_f1 - ml_gold_f1) / ml_gold_f1) * 100\n",
        "\n",
        "# print(f\"\\nüìà Relative Improvements:\")\n",
        "# print(f\"  Traditional ML vs VADER:   +{improvement_ml_vs_vader:.1f}%\")\n",
        "# print(f\"  BERT vs Traditional ML:    +{improvement_bert_vs_ml:.1f}%\")\n",
        "\n",
        "# print(f\"\\nüí° Implications:\")\n",
        "# print(f\"  ‚úì Rule-based (VADER) struggles with Manglish code-switching\")\n",
        "# print(f\"  ‚úì Traditional ML benefits from supervised learning on domain data\")\n",
        "# print(f\"  ‚úì BERT's cross-lingual pre-training provides significant advantage\")\n",
        "# print(f\"  ‚úì Transformer architecture captures contextual nuances in code-switched text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "D8-6f5JZbme-",
      "metadata": {
        "id": "D8-6f5JZbme-"
      },
      "outputs": [],
      "source": [
        "# # Visualization: Gold Standard Comparison\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# # Plot 1: Macro-F1 Comparison\n",
        "# models = ['VADER\\n(Rule-Based)', f'Traditional ML\\n({best_model_name})', 'XLM-RoBERTa\\n(BERT)']\n",
        "# gold_f1_scores = [\n",
        "#     gold_metrics_vader['macro_f1'],\n",
        "#     gold_metrics_tuned['macro_f1'],\n",
        "#     0.9170  # Update with your actual BERT value\n",
        "# ]\n",
        "\n",
        "# colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
        "# bars = axes[0].bar(models, gold_f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "# axes[0].set_ylabel('Macro-F1 Score', fontsize=12, fontweight='bold')\n",
        "# axes[0].set_title('Gold Standard: Macro-F1 Comparison', fontsize=14, fontweight='bold')\n",
        "# axes[0].set_ylim([0, 1.0])\n",
        "# axes[0].axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, label='Strong Performance (0.8)')\n",
        "# axes[0].grid(axis='y', alpha=0.3)\n",
        "# axes[0].legend()\n",
        "\n",
        "# # Add value labels on bars\n",
        "# for bar in bars:\n",
        "#     height = bar.get_height()\n",
        "#     axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "#                 f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# # Plot 2: Per-Class F1 Comparison\n",
        "# class_labels = ['Negative F1', 'Positive F1']\n",
        "# x = np.arange(len(class_labels))\n",
        "# width = 0.25\n",
        "\n",
        "# vader_scores = [gold_metrics_vader['neg_f1'], gold_metrics_vader['pos_f1']]\n",
        "# ml_scores = [gold_metrics_tuned['neg_f1'], gold_metrics_tuned['pos_f1']]\n",
        "# bert_scores = [0.8940, 0.9340]  # Update with your actual BERT values\n",
        "\n",
        "# axes[1].bar(x - width, vader_scores, width, label='VADER', color=colors[0], alpha=0.8, edgecolor='black')\n",
        "# axes[1].bar(x, ml_scores, width, label=f'Traditional ML', color=colors[1], alpha=0.8, edgecolor='black')\n",
        "# axes[1].bar(x + width, bert_scores, width, label='BERT', color=colors[2], alpha=0.8, edgecolor='black')\n",
        "\n",
        "# axes[1].set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
        "# axes[1].set_title('Gold Standard: Per-Class F1 Comparison', fontsize=14, fontweight='bold')\n",
        "# axes[1].set_xticks(x)\n",
        "# axes[1].set_xticklabels(class_labels, fontsize=11)\n",
        "# axes[1].set_ylim([0, 1.0])\n",
        "# axes[1].legend(fontsize=10)\n",
        "# axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# print(f\"\\n‚úì Visualization complete: Gold Standard performance comparison\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}